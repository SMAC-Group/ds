[
["index.html", "Let’s find a good title Chapter 1 Introduction", " Let’s find a good title Stéphane Guerrier, Justin Lee &amp; Roberto Molinari 2017-07-19 Chapter 1 Introduction Explain what this text is about + objectives. This chapter should summarize the advantage of using R. Following Hadley’s Advanced R, here are the main points: Free, open source which implies easy results can easily be replicated. Massive set of packages Easy packages creation to allow code sharing Cutting edge tools (research in stat and machine learning often use R) Powerful tools for communicating results (Rmarkdown, shinny) Can be “connected” to high-performance languages. In this text, we will discuss C++ but C and FORTRAN are also natural candidates Maybe add drawback of R, for me the main one is that R is slow and doesn’t manages well memory. There are certainly other just as the inconsistency between packages. It would be nice to add some kind of list of “what you should be able to do after reading this text”. Finally, a few remarks on conventions, references (this text will use a lot Hadley’s books) and some acknowledgements. "],
["data.html", "Chapter 2 Data Structures 2.1 Vectors 2.2 Matrix 2.3 Array 2.4 List 2.5 Dataframe 2.6 Data frames", " Chapter 2 Data Structures &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Briefly explain the types of data strucutres in R. Basically there are five ======= There are different data types that are commonly used in R among which the most important ones are the following: &gt;&gt;&gt;&gt;&gt;&gt;&gt; origin/master Numeric (or double): are used to store real numbers. Examples: -4, 12.4532, 6. Integer: examples: 2L, 12L. Logical (or boolean): examples: TRUE, FALSE. Character: examples: &quot;a&quot;, &quot;Bonjour&quot;. In R there are basically five type of data structures that can be used to store elements in. A data structure is said to homogeneous if it only contains elements of the same type (for example it only contains character or numeric values) and heterogenous if it contains elements of more than one type. The five type of data structrures are commonly summarized in the table similar to the one below: Dimension Homogenous Heterogeneous 1 Vector List 2 Matrix Dataframe n Array To illustrate how to use these data structures, we will consider the simple data set of five best (as ranked by ATP on 07-15-2017) single men tennis players. The data are presented in the table below: Name Date of Birth Born Country ATP Ranking Prize Money Win Percentage Grand Slam Wins Andy Murray 15 May 1987 Glasgow, Scotland Great Britain 1 60,449,649 78.07 9 Rafael Nadal 3 June 1986 Manacor, Spain Spain 2 85,920,132 82.48 15 Stan Wawrinka 28 March 1985 Lausanne, Switzerland Switzerland 3 30,577,981 63.96 5 Novak Djokovic 22 May 1987 Belgrade, Serbia Serbia 4 109,447,408 82.77 12 Roger Federer 8 August 1981 Basel, Switzerland Switzerland 5 104,445,185 81.80 18 2.1 Vectors A vector has three important properties: Type, which corresponds the “kind” of objects in contains. It is possible to use the function typeof() to evaluate the type of objects in a vector. Length, i.e. the number of elements in a vector. This information can be obtained using the function length(). Attributes, some additional metadata attached to a vector. The functions attr() and attributes() can be used to store and retrive attributes (more details can be found in Section 2.1.4) For example, let us consider the number of grand slam won by the five players we are considering: grand_slam_win = c(9, 15, 5, 12, 18) To display the values stores in grand_slam_win we could simply do: grand_slam_win ## [1] 9 15 5 12 18 Alteratively, we could have create and display the value by using () around the definition, for example: (grand_slam_win = c(9, 15, 5, 12, 18)) ## [1] 9 15 5 12 18 Various forms of “nest concatenation” can be used to defined vectors, for example we could also define grand_slam_win as (grand_slam_win = c(9, c(15, 5, c(12, c(18))))) ## [1] 9 15 5 12 18 This approach is often used to assemnble vector in various ways. It is also possible to define vector with characters, for example we could define a vector with the player names as follows: (players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;)) ## [1] &quot;Andy Murray&quot; &quot;Rafael Nadal&quot; &quot;Stan Wawrinka&quot; &quot;Novak Djokovic&quot; ## [5] &quot;Roger Federer&quot; 2.1.1 Type We can evaluate the kind or type of elements that are stored in a vector using the function typeof(). For example, for the vectors we just created we obtain: typeof(grand_slam_win) ## [1] &quot;double&quot; typeof(players) ## [1] &quot;character&quot; This is a little suprzing as all the elements in grand_slam_win are integers it would seem natural to expect this as an output of the function typeof(). This is R considers by default any number as a “double”, expet when spefiying L after an integer. For example, typeof(1) ## [1] &quot;double&quot; typeof(1L) ## [1] &quot;integer&quot; Therefore, we could express exp_res as follows: (grand_slam_win_int = c(1L, 2L, 3L, 5L, 1L)) ## [1] 1 2 3 5 1 typeof(grand_slam_win_int) ## [1] &quot;integer&quot; Naturally, the difference between the two in general relatively unimportant but we can see that exp_res_int takes less “space” to two. For example, we have object.size(grand_slam_win) ## 88 bytes object.size(grand_slam_win_int) ## 72 bytes 2.1.2 Coercion As indicated earlier a vector is a homogenous data structures data, meaning that it can only contain a single type of data type. Therefore, when more than one data types are presented R coerces the data into a “shared” type. To identify this “shared” we can use this simple rule: \\[\\begin{equation*} \\text{logical} &lt; \\text{integer} &lt; \\text{numeric} &lt; \\text{character}, \\end{equation*}\\] which simply means that if a vector contains more than data type it will be of the type of the “largest” according to the above equations. Here are few examples: # Logical + integer (mix_logic_int = c(TRUE, 1L)) ## [1] 1 1 typeof(mix_logic_int) ## [1] &quot;integer&quot; # Logical + character (mix_logic_char = c(TRUE, &quot;Hi&quot;)) ## [1] &quot;TRUE&quot; &quot;Hi&quot; typeof(mix_logic_char) ## [1] &quot;character&quot; # Integer + numeric (mix_int_num = c(1, 1L)) ## [1] 1 1 typeof(mix_int_num) ## [1] &quot;double&quot; # Integer + character (mix_int_char = c(1L, &quot;Hi&quot;)) ## [1] &quot;1&quot; &quot;Hi&quot; typeof(mix_int_char) ## [1] &quot;character&quot; 2.1.3 Subsetting Naturally, it is possible to “subset” the values of in our vectror in many ways. Essentially, there are four main ways of subsetting a vector. Here we’ll only discuss the first three: Positive Index: We can access or subset the \\(i\\)-th element of a vector by simply using exp_res[i] where \\(i\\) is a positive number between 1 and length of the vector. # Accesing the first element grand_slam_win[1] ## [1] 9 # Accesing the third and first value grand_slam_win[c(3, 1)] ## [1] 5 9 # Duplicated indices yield duplicated values grand_slam_win[c(1, 1, 2, 2, 3, 4)] ## [1] 9 9 15 15 5 12 Negative Index: We remove elements in a vector using negative indices: # Removing the second obervation grand_slam_win[-2] ## [1] 9 5 12 18 # Removing the first and fourth obserations grand_slam_win[c(-1, -4)] ## [1] 15 5 18 Logical Indices: Another usefull approach is based on logical operators: # Access the first and fourth observations grand_slam_win[c(TRUE, FALSE, FALSE, TRUE, FALSE)] ## [1] 9 12 Here we could add some remarks on weird cases, for example exp_res[c(1.2, 3.4)] (which rounds things up) or exp_res[c(-1, 2)] (which doesn’t work as “mixed” indices are not permitted). 2.1.4 Attributes Our experiment was conducted under specific conditions, say a date and a place which should be store are attributes. Indeed, objects can have arbitrary additional attributes, used to store metadata about the considered object. For example: attr(grand_slam_win, &quot;date&quot;) = &quot;07-15-2017&quot; attr(grand_slam_win, &quot;type&quot;) = &quot;Men, Single&quot; To display the vector with its attributes grand_slam_win ## [1] 9 15 5 12 18 ## attr(,&quot;date&quot;) ## [1] &quot;07-15-2017&quot; ## attr(,&quot;type&quot;) ## [1] &quot;Men, Single&quot; To only display the attributes attributes(grand_slam_win) ## $date ## [1] &quot;07-15-2017&quot; ## ## $type ## [1] &quot;Men, Single&quot; It is also possible to extract a specific attribute attr(grand_slam_win, &quot;date&quot;) ## [1] &quot;07-15-2017&quot; 2.1.5 Adding labels In some cases, it might be useful to add label to vectors. For example, we could defined the vector grand_slam_win and use as labels the player’s names, i.e. (grand_slam_win = c(&quot;Andy Murray&quot; = 9, &quot;Rafael Nadal&quot; = 15, &quot;Stan Wawrinka&quot; = 5, &quot;Novak Djokovic&quot; = 12, &quot;Roger Federer&quot; = 18)) ## Andy Murray Rafael Nadal Stan Wawrinka Novak Djokovic Roger Federer ## 9 15 5 12 18 The main advantage of this approach is that the number of grand slam won can now be referred to by the player’s name. For example: grand_slam_win[&quot;Andy Murray&quot;] ## Andy Murray ## 9 grand_slam_win[c(&quot;Andy Murray&quot;,&quot;Roger Federer&quot;)] ## Andy Murray Roger Federer ## 9 18 All labels (players’ names in our case) can be obtained witht the function names, i.e. names(grand_slam_win) ## [1] &quot;Andy Murray&quot; &quot;Rafael Nadal&quot; &quot;Stan Wawrinka&quot; &quot;Novak Djokovic&quot; ## [5] &quot;Roger Federer&quot; 2.1.6 Useful functions with vectors Add some text here length() sum() mean() sort() and order() For example length(grand_slam_win) ## [1] 5 sum(grand_slam_win) ## [1] 59 mean(grand_slam_win) ## [1] 11.8 To sort the player by number of grand slam we could use the function order() which returns the position of the elements of a vector sorted in an ascending manner, order(grand_slam_win) ## [1] 3 1 4 2 5 Therefore, we can sort the players as follow players[order(grand_slam_win)] ## [1] &quot;Stan Wawrinka&quot; &quot;Andy Murray&quot; &quot;Novak Djokovic&quot; &quot;Rafael Nadal&quot; ## [5] &quot;Roger Federer&quot; showing the Roger Federer won the most grand slam. Another related function is sort() which simply sorts the elements of a vector (in an ascending manner). For example, sort(grand_slam_win) ## Stan Wawrinka Andy Murray Novak Djokovic Rafael Nadal Roger Federer ## 5 9 12 15 18 which is compact version of grand_slam_win[order(grand_slam_win)] ## Stan Wawrinka Andy Murray Novak Djokovic Rafael Nadal Roger Federer ## 5 9 12 15 18 There are of course many other usefull to dealing with vectors. 2.1.7 Creation sequences When uing R for statstical programming or even data analysis it is very common to create sequences of numbers. Here are three common ways for generatign such sequences: from:to: This method is quite inituitive and very compact. For example: (x = 1:3) ## [1] 1 2 3 (y = 3:1) ## [1] 3 2 1 (w = -1:-4) ## [1] -1 -2 -3 -4 (z = 1.3:3) ## [1] 1.3 2.3 seq_len(n): This function provides a simple way to generate a sequence from 1 to an arbitrary number n. In general, 1:n and seq_len(n) are equivalent with the notable exeptions of the cases n = 0 and n &lt; 0. The reason for these exeption will become clear in Section 3.2.2.1. Let’s see a few examples: n = 3 1:n ## [1] 1 2 3 seq_len(n) ## [1] 1 2 3 n = 0 1:n ## [1] 1 0 seq_len(n) ## integer(0) seq(a, b, by/length.out = d): This function can be used to create more “complexe” sequences. It either be used to create a sequence from a to b by increments of d (using the option by) or of a total length of d (using the option length.out). A few examples: (x = seq(1, 2.8, by = 0.4)) ## [1] 1.0 1.4 1.8 2.2 2.6 (y = seq(1, 2.8, length.out = 6)) ## [1] 1.00 1.36 1.72 2.08 2.44 2.80 Maybe it would be interesting to add something rep() for example: rep(c(1,2), times = 3, each = 1) ## [1] 1 2 1 2 1 2 rep(c(1,2), times = 1, each = 3) ## [1] 1 1 1 2 2 2 2.1.8 Example: Apple Stock Price How to get today’s date (today = Sys.Date()) ## [1] &quot;2017-07-19&quot; Three monmths ago (three_months_ago = seq(today, length = 2, by = &quot;-3 months&quot;)[2]) ## [1] &quot;2017-04-19&quot; Let’s download Apple stock price library(quantmod) getSymbols(&quot;AAPL&quot;, from = three_months_ago, to = today) ## [1] &quot;AAPL&quot; candleChart(AAPL, theme=&#39;white&#39;, type=&#39;candles&#39;) Let’s compute some returns… \\[\\begin{equation} r_t = \\frac{S_t - S_{t-1}}{S_{t-1}} \\end{equation}\\] where \\(r_t\\) are the return, \\(S_t\\) the stock price. This is implemented in the function ClCl() of the package quantmod. For example, we can create a vector of returns as follows AAPL_returns = as.numeric(na.omit(ClCl(AAPL))) na.omit to remove missing value as if we have \\(n+1\\) stock prices we only \\(n\\) returns as.numeric to transform into a vector. We can now compute mean and median return over the period mean(AAPL_returns) ## [1] 0.001179654 median(AAPL_returns) ## [1] 0.002557922 Excess Kurtosis can be defined for a random variable \\(X\\) as \\[\\begin{equation} \\text{Kurt} = \\frac{{E}\\left[\\left(X - E[X]\\right)^4\\right]}{\\left({E}\\left[\\left(X - E[X]\\right)^2\\right]\\right)^2} - 3 \\end{equation}\\] The reason excess is …. A common estimator of the excess Kurtosis is \\[\\begin{equation} k = \\frac{\\frac{1}{n} \\sum_{t = 1}^{n} \\left(r_t -\\bar{r}\\right)^4}{\\left(\\frac{1}{n} \\sum_{t = 1}^{n} \\left(r_t -\\bar{r}\\right)^2 \\right)^2} - 3 \\end{equation}\\] where \\(\\bar{k}\\) denotes the sample average of the returns, i.e. \\[\\begin{equation} \\bar{k} = \\frac{1}{n} \\sum_{i = 1}^n r_i \\end{equation}\\] mu = mean(AAPL_returns) (k = mean((AAPL_returns - mu)^4)/(mean((AAPL_returns - mu)^2))^2 - 3) ## [1] 2.127954 which is quite high tends to indicate the returns have a heavier tails than the normal distribution. 2.2 Matrix Example of matrix creation (mat = matrix(1:12, ncol = 4, nrow = 3)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 explain cbind and rbind players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;) grand_slam_win = c(9, 15, 5, 12, 18) win_percentage = c(78.07, 82.48, 63.96, 82.77, 81.80) (mat = cbind(grand_slam_win, win_percentage)) ## grand_slam_win win_percentage ## [1,] 9 78.07 ## [2,] 15 82.48 ## [3,] 5 63.96 ## [4,] 12 82.77 ## [5,] 18 81.80 explain rownames colnames rownames(mat) &lt;- players colnames(mat) &lt;- c(&quot;GS win&quot;, &quot;Win rate&quot;) mat ## GS win Win rate ## Andy Murray 9 78.07 ## Rafael Nadal 15 82.48 ## Stan Wawrinka 5 63.96 ## Novak Djokovic 12 82.77 ## Roger Federer 18 81.80 2.2.1 Subsetting mat[c(&quot;Stan Wawrinka&quot;, &quot;Roger Federer&quot;), ] ## GS win Win rate ## Stan Wawrinka 5 63.96 ## Roger Federer 18 81.80 mat[c(1, 3), ] ## GS win Win rate ## Andy Murray 9 78.07 ## Stan Wawrinka 5 63.96 mat[, 2] ## Andy Murray Rafael Nadal Stan Wawrinka Novak Djokovic Roger Federer ## 78.07 82.48 63.96 82.77 81.80 mat[1:3, 1] ## Andy Murray Rafael Nadal Stan Wawrinka ## 9 15 5 2.2.2 Useful fun + matrix algebra The function dim() allows to determine the dimension of a matrix. For example, consider the following \\(4 \\times 2\\) matrix \\[\\begin{equation*} \\mathbf{A} = \\left[ \\begin{matrix} 1 &amp; 5\\\\ 2 &amp; 6\\\\ 3 &amp; 7\\\\ 4 &amp; 8 \\end{matrix} \\right] \\end{equation*}\\] which can be defined as: (A = matrix(1:8, 4, 2)) ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 ## [4,] 4 8 Therefore, we expect dim(A) to retrun the vector c(4, 2). Indeed, we have dim(A) ## [1] 4 2 Next, we consider the function t() allows transpose a matrix. For example, \\(\\mathbf{A}^T\\) is equal to: \\[\\begin{equation*} \\mathbf{A}^T = \\left[ \\begin{matrix} 1 &amp; 2 &amp; 3 &amp; 4\\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\end{matrix} \\right], \\end{equation*}\\] which is a \\(2 \\times 4\\) matrix. In R, we obtain (At &lt;- t(A)) ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 dim(At) ## [1] 2 4 The operator %*% is used in R to denote matrix multiplicate. Note that the regular product opertor * used with matrices performs the Hadamar product (or element by element product). For example, %*% det() solve() 2.2.3 Example: variance as matrix n = 100 x = rnorm(n, 0, sqrt(4)) (sig2 = 1/n*sum((x - mean(x))^2)) ## [1] 4.77491 (sig2_mat = as.numeric(1/n*t(x)%*%x - (1/n*t(rep(1,n))%*%x)^2)) ## [1] 4.77491 It is also interesting to compare sig2 and sig2_mat sig2 - sig2_mat ## [1] 8.881784e-16 numerical error 2.2.4 Example: Least-squares If the matrix \\(\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\), the least-squares estimator for \\(\\boldsymbol{\\beta}\\) is given by: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} \\tag{2.1} \\end{equation}\\] In the comment box below, we derive Eq. (2.1). If you aren’t familiar with such calculation it might to read or something like this The least-square estimator \\(\\hat{\\boldsymbol{\\beta}}\\) can be defined as \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\operatorname{argmin}_{\\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) \\end{equation*}\\] The first step of this derivation is to rexpress the term \\(\\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right)\\) as follows: \\[\\begin{equation*} \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) = \\mathbf{y}^T\\mathbf{y} + \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} - 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y}. \\end{equation*}\\] In case you were suprizied by the term \\(2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y}\\) remeber that a scalar can always be transpose with changing its value and therefore we have that $ ^T ^T = ^T $. Now, out next step is the compute \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right). \\end{equation*}\\] To do this we should remeber the following results \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y} = \\boldsymbol{y}^T \\mathbf{X}, \\end{equation*}\\] and \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} = 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X}. \\end{equation*}\\] The proof of these two results can for example be found in Propositions 7 and 9 of Prof. Barnes’ notes. Using these two results we obtain \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) = 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} - 2 \\boldsymbol{y}^T \\mathbf{X}. \\end{equation*}\\] By solving for the first order condition (and under some technical assumptions not discussed here) we can redefine \\(\\hat{\\boldsymbol{\\beta}}\\) through the follwing equation \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}}^T \\mathbf{X}^T \\mathbf{X} = \\boldsymbol{y}^T \\mathbf{X}, \\end{equation*}\\] which is equivalent to \\[\\begin{equation*} \\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\boldsymbol{y}. \\end{equation*}\\] If \\(\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\) exist, \\(\\hat{\\boldsymbol{\\beta}}\\) is therefore given by \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y}, \\end{equation*}\\] which verifies Eq. (2.1). The variance of \\(\\hat{\\boldsymbol{\\beta}}\\) is given by \\[\\begin{equation} \\text{Var} \\left(\\hat{\\boldsymbol{\\beta}} \\right) = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}, \\tag{2.2} \\end{equation}\\] the derivation of this results is explain in the comment box below. We let \\(\\mathbf{A} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T\\). Then, we have \\[\\begin{equation*} \\begin{aligned} \\text{Var} \\left(\\hat{\\boldsymbol{\\beta}} \\right) &amp;= \\text{Var} \\left( \\mathbf{A} \\mathbf{y} \\right) = \\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T \\\\ &amp; = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}, \\end{aligned} \\end{equation*}\\] which verifies Eq. (2.2). To understand the above derivation we might be usefull to remind and point out a few things: \\(\\text{Var} \\left( \\mathbf{A} \\mathbf{y} \\right) = \\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T\\) since \\(\\mathbf{A}\\) is not a random variable. \\(\\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T\\) since\\(\\text{Var} \\left( \\mathbf{y} \\right) = \\sigma^2 \\mathbf{I}\\) and therefore we have \\(\\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{I} \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T\\). The result \\(\\mathbf{A} \\mathbf{A}^T = (\\mathbf{X}^T \\mathbf{X})^{-1}\\) is based on the fact that \\((\\mathbf{X}^T \\mathbf{X})^{-1}\\) is symmetric but this is not necessarily intuitive. Indeed, this follows from the fact that any square and invertible matrix \\(\\mathbf{B}\\) is such that the inverse and transpose operator commute, meaning that \\(( \\mathbf{B}^T )^{-1} = ( \\mathbf{B}^{-1} )^T\\). Therefore since the matrix \\(\\mathbf{X}^T \\mathbf{X}\\) is square and (by assumption) invertible we have \\([(\\mathbf{X}^T \\mathbf{X})^{-1}]^T = [(\\mathbf{X}^T \\mathbf{X})^{T}]^{-1} = ( \\mathbf{X}^T \\mathbf{X})^{-1}\\). In general, the residual variance is unknown and needs to estimate. A common and unbiased estimator of \\(\\sigma^2\\) is given by \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{n - p} \\left( \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\right) \\tag{2.3} \\end{equation}\\] DO YOU GUYS THINK WE SHOULD SHOW THE UNBIASEDNESS IN BOX HERE. THIS IS A LITTLE MORE ADVANCED AS WE NEED TO USE PROJECTION MATRICES AND THEIR PROPERTIES. LET ME KNOW. Let’s implement Eq. (2.1) to (2.3) and compare with the lm() implemented in base R. Before doing maybe we could use the dataset hubble. I think it is quite cool as it can be used to estimate the “age” of the universe and test wether the estimate of the age of the universe by Creation Scientists (based on a reading of the Bible) is reasonable. This is an example based on Simon Woods book. If we do this example, we should add something on confidence interval and I think he could the normal distribution (instead of t-distribution) to avoid going into details. Anyway the t-test relies on Normal assumption which is hard to verify. In the following chapter, we could use this example to show the various form of boostrap (parametric, non-parametric, semi-parametric). Let me know what you think. 2.3 Array 2.4 List 2.5 Dataframe 2.5.1 Example: Making Maps birth_place = c(&quot;Glasgow, Scotland&quot;, &quot;Manacor, Spain&quot;, &quot;Lausanne, Switzerland&quot;, &quot;Belgrade, Serbia&quot;, &quot;Basel, Switzerland&quot;) library(ggmap) glasgow_coord = geocode(&quot;Glasgow, Scotland&quot;) glasgow_coord ## lon lat ## 1 -4.251806 55.86424 birth_coord = geocode(birth_place) birth_coord ## lon lat ## 1 -4.251806 55.86424 ## 2 3.209532 39.56972 ## 3 6.632273 46.51965 ## 4 20.448922 44.78657 ## 5 7.588576 47.55960 class(birth_coord) ## [1] &quot;data.frame&quot; birth_coord$Players = players birth_coord$GS = grand_slam_win birth_coord ## lon lat Players GS ## 1 -4.251806 55.86424 Andy Murray 9 ## 2 3.209532 39.56972 Rafael Nadal 15 ## 3 6.632273 46.51965 Stan Wawrinka 5 ## 4 20.448922 44.78657 Novak Djokovic 12 ## 5 7.588576 47.55960 Roger Federer 18 Let’s represent this information graphically. We haven’t seen how to make graph yet so don’t worry to much about the details of how this graph is made library(mapproj) map &lt;- get_map(location = &#39;Switzerland&#39;, zoom = 4) ggmap(map) + geom_point(data = birth_coord, aes(lon, lat, col = Players, size = GS)) + scale_size(name=&quot;Grand Slam Wins&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) 2.6 Data frames A data frame is the most common way of storing data in R, it has a 2D structure and shares properties of both the matrix and the list. We can create a data frame using data.frame() ### Creation players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;) grand_slam_win = c(9, 15, 5, 12, 18) date_of_birth = c(&quot;15 May 1987&quot;, &quot;3 June 1986&quot;, &quot;28 March 1985&quot;, &quot;22 May 1981&quot;, &quot;8 August 1981&quot;) country = c(&quot;Great Britain&quot;, &quot;Spain&quot;, &quot;Switzerland&quot;, &quot;Serbia&quot;, &quot;Switzerland&quot;) ATP_ranking = c(1, 2, 3, 4, 5) prize_money = c(60449649, 85920132, 30577981, 109447408, 104445185) tennis = data.frame(date_of_birth, grand_slam_win, country, ATP_ranking, prize_money) dimnames(tennis)[[1]] = players tennis ## date_of_birth grand_slam_win country ATP_ranking ## Andy Murray 15 May 1987 9 Great Britain 1 ## Rafael Nadal 3 June 1986 15 Spain 2 ## Stan Wawrinka 28 March 1985 5 Switzerland 3 ## Novak Djokovic 22 May 1981 12 Serbia 4 ## Roger Federer 8 August 1981 18 Switzerland 5 ## prize_money ## Andy Murray 60449649 ## Rafael Nadal 85920132 ## Stan Wawrinka 30577981 ## Novak Djokovic 109447408 ## Roger Federer 104445185 We can check if we have achived our gooal by using: is.data.frame(tennis) ## [1] TRUE 2.6.1 Combination Different data frames can also be combined. Let say we want to add some ifomrmation to our initial table e.g. the player’s height and if he is right-handed or letf-handed. We can do so by using cbind() and rbind(): height &lt;- c(1.90, 1.85, 1.83, 1.88, 1.85) hand &lt;- c(&quot;R&quot;,&quot;L&quot;,&quot;R&quot;,&quot;R&quot;,&quot;R&quot;) (tennis = cbind(tennis, data.frame(height, hand))) ## date_of_birth grand_slam_win country ATP_ranking ## Andy Murray 15 May 1987 9 Great Britain 1 ## Rafael Nadal 3 June 1986 15 Spain 2 ## Stan Wawrinka 28 March 1985 5 Switzerland 3 ## Novak Djokovic 22 May 1981 12 Serbia 4 ## Roger Federer 8 August 1981 18 Switzerland 5 ## prize_money height hand ## Andy Murray 60449649 1.90 R ## Rafael Nadal 85920132 1.85 L ## Stan Wawrinka 30577981 1.83 R ## Novak Djokovic 109447408 1.88 R ## Roger Federer 104445185 1.85 R 2.6.2 Subsetting Like for vectors, it is also possible to subset the values that we have stored in our data frames. Since data frames possess the characteristics of both lists and matrices: if you subset with a single vector, they behave like lists; if you subset with two vectors, they behave like matrices. # Let say we want only want to know the country and date of # birth of the players # There are two ways to select columns from a data frame # Like a list: tennis[c(&quot;country&quot;, &quot;date_of_birth&quot;)] ## country date_of_birth ## Andy Murray Great Britain 15 May 1987 ## Rafael Nadal Spain 3 June 1986 ## Stan Wawrinka Switzerland 28 March 1985 ## Novak Djokovic Serbia 22 May 1981 ## Roger Federer Switzerland 8 August 1981 # Like a matrix tennis[, c(&quot;country&quot;, &quot;date_of_birth&quot;)] ## country date_of_birth ## Andy Murray Great Britain 15 May 1987 ## Rafael Nadal Spain 3 June 1986 ## Stan Wawrinka Switzerland 28 March 1985 ## Novak Djokovic Serbia 22 May 1981 ## Roger Federer Switzerland 8 August 1981 # To acces a single element, let say the date of birth, # you can also use: tennis$date_of_birth ## [1] 15 May 1987 3 June 1986 28 March 1985 22 May 1981 8 August 1981 ## 5 Levels: 15 May 1987 22 May 1981 28 March 1985 ... 8 August 1981 2.6.3 Application: Non-parametric bootstrap Suppose we ask 10 students how much time they work at home for their calculus class, we obtain the following results (in hour) student_work &lt;- c(0, 0, 0, 0.25, 0.25, 0.75, 0.75, 1, 1.25, 4) We can compute the mean time spent mean(student_work) ## [1] 0.825 ADD SOMETHING ON T TEST t.test(student_work)$conf.int ## [1] -0.03495865 1.68495865 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 We can see that our confidence interval includes a negative values which clearly isn’t meaningful. Solution: (non-parametric) bootstrap which works as follows….. ADD SOMETHING Here is a simple function to implement this approach: # Number of boostrap replications B = 500 # Compute the length of vector n = length(student_work) # Confidence level alpha = 0.05 # Initialisation of boot_mean = rep(NA, B) for (i in 1:B){ student_work_star = student_work[sample(1:n, replace = TRUE)] boot_mean[i] = mean(student_work_star) } quantile(boot_mean, c(alpha/2, 1 - alpha/2)) ## 2.5% 97.5% ## 0.286875 1.600000 #hist(boot_mean, probability = TRUE) "],
["control.html", "Chapter 3 Logical Operators and Control Stuctures 3.1 Logical Operators 3.2 Control Structures 3.3 Applications:", " Chapter 3 Logical Operators and Control Stuctures 3.1 Logical Operators 3.2 Control Structures 3.2.1 Selection Controls Basically if, if/else, if/elseif/else and switch 3.2.2 Iteration Controls 3.2.2.1 for 3.3 Applications: 3.3.1 Non-parametric Bootstrap Suppose we ask 10 students how much time they work at home for their calculus class, we obtain the following results (in hour) student_work = c(0, 0, 0, 0.25, 0.25, 0.75, 0.75, 1, 1.25, 4) We can compute the mean time spent mean(student_work) ## [1] 0.825 ADD SOMETHING ON T TEST t.test(student_work)$conf.int ## [1] -0.03495865 1.68495865 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 We can see that our confidence interval includes a negative values which clearly isn’t meaningful. Solution: (non-parametric) bootstrap which works as follows….. ADD SOMETHING Here is a simple function to implement this approach: # Number of boostrap replications B = 500 # Compute the length of vector n = length(student_work) # Confidence level alpha = 0.05 # Initialisation of boot_mean = rep(NA, B) for (i in 1:B){ student_work_star = student_work[sample(1:n, replace = TRUE)] boot_mean[i] = mean(student_work_star) } quantile(boot_mean, c(alpha/2, 1 - alpha/2)) ## 2.5% 97.5% ## 0.236875 1.575000 #hist(boot_mean, probability = TRUE) "],
["functions.html", "Chapter 4 Functions 4.1 Function arguments 4.2 Function body 4.3 Function environment", " Chapter 4 Functions This chapter aims at highlighting the main advantages, characteristics, arguments and structure of functions in R. As you already know, a function is a collection of logically connected commands and operations that allow the user to input some arguments and obtain a desired output (based on the given arguments) without having to rewrite the mentioned code each time that specific output is needed. Indeed, a common task in statistical research consists in running some simulation studies which give support (or not) to the use of a certain method of inference. In this case, it’s not efficient to rewrite the code each time it is needed within a simulation study because it would lead to lengthy code and increased risk of miss-coding. Considering this, in the previous chapter we discussed the non-parametric bootstrap which is a technique used to perform statistical inference in different cases (e.g. small sample sizes, uncertainty on the distribution of the underlying variable, etc.). However, there exist many other types of bootstrap techniques, including the parametric bootstrap which assumes a parametric model for the data (e.g. a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) that represent the parameters). For this bootstrap technique, we assume that we know the underlying model generating the data and therefore estimate its parameters. Once this is done, we use the estimated parameters as if they were the true parameters and use them to simulate from the assumed model. In this manner we generate a parametric bootstrap sample on which we can compute the statistic we are intersted in (e.g. the sample mean), thereby obtaining a distribution for this statistic by simulating many other samples. What are the advantages of these different inferential methods? Which approach appears to be preferable to the others and under what circumstances? We can run some specific simulation studies to get an idea of the answers to these questions but, to do so, we will need to create functions the implement these methods. The next sections will present the main features of functions in R by studying their main components. R functions have three main components to them: body: the code lines containing the commands and operations which deliver the desired output; arguments: the inputs the user gives to the function which will determine the value or type of output of a function; environment: every R function is built within an enviroment of reference from which to source possible input values and/or other functions necessary for it to work. We will briefly go over these components by building a function implementing the non-parametric bootstrap to find a confidence interval for the sample mean which we saw in the previous chapter: nonpar_boot &lt;- function(x, B = 500, alpha = 0.05) { boot_mean &lt;- rep(NA, B) n &lt;- length(x) for (i in 1:B) { sample_star = x[sample(1:n, replace = TRUE)] boot_mean[i] = mean(sample_star) } return(quantile(boot_mean, c(alpha/2, 1 - alpha/2))) } 4.1 Function arguments As you can see, the function’s name is “nonpar_boot( )” and it has three arguments to it: “x” which represents the original sample on which we would like to perform the bootstrap procedure; “B” which represents the number of bootstrap samples we wish to generate; “alpha” which determines the size of the confidence interval we wish to compute The function therefore has three arguments to it and these can be extracted by using the function “formals( )” formals(nonpar_boot) ## $x ## ## ## $B ## [1] 500 ## ## $alpha ## [1] 0.05 A feature which can be underlined at this stage is that, as can be seen for the above mentioned function, it is possible to provide default values for some, all or none of the function arguments. In this case, default values have been given for the arguments B and alpha, consequently the user does not need to specify the value of these arguments unless they wish to specify different values compared to the default values given. R functions are quite flexible when specifying the arguments. These can in fact be specified by exact name, prefix matching or position (and are given priority in this order). For example, one could specify the arguments for the above non-parametric bootstrap function as follows “nonpar_boot(B = 200, sim, a = 0.1)” where the object “sim” is the sample which in the function is named “x”. For a more in-depth overview of this feature refer to Wickham (2014). 4.2 Function body 4.3 Function environment References "],
["build-a-package.html", "Chapter 5 Build a package", " Chapter 5 Build a package Expalin how to build a package, I would add style guide and something on pkgdown "],
["case-study-monte-carlo-integration.html", "Chapter 6 Case Study: Monte-Carlo Integration", " Chapter 6 Case Study: Monte-Carlo Integration Monte Carlo integration is a powerful technique for numerical integration. It is particulary useful to evaluate integrals of “high-dimension”. A detailed (and formal) discussion of this method is clearly beyond the scope of this class and we shall restrict our attention to most basic form(s) of Monte Carlo Integration and breifly discuss the rational behind this method. Originally, such Monte Carlo methods were known under various names among which statistical sampling was probably the most commonly used. In fact, the name Monte Carlo was popularized by several physics researchers, including Stanislaw Ulam, Enrico Fermi and John von Neumann. The name is believed to be a reference to a famous casino in Monaco where Stanislaw Ulam’s uncle would borrow money to gamble. Enrico Fermi was one of the first to this technique which he employed to study the properties newly-discovered neutron in the 1930s. Later, these methods played for example a central role in many of thesimulations required for the Manhattan project. Suppose we are interested in computing the following intergal: \\[I = \\int_a^b f(x) dx.\\] Of course, this integral can be approximated by a Riemann sum, \\[I \\approx \\Delta x \\sum_{i = 1}^n f(a + (i-1) \\Delta x),\\] where \\(\\Delta x = \\frac{b - a}{n}\\;\\) and the idea behind this approaximation is that as the number of partions \\(n\\) increases the Riemann sum will become closer and closer to \\(I\\). Also (and under some technical conditions), we have that \\[I = \\lim_{n \\to \\infty} \\Delta x \\sum_{i = 1}^n f(a + (i-1) \\Delta x).\\] In fact, the rational of a Monte Carlo Integral is quite close to the Riemann sum since, in its most basic form, we approximate \\(I\\) by averaging samples of the function \\(f(x)\\) at uniform random point within the interval \\([a, b]\\). Therefore, the Monte Carlo estimator of \\(I\\) is given by \\[\\begin{equation} \\hat{I} = \\frac{b - a}{B} \\sum_{i = 1}^B f(X_i), \\tag{6.1} \\end{equation}\\] where \\(X_i = a + U_i (b - a)\\) and \\(U_i \\sim \\mathcal{U}(0,1)\\). In fact, (6.1) is quite intuniteve as \\(\\frac{1}{B} \\sum_{i = 1}^B f(X_i)\\) respresents an estimation of the average value of \\(f(x)\\) in the interval \\([a, b]\\) and thus \\(\\hat{I}\\) is simply the average value time the length of the interval, i.e. \\((b-a)\\). A more formal argument on the validity of this approach can be found in analysyzing the statistical properties of the estimator \\(\\hat{I}\\). In order to do so, we start by considering its expected value \\[ \\mathbb{E}\\left[ \\hat{I} \\right] = \\frac{b - a}{B} \\sum_{i = 1}^B \\mathbb{E}\\left[ f(X_i) \\right] = \\frac{b - a}{B} \\sum_{i = 1}^B \\int f(x) g(x) dx, \\] where \\(g(x)\\) denotes the pdf of \\(X_i\\). Since \\(X_i \\sim \\mathcal{U}(a, b)\\) it follows that \\[ g(x) = \\left\\{ \\begin{array}{ll} \\frac{1}{b - a} &amp; \\mbox{if } x \\in [a, b] \\\\ 0 &amp; \\mbox{if } x \\not\\in [a, b] \\end{array} \\right. \\] Therefore, we have \\[ \\mathbb{E}\\left[ \\hat{I} \\right] = \\frac{b - a}{B} \\sum_{i = 1}^B \\int_a^b \\frac{f(x)}{b-a} dx = \\int_a^b f(x) dx = I, \\] Since \\(X_i\\) are iid, the same can be said about \\(f(X_i)\\) and therefore by the Strong Law of Large Numbers we have that \\(\\hat{I}\\) converge almost surely to \\(I\\), which means that \\[ \\mathbb{P}\\left(\\lim_{B \\to \\infty} \\hat{I} = I \\right) = 1. \\] This result imples that as the number of simulations \\(B\\) goes to infinity we can guarantee that the solution will be exact. Unfortunately, this result does give us any information on how quickly this estimate converges to “sufficiently accurate” solution for the problem at hand. This can be done by studying the variance of \\(\\hat{I}\\) and its rate of convergence. \\[ \\begin{aligned} \\operatorname{var} \\left( \\hat{I} \\right) &amp;= \\left(\\frac{b - a}{B}\\right)^2 \\sum_{i = 1}^B \\left\\{\\mathbb{E}\\left[f^2(X_i)\\right] - \\mathbb{E}^2\\left[f(X_i)\\right]\\right\\}\\\\ &amp;= \\frac{1}{B^2} \\sum_{i = 1}^B \\left\\{(b-a) \\int_a^b f^2(x) dx - \\left(\\int_a^b f(x) dx \\right)^2 \\right\\}\\\\ &amp;= \\frac{(b-a) I_2 - I^2}{B} \\end{aligned} \\] where \\(I_2 = \\int_a^b f^2(x) dx\\). A simple estimator of this quantity is given by \\[ \\hat{I}_2 = \\frac{b - a}{B} \\sum_{i = 1}^B f^2(X_i), \\] and therefore using \\(\\hat{I}\\) we obtain: \\[ \\widehat{\\operatorname{var}} \\left(\\hat{I} \\right) = \\frac{(b-a) \\hat{I}_2 - \\hat{I}^2}{B} = \\frac{b - a}{B^2} \\sum_{i = 1}^B\\left[ (b - a )f^2(X_i) - f(X_i)\\right] \\] Thus, it is easy to see that the rate of convergence of \\(\\widehat{\\operatorname{var}} \\left(\\hat{I} \\right)\\) is \\(B^{-1}\\). This implies that if we wish to reduce the error (or standard deviation) by half we need to quadruple \\(B\\). Such phenomon is very common in many research such as Statistics is often called the curse of dimensionality. The function below implements the above precedure and is also available in the stat298 package. There isn’t anything difficult about this code but one thing we haven’t seen yet is how to evaluate a string my_fun = &quot;x^2&quot; x = 0:3 eval(parse(text = my_fun)) ## [1] 0 1 4 9 #&#39; Generate random points with bivariate uniform distribution #&#39; #&#39; @param x_range A vector of dimension 2 used to denote the parameter #&#39; of the uniform distribution on the x-axis. #&#39; @param y_range A vector of dimension 2 used to denote the parameter #&#39; of the uniform distribution on the y-axis. #&#39; @param B A positive number used to denote the number of simulation. #&#39; @param seed A number used to control the seed of the random number #&#39; generated by this function. #&#39; @return A B x 2 matrix containing the simulated points. #&#39; @examples #&#39; gen_points(x_range = c(0, 1), y_range = c(0, 2), B = 10) #&#39; gen_points(x_range = c(-10, 1), y_range = c(0, 0.1), B = 5) monte_carlo_integral_uniform = function(x_range, fun, B, seed = 1291){ # A few checks # Check x_range if (length(x_range) != 2 || x_range[1] &gt;= x_range[2]){ error(&quot;x_range is uncorrely specified&quot;) } # Check fun if (class(fun) != &quot;character&quot;){ error(&quot;fun is uncorrectly specified and should be a character&quot;) } x = mean(x_range) test_fun = try(eval(parse(text = fun)), silent = TRUE) if (class(test_fun) == &quot;try-error&quot;){ error(&quot;fun cannot be evaluated&quot;) } # Check B if (B &lt; 1){ error(&quot;B is uncorrely specified&quot;) } # Set seed set.seed(seed) # Compute the length of the interval, i.e. (b-a) interval_length = diff(x_range) # Let&#39;s draw some uniforms to get Ui and Xi Ui = runif(B) Xi = x_range[1] + Ui*interval_length # Compute \\hat{I} x = Xi I_hat = interval_length*mean(eval(parse(text = fun))) # Compute \\hat{I}_2 I2_hat = interval_length*mean((eval(parse(text = fun)))^2) var_I_hat = (interval_length*I2_hat - I_hat^2)/B # Output list out = list(I = I_hat, var = var_I_hat) out } n_step = 10^3 x = seq(from = -0.35, to = 3.35, length.out = n_step) y = x^2 set.seed(6) Ui = runif(4, 0 ,3) par(mfrow = c(1, 4)) for (i in 1:4){ plot(NA, xlim = range(x), ylim = range(y), xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;) grid() trans_col = hcl(h = seq(15, 375, length = 5), l = 65, c = 100, alpha = 0.15)[1:5] lines(x, y, col = &quot;blue4&quot;) points(Ui[i], Ui[i]^2, pch = 16, col = &quot;yellow2&quot;) polygon(c(0, Ui[i], Ui[i], 0), c(0, 0, Ui[i]^2, Ui[i]^2), border = NA, col = trans_col[i]) } Example \\[ I = \\int_0^3 x^2 dx = \\frac{1}{3} \\left[x^3\\right]_0^3 = 9 \\] B = 4^(4:12) results = matrix(NA, length(B), 3) for (i in 1:length(B)){ mc_res = monte_carlo_integral_uniform(c(0, 3), &quot;x^2&quot;, B = B[i], seed = i+1) results[i, ] = mc_res$I + 2*c(0, sqrt(mc_res$var), -sqrt(mc_res$var)) } trans_blue = hcl(h = seq(15, 375, length = 3), l = 65, c = 100, alpha = 0.15)[2] plot(NA, xlim = range(B), ylim = range(results[, 2:3]), log = &quot;x&quot;, ylab = &quot;Estimated Integral&quot;, xlab = &quot;Number of Simulations B&quot;, xaxt = &#39;n&#39;) grid() axis(1, at = B, labels = parse(text = paste(&quot;4^&quot;, 4:12, sep = &quot;&quot;))) polygon(c(B, rev(B)), c(results[, 2], rev(results[, 3])), border = NA, col = trans_blue) lines(B, results[, 1], type = &quot;b&quot;, col = &quot;blue4&quot;, pch = 16) abline(h = 9, col = &quot;red4&quot;, lty = 2) legend(&quot;topright&quot;, c(expression(I[1]), &quot;Confidence Interval&quot;, &quot;True Value&quot;), bty = &quot;n&quot;, pch = c(16, 15, NA), lwd = c(1, NA, 1), lty = c(1, NA, 2), pt.cex = c(1, 2, NA), col = c(&quot;blue4&quot;, trans_blue, &quot;red4&quot;)) \\[ \\widehat{\\operatorname{var}} \\left(\\hat{I} \\right) = \\mathcal{O}(B^{-1}). \\] The Monte-Carlo approach generalized in the following way. We suppose again that we wish to solve \\[\\int_a^b f(x) dx,\\] and that \\(X_i \\sim G\\) where the pdf of \\(X_i\\) is such that \\[ \\max_{\\mathbb{R} \\setminus [a, b]} \\; g(x) = 0 . \\] \\[ \\tilde{I} = \\frac{1}{B}\\sum_{i = 1}^B \\frac{f(X_i)}{g(X_i)} \\] Of course, in the case \\(X_i \\mathcal{U}(a, b)\\), then \\(\\hat{I}\\) and \\(\\hat{I}\\) are equivalent. [ = ] To understand the very basic idea of Monte Carlo integration we shall start with a simple example. Suppose we wish to compute: \\[ I_1 = \\int_{-1}^2 3x^2 dx. \\] It is easy to verify that \\(\\max_{\\Omega} \\; f(x) = 12\\) and \\(\\min_{\\Omega} \\; f(x) = 0\\) where \\(\\Omega = (-1, 2)\\) and therefore we have that \\[I_1 \\leq I_2 = \\int_{-1}^2 \\int_{0}^{12} dy dx = 36.\\] The following code and image illustrates the situation: n_step = 10^3 x = seq(from = -1.35, to = 2.35, length.out = n_step) x_in_omega = x[x &gt;= -1 &amp; x &lt;= 2] y = 3*x^2 col_I1 = hcl(h = seq(15, 375, length = 3), l = 65, c = 100, alpha = 0.2)[2] plot(NA, xlim = range(x), ylim = range(y), xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;) grid() polygon(c(-1, 2, 2, -1), c(0, 0, 12, 12), border = &quot;red3&quot;, lty = 2, lwd = 2) polygon(c(x_in_omega, rev(x_in_omega)), c(rep(0, length(x_in_omega)), rev(y[x &gt;= -1 &amp; x &lt;= 2])), border = &quot;NA&quot;, col = col_I1) lines(x, y, col = &quot;blue4&quot;) legend(&quot;topleft&quot;, c(expression(I[1]), expression(I[2]), &quot;f(x)&quot;), bty = &quot;n&quot;, pch = c(NA, 15, NA), lwd = c(2, NA, 1), lty = c(2, NA, 1), pt.cex = c(2, 2, NA), col = c(&quot;red3&quot;, col_I1, &quot;blue4&quot;)) Suppose that we generate (uniformly) a large number points, with coordinates (\\(U_1\\), \\(U_2\\)) inside the red region. We assume that \\(U_1 \\sim \\mathcal{U}(-1, 2)\\) and \\(U_2 \\sim \\mathcal{U}(0, 12)\\). The function below also t #&#39; Generate random points with bivariate uniform distribution #&#39; #&#39; @param x_range A vector of dimension 2 used to denote the parameter #&#39; of the uniform distribution on the x-axis. #&#39; @param y_range A vector of dimension 2 used to denote the parameter #&#39; of the uniform distribution on the y-axis. #&#39; @param B A positive number used to denote the number of simulation. #&#39; @param seed A number used to control the seed of the random number #&#39; generated by this function. #&#39; @return A B x 2 matrix containing the simulated points. #&#39; @examples #&#39; gen_points(x_range = c(0, 1), y_range = c(0, 2), B = 10) #&#39; gen_points(x_range = c(-10, 1), y_range = c(0, 0.1), B = 5) gen_points = function(x_range, y_range, B, seed = 1291){ # A few checks # Check x_range if (length(x_range) != 2 || x_range[1] &gt;= x_range[2]){ error(&quot;x_range is uncorrely specified&quot;) } # Check y_range if (length(y_range) != 2 || y_range[1] &gt;= y_range[2]){ error(&quot;y_range is uncorrely specified&quot;) } # Check B if (B &lt; 1){ error(&quot;B is uncorrely specified&quot;) } # Set seed set.seed(seed) # Let&#39;s draw some uniforms mat_out = matrix(runif(2*B), B, 2) mat_out[, 1] = mat_out[, 1]*diff(x_range) + x_range[1] mat_out[, 2] = mat_out[, 2]*diff(y_range) + y_range[1] mat_out } B = 500 mc_points = gen_points(x_range = c(-1, 2), y_range = c(0, 12), B = B) n_step = 10^3 x = seq(from = -1.35, to = 2.35, length.out = n_step) x_in_omega = x[x &gt;= -1 &amp; x &lt;= 2] y = 3*x^2 col_I1 = hcl(h = seq(15, 375, length = 3), l = 65, c = 100, alpha = 0.2)[2] plot(NA, xlim = range(x), ylim = range(y), xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;) grid() polygon(c(-1, 2, 2, -1), c(0, 0, 12, 12), border = &quot;red3&quot;, lty = 2, lwd = 2) polygon(c(x_in_omega, rev(x_in_omega)), c(rep(0, length(x_in_omega)), rev(y[x &gt;= -1 &amp; x &lt;= 2])), border = &quot;NA&quot;, col = col_I1) lines(x, y, col = &quot;blue4&quot;) points(mc_points, pch = 19, cex = 0.5) legend(&quot;topleft&quot;, c(expression(I[1]), expression(I[2]), &quot;f(x)&quot;), bty = &quot;n&quot;, pch = c(NA, 15, NA), lwd = c(2, NA, 1), lty = c(2, NA, 1), pt.cex = c(2, 2, NA), col = c(&quot;red3&quot;, col_I1, &quot;blue4&quot;)) "],
["chap-graphs.html", "Chapter 7 Graphics", " Chapter 7 Graphics In this chapter we discuss graphics with R. In this chapter, we will as an example the function sin(x) in the range \\(0, 2\\pi)\\). nb_points &lt;- 60 x &lt;- seq(from = 0, to = 4*pi, length.out = nb_points) y &lt;- sin(x) plot(x, y) In general, we found that the following approach is quite usefull, Step 1: Construct graph “frame”, i.e. basically an empty graph with the right title, labels, axis and so on. Often, this can be achieve with plot(). It is also useful to grid. Here is an example: plot(NA, xlim = range(x), ylim = range(y), main = &quot;my title&quot;, xlab = &quot;my xlab&quot;, ylab = &quot;my ylab&quot;, col.main = &quot;red&quot;, col.lab = &quot;darkgreen&quot;, col.axis = &quot;darkblue&quot;, cex.main = 2, cex.lab = 1, cex.axis = 0.5) grid() full link "],
["r-markdown.html", "Chapter 8 R Markdown 8.1 Including Plots", " Chapter 8 R Markdown RMarkdown is a framework that provides a literate format for data science. It can be used to save and execute R code within RStudio, and also is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 8.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "]
]
