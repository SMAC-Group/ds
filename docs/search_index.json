[
["index.html", "An Introduction to Statistical Programming Methods with R Chapter 1 Introduction 1.1 R and RStudio 1.2 Basic Probability and Statistics with R 1.3 Main References 1.4 License", " An Introduction to Statistical Programming Methods with R Matthew Beckman, Stéphane Guerrier, Justin Lee &amp; Roberto Molinari 2017-08-20 Chapter 1 Introduction This book is currently under development and has been designed as a support for students who are following (or are interested in) courses that provide the basic knowledge to master “statistical programming” with R. By the latter we mean that area of computer programming which focuses on the implementation of methods that not only manage data but also extract meaningful information from it. The importance of this area of research comes from the increased collection of data from different sources such as academic research, public institutions and private companies that has required a corresponding increase in data management and analysis tools. Consequently, the need to develop applications and methods that are able to deliver these tools has led to a surge in the demand for expertise not only in computer programming but also in statistical and numerical analysis. Indeed, while it is essential to master the basics of programming to build the necessary software, it is now also paramount to understand the programming tools that can effectively respond to the need of finding, extracting, and analyzing the information to achieve the required goals. Within the above framework, the statistical software R has seen a rise in use due to its flexibility as an efficient language that builds a bridge between software development and data analysis. There are of course many other programming languages that have different advantages over R but, as you will see, one strength of R is the facility to develop and quickly adapt to the different needs coming from the data management and analysis community while at the same time making use of other languages in order to deliver computationally efficient solutions (as well as other interesting features described below). This book intends to present the basic tools to statistical programming and software development using the wide variety of tools made available through R, from method-specific packages to version control programs. The general goals of the book are therefore the following: understand data structures in order to appropriately manage data, computer memory and computations; manipulate data structures through controls, instructions, and tailored functions in order to achieve a desired output; create new software tools (packages and web applications) that accommodate a previously unmet need; learn how to manage software development via version control tools (e.g., GitHub) and create documentation for this software (with embedded code) to allow others to make use of the software. All these goals are common to any basic programming course, however all these will be heavily focused on the use and development of statistical tools. In fact, as highlighted earlier, it has become increasingly important to include statistical methodologies within the programming framework thereby allowing software to not only manage data efficiently but also to extract and analyse data in an appropriate manner while doing so. The rest of this introductory chapter will present the R software by explaining why it is used for this book and describing the basic notations and tools that need to be known in order to better grasp its contents. This document is under development and it is therefore preferable to always access the text online to be sure you are using the most up-to-date version. Due to its current development, you may encounter errors ranging from broken code to typos or poorly explained topics. If you do, please let us know! Simply add an issue to the GitHub repository used for this document (which can be accessed here https://github.com/SMAC-Group/ds/issues) and we will make the changes as soon as possible. In addition, if you know RMarkdown and are familiar with GitHub, make a pull request and fix an issue yourself, otherwise, if you’re not familiar with these tools, they will be explained later on in the book itself. 1.1 R and RStudio The statistical computing language R has become commonplace for many applications in industry, government, and academia. Having started as an open-source language to make available different statistics and analytical tools to researchers and the public, it steadily developed into one of the major software languages which not only allows to develop up-to-date, sound, and flexible analytical tools, but also to include these tools within a framework which is well-integrated with other important programming languages, communication, and version-control features. The latter is also possible thanks to the development of the RStudio interface which provides a pleasant and functional user-interface for R as well as an efficient Integrated Development Environment (IDE) in which different programming languages, web-applications and other important tools are available to the user. In order to illustrate the relationship between R &amp; RStudio in statistical programming, one might think of a car analogy in which R would be like the engine and RStudio might be like leather seats. R is doing the work (for the most part), and RStudio generally is making you more comfortable while you use R. 1.1.1 Why R? There are many reasons to use R. Two compelling reasons are that R is both free as in “free pizza”, and free as in “free speech”). Free–like “free pizza”–means that there is never a need to pay for any part of the R software, or contributed packages (i.e. add-on modules). Free–like “free speech”–means that there are very few restrictions on how R can be used or barriers to those who would like to contribute packages (i.e. add-on modules). The fact that is a free and open-source software which per se does not necessarily imply that it is a good software (although it is also that). The reason why this is an important feature consists in the fact that the results of any code or program developed in the R environment can easily be replicated therefore ensuring accessibility and transparency for the general user. More importantly however, this replicability of results is also accompanied by a wide variety of packages that are made available through the R environment in which users can find a diversity of codes, functions and features that are designed to tackle a large amount of programming and analytical tasks. Moreover, these packages are relatively simple to create and are extremely useful for code-sharing purposes since they enclose the codes, functions and external dependencies that allow anyone to install any of these features all at once in easy and efficient manner. In addition to its accessibility and code-sharing features, R has acquired visibility and importance mainly due to the cutting-edge tools that it makes available to the general user. Indeed, a growing area of research both in academia and in industry is Statistics and Machine Learning through which it is possible to find, extract and make an efficient use of the increasing amount of data and information being collected. All the latest methods and approaches going from data-mining techniques to predictive analysis are available in R and, due to its nature, all future methods and approaches will be made available to all users through R. For this reason, any individual, company or organization has a keen interest in acquiring and developing expertise in R since it makes available the most appropriate tools for any data-based analysis and decision-making process. Like any other software, there are of course some drawbacks with using R. First, the presence of an extended amount of user-contributed packages can make its usage and bug-reporting problematic. Although this does not represent a major problem since many forums exist and solutions are usually quickly fixed, there can be many issues concerning package updates or deletions that can create problems for other existing packages that depend on them. This is fairly rare, but there can consequently be problems in the use of packages that become obsolete and need to be fixed due to dependency issues. Another drawback consists in the extensive use of computer memory that R entails through its commands which generally give little relevance to this issue. However, many different solutions are being developed which deal with this problem along with the increased memory made available by current operating systems. In the perspective of improving the usage of computer memory, R has been developing efficient and “seemless” connections with high-performance languages which allow functions and packages to make use of them thereby greatly lightening and accelerating computations made through R. An important example of this is given by the connections made available to the C++ language. In this book we will discuss the connections with this language that are particularly well implemented, but other high-performance languages can be used such as C and FORTRAN. 1.1.2 Getting started with R As mentioned earlier, R can be thought of as a programming language as well as a software environment for statistical programming. Since it is a free and open-source software, all you will need to do is to download it from the following link: R Once you’ve downloaded and installed R on your computer you will be able to start using the programming language and packages that the R environment provides. Nevertheless, to make full use of the latest developments and features of this software, in this book we recommend using the IDE called RStudio which can be downloaded from the following link: RStudio You cannot use RStudio without having installed R on your computer. 1.1.3 About RStudio RStudio is a customizable IDE for the R enviornment where the user can have easy access to plots, data, help, files, objects and many other features that are useful to work efficiently with R. For the most part, RStudio provides everything the R user will need in a self-contained, and well-organized environment. Moreover, it is possible to create “projects” in which it is possible to create a dedicated environment space for sets of specific functions and files aimed to deal with various tasks. Rob what about a video here to introduce RStudio??? In addition, RStudio provides embedded functionality to utilize collaborative version-control software including GitHub &amp; Subversion as well as a set of powerful tools to save and communicate results (whether they be simulations, data analysis, or presenting and making available a new package to other users). Some examples of these tools are Rmarkdown which can be used respectively to integrate written narrative with embedded R code and other content, as well as and Shiny Web Apps which can provide an interactive user-friendly interface that permits a user to actively engage with a wide variety of tools built in R without the need to encounter raw R code. GitHub and Rmarkdown will be the object of a more in-depth description in the first chapters of this book in order to provide the reader with the version-control and annotation tools that can be useful for the following chapters of this book. 1.1.4 Conventions Throughout this book, R code will be typeset using a monospace font which is syntax highlighted. For example: a = pi b = 0.5 sin(a*b) Similarly, R output lines (that usally appear in your Console) will begin with ## and will not be syntax highlighted. The output of the above example is the following: ## [1] 1 Aside from R code and its outputs, this book will also insert some boxes that will draw the reader’s attention to important, curious, or otherwise informative details. An example of these boxes was seen at the beginning of this introduction where an important aspect was pointed out to the reader regarding the “under construction” nature of this book. Therefore the following boxes and symbols can be used to represent information of different nature: This is an important piece of information. This is some additional information that could be useful to the reader. This is something that the reader should pay caution to but should not create major problems if not considered. This is a warning which should be heeded by the reader to avoid problems of different nature. This is a tip for the reader when following or developing something based on this book. 1.1.5 Simple calculations A basic aspect to underline about the R environment is that it serves as an advanced calculator which therefore allows also for simple calculations. In the table below we show a few examples of such calculations where the first column gives a mathematical expression (calculation), the second gives the equivalent of this expression in R and finally in the third column we can find the result that is output from R. Math. R Result 2+2 2+2 4 \\(\\frac{4}{2}\\) 4/2 2 \\(3 \\cdot 2^{-0.8}\\) 3*2^(-0.8) 1.723048 \\(\\sqrt{2}\\) sqrt(2) 1.414214 \\(\\pi\\) pi 3.141593 \\(\\ln(2)\\) log(2) 0.6931472 \\(\\log_{3}(9)\\) log(9, base = 3) 2 \\(e^{1.1}\\) exp(1.1) 3.004166 \\(\\cos(\\sqrt{0.9})\\) cos(sqrt(0.9)) 0.5827536 1.1.6 Getting help In the previous section we presented some examples on how R can be used as a calculator and we have already seen several functions such as sqrt() or log(). To obtain documentation about a function in R, simply put a question mark in front of the function name (or just type help() around the function name), or use the search bar on the “Help” tab in your RStudio window, and its documentation will be displayed. For example, if you are interested in learning about the function log() you could simply type: ?log which will display something similar to: The R documentation is written by the author of the package. For mainstream packages in widespread use, the documentation is almost always quite good, but in some cases it can be quite technical, difficult to interpret, and possibly incomplete. In these cases, the best solution to understand a function is to search for help on any search engine. Often a simple search like “side by side boxplots in R” or “side by side boxplots in ggplot2” will produce many useful results. The search results often include user forums such as “CrossValidated” or “StackExchange” in which the questions you have about a function have probably already been asked and answered by many other users. You can often use the error message to search for answers about a problem you may have with a function. 1.1.7 Installing packages R comes with a number of built-in functions but one of its main strengths is that there is a large number of packages on an ever-increasing range of subjects available for you to install. These packages provide additional functions, features and data to the R environement. If you want to do something in R that is not available by default, there is a good chance that there are packages that will respond to your needs. In this case, an appropriate way to find a package in R is to use the search option in the CRAN repository which is the official network of file-transfer protocols and web-servers that store updated versions of code and documentation for R (see CRAN website). Another general approach to find a package in R is simply to use a search engine in which to type the keywords of the tools you are looking for followed by “R package”. R packages can be installed in various ways but the most widely used approach is through the install.packages() function. Another way is to use the “Tools -&gt; Install Packages…” path from the dropdown menus in RStudio or clicking on the “install” button in the “Packages” pane in the RStudio environment. The install.packages() function is very straight-forward and transcends any platform for the R environment. It is noteworthy that this approach assumes that the desired package(s) are available within the CRAN repository. This is very often the case, but there is a growing number of packages that are under-development or completed and are made available through other repositories. In the latter setting, Chapter 02 will show other ways of installing packages from a commonly used repository called “GitHub”. Sticking momentarily to the packages available in the CRAN repository, the use of the install.packages() is quite simple. For example, if you want to install the package devtools you can simply write: install.packages(&quot;devtools&quot;) Once a package is installed it is not directly usable within your R session. To do so you will have to “load” the package into your current R session which is generally done through the function library(). For example, after having installed the devtools package, in order to use it within your session you would write: library(devtools) Once this is done, all the functions and documentation of this package are available and can be used within your current session. However, once you close your R session, all loaded packages will be closed and you will have to load them again if you want to use them in a new R session. Please notice that although packages need to be loaded at each session if you want to use them, they need to be installed only once. The only exception to this rule is when you need to update the package or reinstall it for some reason. One of the main packages that is required for this class would be our STAT 297 package, that contains all the necessary packages and functions that will be utilized in this course. Run the following code to install the package directly from GitHub. install_github(&quot;SMAC-Group/stat297&quot;) 1.1.8 Additional References There are many more elements in RStudio, and we encourage you to use the RStudio Cheatsheet as a reference. 1.2 Basic Probability and Statistics with R The R environment provides an up-to-date and efficient programming language to develop different tools and applications. Nevertheless, its main functionality lies in the core statistical framework and tools that consistute the basis of this language. Indeed, this book aims at introducing and describing the methods and approaches of statistical programming which therefore require a basic knowledge of Probability and Statistics in order to grasp the logic and usefulness of the features presented in this book. For this reason, we will briefly take the reader through some of the basic functions that are available within R to obtain probabilities based on parametric distributions, compute summary statistics and understand basic data structures. The latter is just an introduction and a more in-depth description of different data structures will be given in a future chapter. 1.2.1 Probability Distributions Probability distributions can be uniquely characterized by different functions such as, for example, their density or distribution functions. Based on these it is possible to compute theoretical quantiles and also randomly sample observations from them. Replacing the R syntax for a given probability distribution with the general syntax name, all these functions and calculations are made available in R through the built-in functions: dname calculates the value of the density function (pdf); pname calculates the value of the distribution function (cdf); qname calculates the value of the theoretical quantile; rname generates a random sample from a particular distribution. Note that, when using these functions in practice, name is replaced with the syntax used in R to denote a specific probability distribution. For example, if we wish to deal with a Uniform probability distribution, then the syntax name is replaced by unif and, furthering the example, to randomly generate observations from a uniform distribution the function to use will be therefore runif. R allows to make use of these functions for a wide variety of probability distributions that include, but are not limited to: Gaussian (or Normal), Binomial, Chi-square, Exponential, F-distribution, Geometric, Poisson, Student-t and Uniform. In order to get an idea of how these functions can be used, below is an example of a problem that can be solved using them. 1.2.1.1 Example: Normal Test Scores of College Entrance Exam Assume that the test scores of a college entrance exam follows a Normal distribution. Furthermore, suppose that the mean test score is 70 and that the standard deviation is 15. How would we find the percentage of students scoring 90 or more in this exam? In this case, we consider a random variable \\(X\\) that is normally distributed as follows: \\(X \\sim N(\\mu=70, \\sigma^2=225)\\) where \\(\\mu\\) and \\(\\sigma^2\\) represent the mean and variance of the distribution respectively. Since we are looking for the probability of students scoring higher than 90, we are interested in finding \\(\\mathbb{P}(X &gt; x=90)\\) and therefore we look at the upper tail of the Normal distribution. To find this probability we need the distribution function (pname) for which we therefore replace name with the R syntax for the Normal distribution: norm. The distribution function in R has various parameters to be specified in order to compute a probability which, at least for the Normal distribution, can be found by typing ?pnorm in the Console and are: q: the quantile we are interested in (e.g. 90); mean: the mean of the distribution (e.g. 70); sd: the standard deviation of the distribution (e.g. 15); lower.tail: a boolean determining whether to compute the probability of being smaller than the given quantile (i.e. \\(\\mathbb{P}(X \\leq x)\\)) which requires the default argument TRUE or larger (i.e. \\(\\mathbb{P}(X &gt; x)\\)) which requires to specify the argument FALSE. Knowing these arguments, it is now possible to compute the probability we are interested in as follows: pnorm(q = 90, mean = 70, sd = 15, lower.tail = FALSE) ## [1] 0.09121122 As we can see from the output, there is roughly a 9% probability of students scoring 90 or more on the exam. 1.2.2 Summary Statistics While the previous functions deal with theoretical distributions, it is also necessary to deal with real data from which we would like to extract information. Supposing–as is often the case in applied statistics–we don’t know from which distribution it is generated, we would be interested in understanding the behavior of the data in order to eventually identify a distribution and estimate its parameters. The use of certain functions varies according to the nature of the inputs since these can be, for example, numerical or factors. 1.2.2.1 Numerical Input A first step in analysing numerical inputs is given by computing summary statistics of the data which, in this section, we can generally denote as x (we will discuss the structure of this data more in detail in the following chapters). For central tendency or spread statistics of a numerical input, we can use the following R built-in functions: mean calculates the mean of an input x; median calculates the median of an input x; var calculates the variance of an input x; sd calculates the standard deviation of an input x; IQR calculates the interquartile range of an input x; min calculates the minimum value of an input x; max calculates the maximum value of an input x; range returns a vector containing the minimum and maximum of all given arguments; summary returns a vector containing a mixture of the above functions (i.e. mean, median, first and third quartile, minimum, maximum). 1.2.2.2 Factor Input If the data of interest is a factor with different categories or levels, then different summaries are more appropriate. For example, for a factor input we can extract counts and percentages to summarize the variable by using table. Using functions and data structures that will be described in the following chapters, below we create an example dataset with 90 observations of three different colors: 20 being Yellow, 10 being Green and 50 being Blue. We then apply the table function to it: table(as.factor(c(rep(&quot;Yellow&quot;, 20), rep(&quot;Green&quot;, 10), rep(&quot;Blue&quot;, 50)))) ## ## Blue Green Yellow ## 50 10 20 By doing so we obtain a frequency (count) table of the colors. 1.2.2.3 Dataset Inputs In many cases, when dealing with data we are actually dealing with datasets (see Chapter 03) where variables of different nature are aligned together (usually in columns). For datasets there is another convenient way to get simple summary statistics which consists in applying the function summary to the dataset itself (instead of simply a numerical input as seen earlier). As an example, let us explore the Iris flower dataset contained in the R built-in datasets package. The data set consists of 50 samples from each of three species of Iris (Setosa, Virginica and Versicolor). Four features were measured from each sample consisting in the length and the width (in centimeters) of the both sepals and petals. This dataset is widely used as an example since it was used by Fisher to develop a linear discriminant model based on which he intended to distinguish the three species from each other using combinations of these four features. Using this dataset, let us use the summary function on it to output the minimum, first quartile and thrid quartile, median, mean and maximum statistics (for the numerical variables in the dataset) and frequency counts (for factor inputs). summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## 1.3 Main References This is not the first (or the last) book that has been written explaining and describing statistical programming in R. Indeed, this can be seen as a book that brings together and reorganizes information and material from other sources structuring and tailoring it to a course in basic statistical programming. The main references (which are far from being an exhaustive review of literature) that can be used to have a more in-depth view of different aspects treated in this book are: Wickham (2014) : a more technical and advanced introduction to R; Wickham (2015) : basic building blocks of building packages in R; Xie (2015) : an overview of document generation in R; 1.4 License You can redistribute it and/or modify this book under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA) 4.0 License. References "],
["rmarkdown.html", "Chapter 2 RMarkdown 2.1 YAML Metadata 2.2 Text 2.3 Code Chunks 2.4 Render Output", " Chapter 2 RMarkdown RMarkdown is a framework that provides a literate programming format for data science. It can be used to save and execute R code within RStudio, and also as a simple formatting syntax for authoring HTML, PDF, ODT, RTF, and MS Word documents as well as seamless transitions between available formats. The name “markdown” is an intentional contrast to other “markup” languages–e.g., hypertext markup language (HTML)–which require syntax that can be quite difficult to decipher for the uninitiated. One aim of the markdown paradigm is a syntax that is as human-readable as possible. “RMarkdown” is an implementation of the “markdown” language dsigned to accommodated embedded R code. What is literate programming? Literate programming is the notion for programmers of adding narrative context with code to produce documententation for the program simultaneously. Consequently, it is possible to read through the code with explanations so that any viewer can follow through the presentation. RMarkdown offers a simple tool that allows to create reports or presentation slides in a reproducible manner with collateral advantages such as avoiding repetitive tasks by, for example, changing all figures when data are updated. What is reproducible research? Reproducible research or reproducible analysis is the notion that an experiment’s whole process, including collecting data, performing analysis, and producing output can be reproduced the same way by someone else. Building non-reproducible experiments has been a problem both in research and in the industry, and having such an issue highly decreases the credibility of the author’s findings and potentially, the author. In essence, allowing for reproducible research implies that anyone could run the code (knit the document, etc.) and obtain the same exact results as the original research and RMarkdown is commonly used to address this issue. JUSTIN: It could be nice to add a sketch to show what is going on in RMarkdown (maybe data -&gt; RMarkdown -&gt; output, under RMarkdown show that data importing/cleaning, analysis, writing slide/report content is done, output show the kind of output, from slides, html, this book, latex document (sweave) and so on). Finally, we could add a video somewhere in the intro to present RMarkdown in RStudio. What do you think? An RMarkdown is a plain text file that contains three different aspects: YAML metadata Code Chunks Text 2.1 YAML Metadata YAML stands for YAML Ain’t Markup Language, and is used to specify document configurations and properties such as name, date, output format, etc. The (optional) YAML header surrounded before and after by “—” on a dedicated line. You can also include additional formatting options, such as a table of contents, or even a custom CSS style template which can utilized to further enhance presentation. For the purpose of the class, the default options should be sufficient. Below is an example knit output of the above RMarkdown file. The default output above is an html_document format. However, this can be specified as well, such as pdf_document. The pdf format, however, requires additional installation and configuration of a TeX distribution such as MikTeX. Once available, the user can also include raw LaTeX and even define LaTeX macros in the RMarkdown document if necessary. Subsections To make your sections numbered as sections and subsections, make sure you specify number_sections: yes as part of the YAML Metadata as shown below. 2.2 Text In addition, due to its literate nature, text will be an essential part in explaining your analysis. With RMarkdown, we can specify custom text formatting, such as with emphasis such as italics, bold, or code style. To understand how to format text, our previous would be as follows in RMarkdown: With RMarkdown, we can specify custom text formatting, such as with emphasis such as *italics*, **bold**, or even a `code style`. Headers As seen above, headings are preceded with a #. A single # produces the largest heading text; to output smaller headings, simply add more #! Heading level also impacts section and subsection nesting in documents and tables of contents, as well as slide breaks in presentation formats. Lists Lists can be extremely convienient to make text more readable or to take course notes during class. RMarkdown allows to create different list structures as shown in the code below: * You can create bullet points by using symbols such as *, +, or -. + simply add an indent or four preceding spaces to indent a list. + You can manipulate the number of spaces or indents to your liking. - Like this. * Here we go back to the first indent. 1. To make the list ordered, use numbers. 1. We can use one again to continue our ordered list. 2. Or we can add the next consecutive number. which delivers the following list structure: You can create bullet points by using symbols such as *, +, or -. Simply add an indent or four preceding spaces to indent a list. You can manipulate the number of spaces or indents to your liking. Like this. Here we go back to the first indent. To make the list ordered, use numbers instead of symbols. Here we write the next consecutive number to continue. Add more content by indenting like before. Hyperlinks To add hyperlinks with the full link, (ex: https://google.com/) you can follow the syntax below: &lt;https://google.com/&gt; whereas to add hyperlinks with a custom link title, (ex: Google) follow the syntax below: [Google](https://google.com) Blockquotes Same thing for citation… What do you think? Use the &gt; character in front of a line, just like in email. Use it if you’re quoting a person, a song or whatever. Pictures To add a picture with captions, follow the syntax below: ![Eberly College of Science Banner](http://science.psu.edu/psu_eberly_blue.png) Eberly College of Science Banner Else, to add a picture without any captions, follow the syntax below: ![](http://kefalosandassociates.com/wp-content/uploads/2015/07/facebook-banner.png) Cache Depending on the complexity of calculations in your embedded R code, it may be convenient to avoid re-running the computations (which could be lengthy) each time you knit the document together. For this purpose, it possible to specify an additional argument for your embedded R code which is the cache argument. By default this argument is assigned the value FALSE and therefore the R code is run every time your document is compiled. However, if you specify this argument as cache = TRUE, then the code is only run the first time the document is compiled while the following times it simply stores and presents the results of the computations when the document was first compiled. Let us consider an example where we want to embed an R code with a very simple operation such as assigning the value of 2 to an object that we call a (that we saw earlier). This is clearly not the best example since this operation runs extremely quickly and there is no visible loss in document compilation time. However, we will use it just to highlight how the cache argument works. Therefore, if we want to avoid running this operation each time the document is compiled, then we just embed our R code as follows: a = 2 You will notice that we called this chunk of embedded R code computeA and the reason for this will become apparent further on. Once we have done this we can compile the document that will run this operation and store its result. Now, if we compile the document again (independently from whether we made changes to the rest of the document or not) this operation will not be run and the result of the previous (first) compiling will be presented. However, if changes are made to the R code which has been “cached”, then the code will be run again and this time its new result will be stored for all the following compilings until it is changed again. This argument can therefore be very useful when computationally intensive R code is embedded within your document. Nevertheless it can suffer from a drawback which consists in dependencies of your “cached” R code with other chunks within the document. In this case, the other chunks of R code can be modified thereby outputting different results but these will not be considered by your “cached” R code. As an example, suppose we have another chunk of R code that we can “cache” and that takes the value of a from the previous chunk: (d = 2*a) ## [1] 4 In this case, the output of this chunk will be ## 4 since a = 2 (from the previous chunk). What happens however if we modify the value of a in the previous chunk? In this case, the previous chunk will be recomputed but the value of d (in the following chunk) will not be updated since it has stored the value of 4 and it is not recomputed since this chunk has not been modified. To avoid this, a solution is to specify the chunks of code that the “cached” code depends on. This is why we initially gave a name to the first chunk of code (“computeA”) so as to refer to it in following chunks of “cached” code that depend on it. To refer to this code you need to use the option dependson as follows: d = 2*a In this manner, if the value of a changes in the first chunk, the value of d will also change but will be stored until either the computeA chunk or the latter chunk is modified. LaTeX What is LaTeX? LaTeX is a document preparation system that uses plain text as opposed to formatted text used for applications such as Microsoft Word. It is widely used in academia as a standard for the publication of scientific documents. It has control over large documents containing sectioning, cross-references, tables and figures. LaTeX in RMarkdown Unlike a highly formatted word processor, we cannot produce equations by clicking on symbols. As data scientists we need to explain distributions and equations that are behind the methods we present. Within the text section of an RMarkdown document you can include LaTeX format text to output different forms of text, mainly equations and mathematical expressions. Inline mathematical expressions can be added using the syntax: $math expression$. For example, if we want to write “where \\(\\alpha\\) is in degrees” we would write: &quot;where $\\alpha$ is in degrees&quot;. Using a slightly different syntax (i.e. $$math expression$$) we can obtain centered mathematical expressions. For example, the binomial probability in LaTeX is written as $$f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}$$ which is output as: \\[f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\\] An introduction to the LaTeX format can be found here if you want to learn more about the basics. An alternative can be to insert custom LaTeX formulas using a graphical interface such as codecogs. Cross-referencing Sections You can also use the same syntax \\@ref(label) to reference sections, where label is the section identifier (ID). By default, Pandoc will generate IDs for all section headers, e.g., # Hello World will have an ID hello-world. To call header hello-world as a header, we type \\@ref(hello-world) to cross-reference the section. In order to avoid forgetting to update the reference label after you change the section header, you may also manually assign an ID to a section header by appending {#id} to it. Citations and Bibliography Citations and bibliographies can automatically be generated with RMarkdown. In order to use this feature we first need to create a “BibTex” database which is a simple plain text file (with the extension “.bib”) where each reference you would like to cite is entered in a specific manner. To illustrate how this is done, let us take the example of a recent paper where two researchers from Oxford University investigated the connection between the taste of food and various features of cutlery such as weight and colour (calling this phenomenon the “taste of cutlery”). The bibtex “entry” for this paper is given below: @article{harrar2013taste, title={The taste of cutlery: how the taste of food is affected by the weight, size, shape, and colour of the cutlery used to eat it}, author={Harrar, Vanessa and Spence, Charles}, journal={Flavour}, volume={2}, number={1}, pages={21}, year={2013}, publisher={BioMed Central} } This may look like a complicated format to save a reference but there is an easy way to obtain this format without having to manually fill in the different slots. To do so, go online and search for “Google Scholar” which is a search engine specifically dedicated to academic or other types of publications. In the latter search engine you can insert keywords or the title and/or authors of the publication you are interested in and find it in the list of results. In our example we search for “The tast of cutlery” and the publication we are interested in is the first in the results list. Below every publication in the list there is a set of options among which the one we are interested in is the “Cite” option that should open a window in which a series of reference options are available. Aside from different reference formats that can be copied and pasted into your document, at the bottom of the window you can find another set of options (with related links) that refer to different bibliography managers. For “.bib” files we are interested in the “BibTex” option and by clicking on it we will be taken to another tab in which the format of the reference we want is provided. All that needs to be done at this point is to copy this format (that we saw earlier in this section) and paste in the “.bib” file you created and save the changes. However, your Rmarkdown document does not know about the existence of this bibliography file and therefore we need to insert this information in the YAML metadata at the start of our document. To do so, let us suppose that you named this file “biblio.bib” (saved in the same location as your Rmarkdown document). All that needs to be done is to add another line in the YAML metadata with bibliography: biblio.bib and your Rmarkdown will now be able to recognise the references within your “.bib” file. There are also a series of other options that can be specified for the bibliography such as its format or the way references can be used within the text (for a more detailed overview the BibTex environment and managing references in Rmarkdown see, for example, this link and this link respectively). Once the “.bib” file has been created and has been linked to your Rmarkdown document through the details in the YAML metadata, you can now start using the references you have collected in the “.bib” file. To insert these references within your document at any point of your text you need to use the name that starts the reference field in your “.bib” file and place it immediately after the @ symbol (without spaces). So, for example, say that we wanted to cite the publication on the “taste of cutlery”: in your Rmarkdown all you have to do is to type @harrar2013taste at the point where you want this citation in the text and you will obtain: Harrar and Spence (2013). The user can also change the name that is used to call the desired reference as long as the same name is used to cite it in the Rmarkdown document and that this name is not the same as another reference. The references in the “.bib” file will not appear in the references that are output from the Rmarkdown compiling procedure unless they are specifically used within the Rmarkdown document. Additional References: Intro to bibtex Ref on ref for rmd Tables For simple tables, we can be manually insert values as such, +---------------+---------------+--------------------+ | Fruit | Price | Advantages | +===============+===============+====================+ | *Bananas* | $1.34 | - built-in wrapper | | | | - bright color | +---------------+---------------+--------------------+ | Oranges | $2.10 | - cures scurvy | | | | - **tasty** | +---------------+---------------+--------------------+ to produce: Fruit Price Advantages Bananas $1.34 built-in wrapper bright color Oranges $2.10 cures scurvy tasty As an alternative we can use the simple graphical user interface online. For more extensive tables, we create dataframe objects and project them using knitr::kable(), which we will explain later. Additional References There are many more elements to creating a useful report using RMarkdown, and we encourage you to use the Rmarkdown Cheatsheet as a reference. 2.3 Code Chunks This is where you enter your code. You can quickly insert chunks like these into your file with the keyboard shortcut Ctrl + Alt + I (OS X: Cmd + Option + I) the Add Chunk command in the editor toolbar by typing the chunk delimiters ```{} and ```. 2.3.1 Code Chunk Options In the third code chunk above, we also configure chunk options. Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. This is a useful way to embed figures. More options can be referred from the Rmarkdown Cheatsheet and Yihui’s notes on knitr options. Here are some explanations of the most commonly used chunk options exerted from above: eval: (TRUE; logical) whether to evaluate the code chunk echo: (TRUE; logical or numeric) whether to include R source code in the output file warning: (TRUE; logical) whether to preserve warnings (produced by warning()) in the output like we run R code in a terminal (if FALSE, all warnings will be printed in the console instead of the output document) cache: (FALSE; logical) whether to cache a code chunk; Plot figure options: fig.path: (‘figure/’; character) prefix to be used for figure filenames (fig.path and chunk labels are concatenated to make filenames) fig.show: (‘asis’; character) how to show/arrange the plots fig.width, fig.height: (both are 7; numeric) width and height of the plot, to be used in the graphics device (in inches) and have to be numeric fig.align: (‘default’; character) alignment of figures in the output document (possible values are left, right and center fig.cap: (NULL; character) figure caption to be used in a figure environment In-line R The variables we store in a RMarkdown will stay within the environment. This means that we can call and manipulate them anywhere within the document. In Rmarkdown, we can reference the variable in-line in the text section by using r variable to show the value of our variable embedded within the sentence. Here is an example: a = 2 Above, we stored the value 2 into a. The value of $a$ is `r a`. This translates in R Markdown to “The value of \\(a\\) is 2.” 2.4 Render Output After you are done, run rmarkdown::render() or click the Knit button at the top of the RStudio scripts pane to save the output in your working directory. Click on the links below for more information on RMarkdown: RStudio RMarkdown tutorial R-blogger’s RMarkdown tutorial References "],
["github.html", "Chapter 3 GitHub 3.1 Version Control 3.2 Git and GitHub 3.3 Git Setup 3.4 Configuration 3.5 GitHub Setup 3.6 Student Developer Pack", " Chapter 3 GitHub You may have experienced a moment like this once in your life: After plenty hours of hard work, you only find yourself saving the same file over and over - even losing track of what changes you made. Version control is a software that keeps track of your changes for you. You know which saves you made where, and can even go back in time to revert to those changes. (Awesome!) 3.1 Version Control Version control is a system that records changes to a file or a set of files over time, so that you can save changes over time. It allows you to: keep the entire history of a file revert to a specific version of the file collaborate on the same platform with other people make changes without modifying the main file 3.2 Git and GitHub Among version control platform, Git is a powerful tool that is commonly used. GitHub is a commercial website that uses Git and stores local files into a “album” called a repository. For the purposes of this course, we will be using this platform. In addition, your GitHub profile will also serve as your data science resume that contain future projects that you save and commit so that employers know what you’re made of. 3.3 Git Setup To install Git, go to the website and choose the platform you are using. (eg. Windows/Mac/Linux/Solaris) 3.4 Configuration The first thing you should do when you install Git is to set your user name and email address. This is important because every Git commit uses this information, and it’s immutably baked into the commits you start creating: $ git config --global user.name &quot;John Doe&quot; $ git config --global user.email johndoe@example.com Again, you need to do this only once if you pass the –global option, because then Git will always use that information for anything you do on that system. If you want to override this with a different name or email address for specific projects, you can run the command without the –global option when you’re in that project. Many of the GUI tools will help you do this when you first run them. 3.5 GitHub Setup To begin, head over to GitHub and sign up with your PSU email address. (don’t worry, you can change your Username/Email anytime after this course!) On Step 2: Choose your plan choose the default plan (Unlimited public repositories for free.) and click Continue. You can either submit your information or skip Step 3: Tailor your experience. Once you have set up your profile, make sure you verify your email address. Also, send me your GitHub user ID (eg. jjl170) to jjl170@psu.edu. 3.6 Student Developer Pack Once you have set up your profile, go to this link and follow the steps below to set up a student developer pack discount request to GitHub. You will need this to make your own private repositories for free. I will add a tutorial to how to submit your homework and assignments using GitHub. "],
["data.html", "Chapter 4 Data Structures 4.1 Vectors 4.2 Matrix 4.3 Array 4.4 List 4.5 Dataframe", " Chapter 4 Data Structures There are different data types that are commonly used in R among which the most important ones are the following: Numeric (or double): these are used to store real numbers. Examples: -4, 12.4532, 6. Integer: examples: 2L, 12L. Logical (or boolean): examples: TRUE, FALSE. Character: examples: &quot;a&quot;, &quot;Bonjour&quot;. In R there are basically five types of data structures in which elements can be stored. A data structure is said to homogeneous if it only contains elements of the same type (for example it only contains character or numeric values) and heterogenous if it contains elements of more than one type. The five types of data structrures are commonly summarized in a table similar to the one below: Dimension Homogenous Heterogeneous 1 Vector List 2 Matrix Dataframe n Array To illustrate how to use these data structures, we will consider the simple data set of the five best male single tennis players (as ranked by ATP on 07-15-2017). The data are presented in the table below: Name Date of Birth Born Country ATP Ranking Prize Money Win Percentage Grand Slam Wins Andy Murray 15 May 1987 Glasgow, Scotland Great Britain 1 60,449,649 78.07 9 Rafael Nadal 3 June 1986 Manacor, Spain Spain 2 85,920,132 82.48 15 Stan Wawrinka 28 March 1985 Lausanne, Switzerland Switzerland 3 30,577,981 63.96 5 Novak Djokovic 22 May 1987 Belgrade, Serbia Serbia 4 109,447,408 82.77 12 Roger Federer 8 August 1981 Basel, Switzerland Switzerland 5 104,445,185 81.80 18 It can be noticed that this data set contains columns with a variety of data types and in the next sections we will focus on these different types separately. 4.1 Vectors A vector has three important properties: Type, which corresponds the “kind” of objects in contains. It is possible to use the function typeof() to evaluate the type of objects in a vector. Length, i.e. the number of elements in a vector. This information can be obtained using the function length(). Attributes, some additional metadata attached to a vector. The functions attr() and attributes() can be used to store and retrive attributes (more details can be found in Section 4.1.4) c() is a generic function which combines arguments to form a vector. All arguments are coerced to a common type which is the type of the returned value, and all attributes except names are removed. To learn more about what a specific function’s documentation, type ?yourfunction onto the R Studio Console. For example, let us consider the number of grand slams won by the five players we are considering which are reported in the eigth column of the dataset: grand_slam_win = c(9, 15, 5, 12, 18) To display the values stored in grand_slam_win we could simply enter the following in the R console: grand_slam_win ## [1] 9 15 5 12 18 Alteratively, we could have created and displayed the value by using () around the definition of the object itself as follows: (grand_slam_win = c(9, 15, 5, 12, 18)) ## [1] 9 15 5 12 18 Various forms of “nested concatenation” can be used to defined vectors, for example we could also define grand_slam_win as (grand_slam_win = c(9, c(15, 5, c(12, c(18))))) ## [1] 9 15 5 12 18 This approach is often used to assemble vectors in various ways. It is also possible to define vector with characters, for example we could define a vector with the player names as follows: (players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;)) ## [1] &quot;Andy Murray&quot; &quot;Rafael Nadal&quot; &quot;Stan Wawrinka&quot; &quot;Novak Djokovic&quot; ## [5] &quot;Roger Federer&quot; 4.1.1 Type We can evaluate the kind or type of elements that are stored in a vector using the function typeof(). For example, for the vectors we just created we obtain: typeof(grand_slam_win) ## [1] &quot;double&quot; typeof(players) ## [1] &quot;character&quot; This is a little surprising as all the elements in grand_slam_win are integers and it would therefore seem natural to expect this as an output of the function typeof(). This is because R considers any number as a “double” by default, except when adding the suffix L after an integer. For example: typeof(1) ## [1] &quot;double&quot; typeof(1L) ## [1] &quot;integer&quot; Therefore, we could express grand_slam_win as follows: (grand_slam_win_int = c(9L, 15L, 5L, 12L, 18L)) ## [1] 9 15 5 12 18 typeof(grand_slam_win_int) ## [1] &quot;integer&quot; Naturally, the difference between the two in general is relatively unimportant but we can see that grand_slam_win_int takes less “space” among the two. Indeed we have object.size(grand_slam_win) ## 88 bytes object.size(grand_slam_win_int) ## 72 bytes 4.1.2 Coercion As indicated earlier, a vector has a homogenous data structure meaning that it can only contain a single type among all the data types. Therefore, when more than one data type is provided, R coerces the data into a “shared” type. To identify this “shared” type we can use this simple rule: \\[\\begin{equation*} \\text{logical} &lt; \\text{integer} &lt; \\text{numeric} &lt; \\text{character}, \\end{equation*}\\] which simply means that if a vector contains more than one data type, the “shared” type will be that of the “largest” type according to the above equations. Here are a few examples: # Logical + integer (mix_logic_int = c(TRUE, 1L)) ## [1] 1 1 typeof(mix_logic_int) ## [1] &quot;integer&quot; # Logical + character (mix_logic_char = c(TRUE, &quot;Hi&quot;)) ## [1] &quot;TRUE&quot; &quot;Hi&quot; typeof(mix_logic_char) ## [1] &quot;character&quot; # Integer + numeric (mix_int_num = c(1, 1L)) ## [1] 1 1 typeof(mix_int_num) ## [1] &quot;double&quot; # Integer + character (mix_int_char = c(1L, &quot;Hi&quot;)) ## [1] &quot;1&quot; &quot;Hi&quot; typeof(mix_int_char) ## [1] &quot;character&quot; 4.1.3 Subsetting Naturally, it is possible to “subset” the values of in our vector in many ways. Essentially, there are four main ways of subsetting a vector. Here we’ll only discuss the first three: Positive Index: We can access or subset the \\(i\\)-th element of a vector by simply using grand_slam_win[i] where \\(i\\) is a positive number between 1 and length of the vector. # Accesing the first element grand_slam_win[1] ## [1] 9 # Accesing the third and first value grand_slam_win[c(3, 1)] ## [1] 5 9 # Duplicated indices yield duplicated values grand_slam_win[c(1, 1, 2, 2, 3, 4)] ## [1] 9 9 15 15 5 12 Negative Index: We remove elements in a vector using negative indices: # Removing the second obervation grand_slam_win[-2] ## [1] 9 5 12 18 # Removing the first and fourth obserations grand_slam_win[c(-1, -4)] ## [1] 15 5 18 Logical Indices: Another useful approach is based on logical operators: # Access the first and fourth observations grand_slam_win[c(TRUE, FALSE, FALSE, TRUE, FALSE)] ## [1] 9 12 Here we could add some remarks on weird cases, for example grand_slam_win[c(1.2, 3.4)] (which rounds things up) or grand_slam_win[c(-1, 2)] (which doesn’t work as “mixed” indices are not permitted). 4.1.4 Attributes Let’s suppose that we conducted an experiment under specific conditions, say a date and a place which should be stored as attributes of the object containing the results of this experiment. Indeed, objects can have arbitrary additional attributes that are used to store metadata on the object of interest. For example: attr(grand_slam_win, &quot;date&quot;) = &quot;07-15-2017&quot; attr(grand_slam_win, &quot;type&quot;) = &quot;Men, Single&quot; To display the vector with its attributes grand_slam_win ## [1] 9 15 5 12 18 ## attr(,&quot;date&quot;) ## [1] &quot;07-15-2017&quot; ## attr(,&quot;type&quot;) ## [1] &quot;Men, Single&quot; To only display the attributes we have attributes(grand_slam_win) ## $date ## [1] &quot;07-15-2017&quot; ## ## $type ## [1] &quot;Men, Single&quot; It is also possible to extract a specific attribute attr(grand_slam_win, &quot;date&quot;) ## [1] &quot;07-15-2017&quot; 4.1.5 Adding labels In some cases it might be useful to add labels to vectors. For example, we could define the vector grand_slam_win and use the player’s names as labels, i.e. (grand_slam_win = c(&quot;Andy Murray&quot; = 9, &quot;Rafael Nadal&quot; = 15, &quot;Stan Wawrinka&quot; = 5, &quot;Novak Djokovic&quot; = 12, &quot;Roger Federer&quot; = 18)) ## Andy Murray Rafael Nadal Stan Wawrinka Novak Djokovic Roger Federer ## 9 15 5 12 18 The main advantage of this approach is that the number of grand slams won can now be referred to by the player’s name. For example: grand_slam_win[&quot;Andy Murray&quot;] ## Andy Murray ## 9 grand_slam_win[c(&quot;Andy Murray&quot;,&quot;Roger Federer&quot;)] ## Andy Murray Roger Federer ## 9 18 All labels (players’ names in our case) can be obtained witht the function names, i.e. names(grand_slam_win) ## [1] &quot;Andy Murray&quot; &quot;Rafael Nadal&quot; &quot;Stan Wawrinka&quot; &quot;Novak Djokovic&quot; ## [5] &quot;Roger Federer&quot; 4.1.6 Useful functions with vectors The reason for extracting or creating vectors often lies in the need to collect information from them. For this purpose, a series of useful functions are available that allow to extract information or arrange the vector elements in a certain manner which can be of interest to the user. Among the most commonly used functions we can find the following ones length() sum() mean() sort() and order() whose name is self-explicative in most cases. For example we have length(grand_slam_win) ## [1] 5 sum(grand_slam_win) ## [1] 59 mean(grand_slam_win) ## [1] 11.8 To sort the players by number of grand slam wins, we could use the function order() which returns the position of the elements of a vector sorted in an ascending order, order(grand_slam_win) ## [1] 3 1 4 2 5 Therefore, we can sort the players in ascending order of wins as follows players[order(grand_slam_win)] ## [1] &quot;Stan Wawrinka&quot; &quot;Andy Murray&quot; &quot;Novak Djokovic&quot; &quot;Rafael Nadal&quot; ## [5] &quot;Roger Federer&quot; which implies that Roger Federer won most grand slams. Another related function is sort() which simply sorts the elements of a vector in an ascending manner. For example, sort(grand_slam_win) ## Stan Wawrinka Andy Murray Novak Djokovic Rafael Nadal Roger Federer ## 5 9 12 15 18 which is compact version of grand_slam_win[order(grand_slam_win)] ## Stan Wawrinka Andy Murray Novak Djokovic Rafael Nadal Roger Federer ## 5 9 12 15 18 There are of course many other useful functions that allow to deal with vectors which we will not mention in this section but can be found in a variety of references (see for example Wickham (2014)). 4.1.7 Creation sequences When uing R for statistical programming or even data analysis it is very common to create sequences of numbers. Here are three common ways used to create such sequences: from:to: This method is quite inituitive and very compact. For example: (x = 1:3) ## [1] 1 2 3 (y = 3:1) ## [1] 3 2 1 (w = -1:-4) ## [1] -1 -2 -3 -4 (z = 1.3:3) ## [1] 1.3 2.3 seq_len(n): This function provides a simple way to generate a sequence from 1 to an arbitrary number n. In general, 1:n and seq_len(n) are equivalent with the notable exeptions where n = 0 and n &lt; 0. The reason for these exeptions will become clear in Section 5.2.2.1. Let’s see a few examples: n = 3 1:n ## [1] 1 2 3 seq_len(n) ## [1] 1 2 3 n = 0 1:n ## [1] 1 0 seq_len(n) ## integer(0) seq(a, b, by/length.out = d): This function can be used to create more “complex” sequences. It can either be used to create a sequence from a to b by increments of d (using the option by) or of a total length of d (using the option length.out). A few examples: (x = seq(1, 2.8, by = 0.4)) ## [1] 1.0 1.4 1.8 2.2 2.6 (y = seq(1, 2.8, length.out = 6)) ## [1] 1.00 1.36 1.72 2.08 2.44 2.80 It could be interesting to use a function like rep() that allows to create vectors with repeated values or sequences, for example: rep(c(1,2), times = 3, each = 1) ## [1] 1 2 1 2 1 2 rep(c(1,2), times = 1, each = 3) ## [1] 1 1 1 2 2 2 where the option times allows to specify how many times the object needs to be repeated and each regulates how many times each element in the object is repeated. 4.1.8 Example: Apple Stock Price Suppose that one is interested in analysing the behavior of Apple’s stock price over the last three months. The first thing that is needed is today’s date which can be obtained as follows (today = Sys.Date()) ## [1] &quot;2017-08-20&quot; Once this is done, we can obtain the date which is exactly three monmths ago (three_months_ago = seq(today, length = 2, by = &quot;-3 months&quot;)[2]) ## [1] &quot;2017-05-20&quot; With this information, we can now download Apple’s stock price and represent these stocks through a candlestick chart which summarizes information on daily opening and closing prices as well as minimum and maximum prices. Just to note, library() loads and attaches add-on packages. library(quantmod) getSymbols(&quot;AAPL&quot;, from = three_months_ago, to = today) ## [1] &quot;AAPL&quot; candleChart(AAPL, theme=&#39;white&#39;, type=&#39;candles&#39;) Once we have the prices, we can compute some returns which are defined as follows \\[\\begin{equation} r_t = \\frac{S_t - S_{t-1}}{S_{t-1}} \\end{equation}\\] where \\(r_t\\) are the returns at time t and \\(S_t\\) is the stock price. This is implemented in the function ClCl() within the quantmod package. For example, we can create a vector of returns as follows AAPL_returns = as.numeric(na.omit(ClCl(AAPL))) where na.omit is used to remove missing values in the stock prices vector since, if we have \\(n+1\\) stock prices, we will only have \\(n\\) returns and as.numeric is used to transform the computed returns into a numeric vector. We can now compute the mean and median of the returns over the considered period. mean(AAPL_returns) ## [1] 0.0004500884 median(AAPL_returns) ## [1] 0.002557922 However, a statistic that is of particular interest to financial operators is the Excess Kurtosis which, for a random variable that we denote as \\(X\\), can be defined as \\[\\begin{equation} \\text{Kurt} = \\frac{{E}\\left[\\left(X - E[X]\\right)^4\\right]}{\\left({E}\\left[\\left(X - E[X]\\right)^2\\right]\\right)^2} - 3 \\end{equation}\\] The reason for defining this statistic as Excess Kurtosis lies in the fact that the standardized kurtosis is compared to that of a Gaussian distribution (whose kurtosis is equal to 3) which has exponentially decaying tails. Consequently, if the Excess Kurtosis is positive, this implies that the distribution has heavier tails than a Gaussian and therefore has higher probabilities of extreme events occurring. Given this statistic, it is useful to compute this on the observed data and for this purpose a common estimator of the excess Kurtosis is \\[\\begin{equation} k = \\frac{\\frac{1}{n} \\sum_{t = 1}^{n} \\left(r_t -\\bar{r}\\right)^4}{\\left(\\frac{1}{n} \\sum_{t = 1}^{n} \\left(r_t -\\bar{r}\\right)^2 \\right)^2} - 3 \\end{equation}\\] where \\(\\bar{k}\\) denotes the sample average of the returns, i.e. \\[\\begin{equation} \\bar{k} = \\frac{1}{n} \\sum_{i = 1}^n r_i \\end{equation}\\] mu = mean(AAPL_returns) (k = mean((AAPL_returns - mu)^4)/(mean((AAPL_returns - mu)^2))^2 - 3) ## [1] 2.737375 which is quite high tends to indicate the returns have a heavier tails than the normal distribution. 4.2 Matrix Matrices are another extremely common data structure in R. Compared to vectors, matrices have an additional dimension which, for example, allows to stock multiple equidimensional vectors within the same object. Below is an example of how to create a matrix in R: (mat = matrix(1:12, ncol = 4, nrow = 3)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Notice that the first argument to the function is a vector (in this case a vector with increasing elements from 1 to 12) which is then transformed into a matrix with four columns (ncol = 4) and three rows (nrow = 3). By default, the vectors are transformed into matrices by placing the elements by column (i.e. starting from the top of the first column to the bottom and then passing to the following column until all columns are full). If you wish to fill the matrix by row, all you need to do is specify the argument byrow = T. Usually the length of the vector (i.e. number of elements in the vector) is the result of the multiplication between the number of columns and number of rows. What happens if the vector has fewer elements for the same matrix dimension? What happens if the vector has more elements? It is often the case however that we already have equidimensional vectors available and we wish to stock them into a matrix. In these cases, two useful functions are cbind() and rbind() where the first function stocks the vectors vertically side-by-side while the second stocks the vectors horizontally one below the other. An example of the former is given below: players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;) grand_slam_win = c(9, 15, 5, 12, 18) win_percentage = c(78.07, 82.48, 63.96, 82.77, 81.80) (mat = cbind(grand_slam_win, win_percentage)) ## grand_slam_win win_percentage ## [1,] 9 78.07 ## [2,] 15 82.48 ## [3,] 5 63.96 ## [4,] 12 82.77 ## [5,] 18 81.80 The result in this case is a \\(5 \\times 2\\) matrix (while with rbind() it would have been a \\(2 \\times 5\\) matrix). Once the matrix is defined, we can assign names to its rows and columns by using resepctively rownames and colnames. Of course, the number of names must correspond to the respective matrix dimensions as shown in the following example where each row corresponds to a specific player (thereby using the players vector) and each column corresponds to a specific statistic on the players. rownames(mat) &lt;- players colnames(mat) &lt;- c(&quot;GS win&quot;, &quot;Win rate&quot;) mat ## GS win Win rate ## Andy Murray 9 78.07 ## Rafael Nadal 15 82.48 ## Stan Wawrinka 5 63.96 ## Novak Djokovic 12 82.77 ## Roger Federer 18 81.80 4.2.1 Subsetting As for vectors, it is possible to subset the elements of a matrix. However, in the case of matrices we’re dealing with two-dimensional data structures and it is therefore necessary to specify the position of the elements of interest in both dimensions. For this purpose, as with vectors, we can use [ ] but, as opposed to vectors, we need to add , within the square brackets where the rows are specified before the comma and the columns after it. Below are a few examples: mat[c(&quot;Stan Wawrinka&quot;, &quot;Roger Federer&quot;), ] ## GS win Win rate ## Stan Wawrinka 5 63.96 ## Roger Federer 18 81.80 mat[c(1, 3), ] ## GS win Win rate ## Andy Murray 9 78.07 ## Stan Wawrinka 5 63.96 mat[, 2] ## Andy Murray Rafael Nadal Stan Wawrinka Novak Djokovic Roger Federer ## 78.07 82.48 63.96 82.77 81.80 mat[1:3, 1] ## Andy Murray Rafael Nadal Stan Wawrinka ## 9 15 5 It can be noticed that, when a space is left blank before or after the comma, this means that respectively all the rows or all the columns are considered. 4.2.2 Linear Algebra As with vectors, there are some useful functions that can be used with matrices. A first example is the function dim() that allows to determine the dimension of a matrix. For example, consider the following \\(4 \\times 2\\) matrix \\[\\begin{equation*} \\mathbf{A} = \\left[ \\begin{matrix} 1 &amp; 5\\\\ 2 &amp; 6\\\\ 3 &amp; 7\\\\ 4 &amp; 8 \\end{matrix} \\right] \\end{equation*}\\] which can be created in R as follows: (A = matrix(1:8, 4, 2)) ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 ## [4,] 4 8 Therefore, we expect dim(A) to retrun the vector c(4, 2). Indeed, we have dim(A) ## [1] 4 2 Next, we consider the function t() allows transpose a matrix. For example, \\(\\mathbf{A}^T\\) is equal to: \\[\\begin{equation*} \\mathbf{A}^T = \\left[ \\begin{matrix} 1 &amp; 2 &amp; 3 &amp; 4\\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\end{matrix} \\right], \\end{equation*}\\] which is a \\(2 \\times 4\\) matrix. In R, we achieve this as follows (At &lt;- t(A)) ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 dim(At) ## [1] 2 4 Aside from playing with matrix dimensions, matrix algebraic operations have specific commands. For example, the operator %*% is used in R to denote matrix multiplication while, as opposed to scalar objects, the regular product operator * performs the Hadamard product (or element by element product) when applied to matrices. For example, consider the following matrix product \\[\\begin{equation*} \\mathbf{B} = \\mathbf{A}^T \\mathbf{A} = \\left[ \\begin{matrix} 30 &amp; 70\\\\ 70 &amp; 174 \\end{matrix} \\right], \\end{equation*}\\] which can be done in R as follows: (B = At %*% A) ## [,1] [,2] ## [1,] 30 70 ## [2,] 70 174 Other common matrix operations include finding the determinant of a matrix and finding its inverse. These are often used, for example, when computing the likelihood function for a variable following a Gaussian distribution or when simulating time series or spatial data. The functions that perform these operations are det() and solve() that respectively find the determinant and the inverse of a matrix (which necessarily has to be square). The function det() can be used to compute the determinant of a (squared) matrix. In the case of a \\(2 \\times 2\\) matrix, there exists a simple solution for the determinant which is \\[\\begin{equation*} \\text{det} \\left( \\mathbf{D} \\right) = \\text{det} \\left( \\left[ \\begin{matrix} d_1 &amp; d_2\\\\ d_3 &amp; d_4 \\end{matrix} \\right] \\right) = d_1 d_4 - d_2 d_3. \\end{equation*}\\] Consider the matrix \\(\\mathbf{B}\\), we have \\[\\begin{equation*} \\text{det} \\left( \\mathbf{B}\\right) = 30 \\cdot 174 - 70^2 = 320. \\end{equation*}\\] In R, we can simply do det(B) ## [1] 320 The function solve() is also an important function when working with matrices as it allows to inverse a matrix. It is worth remembering that a square matrix that is not invertible (i.e. \\(\\mathbf{A}^{-1}\\) doesn’t exist) is called singular and the determinant offers a way to “check” if this is the case for a given matrix. Indeed, a square matrix is singular if and only if its determinant is 0. Therefore, in the case of \\(\\mathbf{B}\\), we should be able to compute its inverse. As for the determinant, there exists a formula to compute the inverse of \\(2 \\times 2\\) matrices, i.e. \\[\\begin{equation*} \\mathbf{D}^{-1} = \\left[ \\begin{matrix} d_1 &amp; d_2\\\\ d_3 &amp; d_4 \\end{matrix} \\right]^{-1} = \\frac{1}{\\text{det}\\left( \\mathbf{D} \\right)} \\left[ \\begin{matrix} \\phantom{-}d_4 &amp; -d_2\\\\ -d_3 &amp; \\phantom{-}d_1 \\end{matrix} \\right]. \\end{equation*}\\] Considering the matrix \\(\\mathbf{B}\\), we obtain \\[\\begin{equation*} \\mathbf{B}^{-1} = \\left[ \\begin{matrix} 30 &amp; 70\\\\ 70 &amp; 174 \\end{matrix} \\right]^{-1} = \\frac{1}{320}\\left[ \\begin{matrix} \\phantom{-}174 &amp; -70\\\\ -70 &amp; \\phantom{-}30 \\end{matrix} \\right] = \\end{equation*}\\] (B_inv = solve(B)) ## [,1] [,2] ## [1,] 0.54375 -0.21875 ## [2,] -0.21875 0.09375 Finally, we can verify that \\[\\begin{equation*} \\mathbf{G} = \\mathbf{B} \\mathbf{B}^{-1}, \\end{equation*}\\] should be equal to the identity matrix, (G = B %*% B_inv) ## [,1] [,2] ## [1,] 1 -8.881784e-16 ## [2,] 0 1.000000e+00 The result is of course extremely close but \\(\\mathbf{G}\\) is not exactly equal to the identity matrix due to rounding and other numerical errors. Another function of interest is the function diag() that can be used to extract the diagonal of a matrix. For example, we have \\[\\begin{equation*} \\text{diag} \\left( \\mathbf{B} \\right) = \\left[30 \\;\\; 174\\right], \\end{equation*}\\] which can be done in R as follows: diag(B) ## [1] 30 174 Therefore, the function diag() allows to easily compute the trace of matrix (i.e. the sum of the diagonal elements). For example, \\[\\begin{equation*} \\text{tr} \\left( \\mathbf{B} \\right) = 204, \\end{equation*}\\] or in R sum(diag(B)) ## [1] 204 Another use of the function diag() is to create diagonal matrices. Indeed, if the argument of this function is a vector, its behavior is the following: \\[\\begin{equation*} \\text{diag} \\left(\\left[a_1 \\;\\; a_2 \\;\\; \\cdots \\;\\; a_n\\right]\\right) = \\left[ \\begin{matrix} a_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; a_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_n \\end{matrix} \\right]. \\end{equation*}\\] Therefore, this provides a simple way of creating an identity matrix by combining the functions diag() and rep() (discussed in the previous section) as follows: n = 4 (ident = diag(rep(1, n))) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 4.2.3 Example: Summary Statistics with Matrix Notation A simple example of the operations we discussed in the previous section is given by many common statistics that can be reexpressed using matrix notation. As an example, we will consider three common statistics that are the sample mean, variance and covariance. Let us consider the following two samples of size \\(n\\) \\[\\begin{equation*} \\begin{aligned} \\mathbf{x} &amp;= \\left[x_1 \\;\\; x_2 \\; \\;\\cdots \\;\\; x_n\\right]^T\\\\ \\mathbf{y} &amp;= \\left[y_1 \\;\\;\\; y_2 \\; \\;\\;\\cdots \\;\\;\\; y_n\\right]^T. \\end{aligned} \\end{equation*}\\] The sample mean of \\(\\mathbf{x}\\) is \\[\\begin{equation*} \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^{n} x_i, \\end{equation*}\\] and its sample variance is \\[\\begin{equation*} s_x^2 = \\frac{1}{n} \\sum_{i = 1}^n \\left(x_i - \\bar{x}\\right)^2. \\end{equation*}\\] The sample covariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is \\[\\begin{equation*} s_{x,y} = \\frac{1}{n} \\sum_{i = 1}^n \\left(X_i - \\bar{x}\\right) \\left(Y_i - \\bar{y}\\right), \\end{equation*}\\] where \\(\\bar{y}\\) denotes the sample mean of \\(\\mathbf{y}\\). Consider the sample mean, this statistic can be expressed in matrix notation as follows \\[\\begin{equation*} \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^{n} x_i = \\frac{1}{n} \\mathbf{x}^T \\mathbf{1}, \\end{equation*}\\] where \\(\\mathbf{1}\\) is a column vector of \\(n\\) ones. \\[\\begin{equation*} \\begin{aligned} s_x^2 &amp;= \\frac{1}{n} \\sum_{i = 1}^n \\left(x_i - \\bar{x}\\right)^2 = \\frac{1}{n} \\sum_{i = 1}^n x_i^2 - \\bar{x}^2 = \\frac{1}{n} \\mathbf{x}^T \\mathbf{x} - \\bar{x}^2\\\\ &amp;= \\frac{1}{n} \\mathbf{x}^T \\mathbf{x} - \\left(\\frac{1}{n} \\mathbf{x}^T \\mathbf{1}\\right)^2 = \\frac{1}{n} \\left(\\mathbf{x}^T \\mathbf{x} - \\frac{1}{n} \\mathbf{x}^T \\mathbf{1} \\mathbf{1}^T \\mathbf{x}\\right)\\\\ &amp;= \\frac{1}{n}\\mathbf{x}^T \\left( \\mathbf{I} - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T \\right) \\mathbf{x} = \\frac{1}{n}\\mathbf{x}^T \\mathbf{H} \\mathbf{x}, \\end{aligned} \\end{equation*}\\] where \\(\\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). This matrix is often called the centering matrix. Similarly, for the sample covariance we obtain \\[\\begin{equation*} \\begin{aligned} s_{x,y} &amp;= \\frac{1}{n} \\sum_{i = 1}^n \\left(x_i - \\bar{x}\\right) \\left(y_i - \\bar{y}\\right) = \\frac{1}{n}\\mathbf{x}^T \\mathbf{H} \\mathbf{y}. \\end{aligned} \\end{equation*}\\] In the code below we verify the validity of these results by comparing the value of the three statistics based on the different formulas. # Sample size n = 100 # Simulate random numbers from a zero mean normal distribution with # variance equal to 4. x = rnorm(n, 0, sqrt(4)) # Simulate random numbers from normal distribution with mean 3 and # variance equal to 1. y = rnorm(n, 3, 1) # Note that x and y are independent. # Sample mean one = rep(1, n) (x_bar = 1/n*sum(x)) ## [1] -0.3156716 (x_bar_mat = 1/n*t(x)%*%one) ## [,1] ## [1,] -0.3156716 # Sample variance H = diag(rep(1, n)) - 1/n * one %*% t(one) (s_x = 1/n * sum((x - x_bar)^2)) ## [1] 4.277322 (s_x_mat = 1/n*t(x) %*% H %*% x) ## [,1] ## [1,] 4.277322 # Sample covariance y_bar = 1/n*sum(y) (s_xy = 1/n*sum((x - x_bar)*(y - y_bar))) ## [1] -0.01609829 (s_xy_mat = 1/n*t(x) %*% H %*% y) ## [,1] ## [1,] -0.01609829 4.2.4 Example: Least-squares If the matrix \\(\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\) exists, the least-squares estimator for \\(\\boldsymbol{\\beta}\\) is given by: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} \\tag{4.1} \\end{equation}\\] In the comment box below, we derive Eq. (4.1). If you aren’t familiar with such calculations we suggest you read some introduction to linear regression (see for example Seber and Lee (2012)). The least-square estimator \\(\\hat{\\boldsymbol{\\beta}}\\) is given by \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\operatorname{argmin}_{\\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) \\end{equation*}\\] The first step of this derivation is to reexpress the term \\(\\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right)\\) as follows: \\[\\begin{equation*} \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) = \\mathbf{y}^T\\mathbf{y} + \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} - 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y}. \\end{equation*}\\] In case you were suprizied by the term \\(2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y}\\) remeber that a scalar can always be transposed without changing its value and therefore we have that $ ^T ^T = ^T $. Now, our next step is to the following derivative \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right). \\end{equation*}\\] To do this we should keep in mind the following results \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y} = \\boldsymbol{y}^T \\mathbf{X}, \\end{equation*}\\] and \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} = 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X}. \\end{equation*}\\] The proof of these two results can for example be found in Propositions 7 and 9 of Prof. Barnes’ notes. Using these two results we obtain \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) = 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} - 2 \\boldsymbol{y}^T \\mathbf{X}. \\end{equation*}\\] By solving for the first order condition (and under some technical assumptions not discussed here) we can redefine \\(\\hat{\\boldsymbol{\\beta}}\\) through the follwing equation \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}}^T \\mathbf{X}^T \\mathbf{X} = \\boldsymbol{y}^T \\mathbf{X}, \\end{equation*}\\] which is equivalent to \\[\\begin{equation*} \\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\boldsymbol{y}. \\end{equation*}\\] If \\(\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\) exists, \\(\\hat{\\boldsymbol{\\beta}}\\) is therefore given by \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y}, \\end{equation*}\\] which verifies Eq. (4.1). The variance of \\(\\hat{\\boldsymbol{\\beta}}\\) is given by \\[\\begin{equation} \\text{Var} \\left(\\hat{\\boldsymbol{\\beta}} \\right) = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}, \\tag{4.2} \\end{equation}\\] the derivation of this result is explained in the comment box below. If we let \\(\\mathbf{A} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T\\), then we have \\[\\begin{equation*} \\begin{aligned} \\text{Var} \\left(\\hat{\\boldsymbol{\\beta}} \\right) &amp;= \\text{Var} \\left( \\mathbf{A} \\mathbf{y} \\right) = \\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T \\\\ &amp; = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}, \\end{aligned} \\end{equation*}\\] which verifies Eq. (4.2). To understand the above derivation it may be useful to remind and point out a few things: \\(\\text{Var} \\left( \\mathbf{A} \\mathbf{y} \\right) = \\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T\\) since \\(\\mathbf{A}\\) is not a random variable. \\(\\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T\\) since\\(\\text{Var} \\left( \\mathbf{y} \\right) = \\sigma^2 \\mathbf{I}\\) and therefore we have \\(\\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{I} \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T\\). The result \\(\\mathbf{A} \\mathbf{A}^T = (\\mathbf{X}^T \\mathbf{X})^{-1}\\) is based on the fact that \\((\\mathbf{X}^T \\mathbf{X})^{-1}\\) is symmetric but this is not necessarily intuitive. Indeed, this follows from the fact that any square and invertible matrix \\(\\mathbf{B}\\) is such that the inverse and transpose operator commute, meaning that \\(( \\mathbf{B}^T )^{-1} = ( \\mathbf{B}^{-1} )^T\\). Therefore since the matrix \\(\\mathbf{X}^T \\mathbf{X}\\) is square and (by assumption) invertible we have \\([(\\mathbf{X}^T \\mathbf{X})^{-1}]^T = [(\\mathbf{X}^T \\mathbf{X})^{T}]^{-1} = ( \\mathbf{X}^T \\mathbf{X})^{-1}\\). In general, the residual variance is unknown and needs to be estimated. A common and unbiased estimator of \\(\\sigma^2\\) is given by \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{n - p} \\left( \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\right) \\tag{4.3} \\end{equation}\\] DO YOU GUYS THINK WE SHOULD SHOW THE UNBIASEDNESS IN BOX HERE. THIS IS A LITTLE MORE ADVANCED AS WE NEED TO USE PROJECTION MATRICES AND THEIR PROPERTIES. LET ME KNOW. ROB COMMENT: Maybe I’d ask them to show the unbiasdness as homework either theoretically or giving numerical support to the claim (e.g. the mean of different beta hats is close to the true beta) JUSTIN COMMENT: I agree with Rob’s suggestion :) Let’s implement Eq. (4.1) to (4.3) and, later, compare the outcome with the lm() function for linear regression implemented in base R. To do so, we will study an example taken from Wood (2017) which discusses a dataset collected from the Hubble Space Telescope key project containing information on the velocity and relative distance of 24 galaxies. This information has been used to compute the “Hubble constant’’ which is a fixed parameter that links velocity and distance of celestial bodies through which it is possible to compute the age of the universe based on the”big bang&quot; theory. The link is given by this simple linear relationship \\[\\begin{equation*} y = \\beta x , \\end{equation*}\\] where \\(y\\) represents the velocity while \\(x\\) is the distance between galaxies. Once the Hubble constant is known, its inverse (i.e. \\(\\beta^{-1}\\)) gives the age of the universe based on the big bang theory. Therefore let us use the abovementioned dataset to estimate Hubble’s constant to then get an estimate of the age of the universe. This dataset can be found in the gamair package under the name hubble and when plotting the two variables of interest in a scatterplot there appears to be a positive linear relationship between the two variables. # Load gamair library and retrieve Hubble dataset library(gamair) data(hubble) # Plot data plot(y = hubble$y, x = hubble$x, col=&quot;red&quot;, main=&quot;Distance vs. Velocity&quot;, ylab = &quot;Velocity (in km/s)&quot;, xlab = &quot;Distance (in Megaparsecs)&quot;) Putting aside possible problems with the data that would require a more in-depth knowledge and discussion of linear regression theory, we can proceed to estimate the Hubble constant by using the velocity as the response variable \\(y\\) and the distance as the explanatory variable \\(x\\). Let us therefore start by implementing Eq. (4.1) to get an estimate of \\(\\beta\\). # Define parameters and variables n = dim(hubble)[1] # number of observations p = 1 # number of explanatory variables df = n - p # degrees of freedom x = hubble$x # distance (explanatory variable) y = hubble$y # velocity (response variable) # Estimate of beta from Eq. (1.1) beta.hat = solve(t(x)%*%x)%*%t(x)%*%y We now have an estimated value for \\(\\beta\\) that we denote as \\(\\hat{\\beta}\\). Due to different measurement units we perform a unit transformation and then compute the age in years (which requires an additional transformation since the previous one gives a unit of \\(s^{-1}\\)). hubble.const = beta.hat/3.09e19 # Estimated Hubble&#39;s constant in inverse seconds age.sec = 1/hubble.const # Age of the universe in seconds age.sec/(60^2*24*365) # Age of the universe in years ## [,1] ## [1,] 12794692825 Based on this estimation, the age of the universe appears to be almost 13 billion years. However, we know that \\(\\hat{\\beta}\\) is a random variable that therefore follows a distribution which, based on asymptotic statistical theory, is a normal distribution with expectaton \\(\\beta\\) and variance \\(\\sigma^2(X^TX)^{-1}\\). Now, let’s suppose that we have a hypothesis on the age of the universe, for example that of Creation Scientists who claim that the universe is 6000 years old based on a reading from the Bible. Assuming the validity of the big bang theory, which is not the case for Creation Scientists, let us nevertheless test if their claim appears to be reasonable within this postulated framework. In order to do so we need to know the variance of \\(\\hat{\\beta}\\) and we consequently need to estimate \\(\\sigma^2\\) since we don’t know it. Let us therefore use Eq. (4.2) and (4.3) to compute it. # Estimate of the residual variance from Eq. (1.3) resid = y - x*beta.hat ## Warning in x * beta.hat: Recycling array of length 1 in vector-array arithmetic is deprecated. ## Use c() or as.vector() instead. sigma2.hat = (1/df)*t(resid)%*%resid # Estimate of the variance of the estimated beta from Eq. (1.2) var.beta = sigma2.hat*solve(t(x)%*%x) We now have all the information regarding the distribution of \\(\\hat{\\beta}\\) to test the Creation Scientists’ hypothesis. Indeed, for this purpose we can build a 95% confidence interval for the true Hubble constant \\(\\beta\\) and understand if the Hubble constant implied by the postulated age of the universe falls within this interval. Firstly, we can determine this constant under the null hypothesis which can be defined as follows \\[\\begin{equation*} H_0 \\, : \\, \\beta = 163 \\times 10^6 , \\end{equation*}\\] since this value of \\(\\beta\\) would imply that the universe is roughly 6000 years old. The alternative hypothesis is that the true \\(\\beta\\) is not equal to the above quantity (i.e. \\(H_A \\, : \\, \\beta \\neq 163 \\times 10^6\\)). Since we now have the necessary information, let us build a confidence interval for which we will assume that the estimated variance of \\(\\hat{\\beta}\\) is actually the true variance (otherwise we would need to build a confidence interval based on the Student-t distribution which we will not deal with at this stage of our analysis). Assuming therefore that \\[\\begin{equation*} \\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta,\\hat{\\sigma}^2(X^TX)^{-1}\\right) , \\end{equation*}\\] we consequently have that the confidence interval is given by \\[\\begin{equation} \\left[\\hat{\\beta} - z_{1-\\alpha/2}\\sqrt{\\hat{\\sigma}^2(X^TX)^{-1}} \\, , \\, \\hat{\\beta} + z_{1-\\alpha/2}\\sqrt{\\hat{\\sigma}^2(X^TX)^{-1}} \\right] , \\tag{4.4} \\end{equation}\\] where \\(z_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)^{th}\\) quantile of the standard normal distribution where, in our case, \\(\\alpha = 0.05\\) which delivers a 95% confidence interval. Let us therefore replace the values in Eq. (4.4): # 95% confidence interval for the Hubble constant ci.beta = c(beta.hat - qnorm(p = 0.975)*sqrt(var.beta), beta.hat + qnorm(p = 0.975)*sqrt(var.beta)) ci.beta ## [1] 68.81032 84.35203 The confidence interval lies between 68 and 84 which clearly does not contain the value implied by the age of the universe postulated by the Creation Scientists. Hence, assuming the validity of the big bang theory, we can reject this hypothesis at a 95% level. To conclude this analysis, let us compare the results of Eq. (4.1) to (4.3) with the outputs of the linear regression function lm() that can be found in base R. # Linear regression with lm() function fit_lm = lm(y~x-1) # Compare outputs equation_results = c(beta.hat, sigma2.hat) function_results = c(fit_lm$coefficients, (1/df)*t(fit_lm$residuals)%*%fit_lm$residuals) results = cbind(equation_results, function_results) row.names(results) = c(&quot;beta.hat&quot;, &quot;sigma2.hat&quot;) results ## equation_results function_results ## beta.hat 76.58117 76.58117 ## sigma2.hat 67046.33165 67046.33165 As one can notice, the two procedures give the same outputs. 4.3 Array Simply, vectors are one-dimensional and matrices are two-dimensional objects, and these homogenous objects are also known as arrays. We can extend the array into more than two dimensions by specifying the dim parameter within the array function. Arrays are rarer than matrices and vectors, but worth being aware of. Below we initialize a three dimensional array, from 2 to 13. three_dim = array(data = 2:13, dim = c(2, 3, 2)) three_dim ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 2 4 6 ## [2,] 3 5 7 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 8 10 12 ## [2,] 9 11 13 As you can see, two 2x3 matrices were created containing numbers from 2 to 13. The , , * syntax describes which specific dimension the matrix is contained in. Like what we experimented with matrices, we can extract and manipulate information. length(three_dim) # length is 12 ## [1] 12 dim(three_dim) # dimensions are 2x3x2 ## [1] 2 3 2 is.array(three_dim) # yes it is an array! ## [1] TRUE Note that when changing the dimension names, we need to save it as a list. We will talk more about lists in the coming section. dimnames(three_dim) = list(c(&quot;apple&quot;, &quot;banana&quot;), c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;), c(&quot;Walmart&quot;, &quot;Costco&quot;)) # feel free to make three_dim ## , , Walmart ## ## one two three ## apple 2 4 6 ## banana 3 5 7 ## ## , , Costco ## ## one two three ## apple 8 10 12 ## banana 9 11 13 4.4 List A list is one of the commonly used heterogeneous data structures, which depicts a generic vector containing other object types. For example, we can have a numeric vector as one element, a matrix, and a character vector as another element. Here we can create a list that contains all these different element types. num_vec = c(160, 188, 140) char_vec = c(&quot;Height&quot;, &quot;Weight&quot;, &quot;Length&quot;) my_mat = matrix(0, nrow = 5, ncol = 5) # List Initialization without Names # list(num_vec, char_vec, my_mat) # List Initialization with Custom Names list(Number = num_vec, Character = char_vec, Empty_Mat = my_mat) ## $Number ## [1] 160 188 140 ## ## $Character ## [1] &quot;Height&quot; &quot;Weight&quot; &quot;Length&quot; ## ## $Empty_Mat ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 ## [3,] 0 0 0 0 0 ## [4,] 0 0 0 0 0 ## [5,] 0 0 0 0 0 Let’s store this list in a variable. my_list = list(Number = num_vec, Character = char_vec, Empty_Mat = my_mat) We can subset lists like we did previously. # Extract the first and third element my_list[c(1, 3)] ## $Number ## [1] 160 188 140 ## ## $Empty_Mat ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 ## [3,] 0 0 0 0 0 ## [4,] 0 0 0 0 0 ## [5,] 0 0 0 0 0 # Compare these two subsets my_list[1] ## $Number ## [1] 160 188 140 my_list[[1]] ## [1] 160 188 140 Do you see what the difference is? One generally uses [[ to select any single element, whereas [ returns a list of the selected elements. Using one bracket retains the class information, while using two brackets simplifies the list into a numeric vector. We can further subset the list. # First element of the $Number list my_list[[1]][1] ## [1] 160 # Why didn&#39;t this work? my_list[1][1] ## $Number ## [1] 160 188 140 4.5 Dataframe Needs some rewording A data frame is used for storing data tables. A data frame is the most common way of storing data in R, it has a 2D structure and shares properties of both the matrix and the list. The table contains lists of equal-length vectors, and most datasets will have a data frame format. We can create a data frame using data.frame() ### Creation players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;) grand_slam_win = c(9, 15, 5, 12, 18) date_of_birth = c(&quot;15 May 1987&quot;, &quot;3 June 1986&quot;, &quot;28 March 1985&quot;, &quot;22 May 1981&quot;, &quot;8 August 1981&quot;) country = c(&quot;Great Britain&quot;, &quot;Spain&quot;, &quot;Switzerland&quot;, &quot;Serbia&quot;, &quot;Switzerland&quot;) ATP_ranking = c(1, 2, 3, 4, 5) prize_money = c(60449649, 85920132, 30577981, 109447408, 104445185) tennis = data.frame(date_of_birth, grand_slam_win, country, ATP_ranking, prize_money) dimnames(tennis)[[1]] = players tennis ## date_of_birth grand_slam_win country ATP_ranking ## Andy Murray 15 May 1987 9 Great Britain 1 ## Rafael Nadal 3 June 1986 15 Spain 2 ## Stan Wawrinka 28 March 1985 5 Switzerland 3 ## Novak Djokovic 22 May 1981 12 Serbia 4 ## Roger Federer 8 August 1981 18 Switzerland 5 ## prize_money ## Andy Murray 60449649 ## Rafael Nadal 85920132 ## Stan Wawrinka 30577981 ## Novak Djokovic 109447408 ## Roger Federer 104445185 We can check if we have achived our goal by using: is.data.frame(tennis) ## [1] TRUE 4.5.1 Combination Different data frames can also be combined. Let say we want to add some ifomrmation to our initial table e.g. the player’s height and if he is right-handed or letf-handed. We can do so by using cbind() and rbind(): height &lt;- c(1.90, 1.85, 1.83, 1.88, 1.85) hand &lt;- c(&quot;R&quot;,&quot;L&quot;,&quot;R&quot;,&quot;R&quot;,&quot;R&quot;) (tennis = cbind(tennis, data.frame(height, hand))) ## date_of_birth grand_slam_win country ATP_ranking ## Andy Murray 15 May 1987 9 Great Britain 1 ## Rafael Nadal 3 June 1986 15 Spain 2 ## Stan Wawrinka 28 March 1985 5 Switzerland 3 ## Novak Djokovic 22 May 1981 12 Serbia 4 ## Roger Federer 8 August 1981 18 Switzerland 5 ## prize_money height hand ## Andy Murray 60449649 1.90 R ## Rafael Nadal 85920132 1.85 L ## Stan Wawrinka 30577981 1.83 R ## Novak Djokovic 109447408 1.88 R ## Roger Federer 104445185 1.85 R 4.5.2 Subsetting Like for vectors, it is also possible to subset the values that we have stored in our data frames. Since data frames possess the characteristics of both lists and matrices: if you subset with a single vector, they behave like lists; if you subset with two vectors, they behave like matrices. # Let say we want only want to know the country and date of # birth of the players # There are two ways to select columns from a data frame # Like a list: tennis[c(&quot;country&quot;, &quot;date_of_birth&quot;)] ## country date_of_birth ## Andy Murray Great Britain 15 May 1987 ## Rafael Nadal Spain 3 June 1986 ## Stan Wawrinka Switzerland 28 March 1985 ## Novak Djokovic Serbia 22 May 1981 ## Roger Federer Switzerland 8 August 1981 # Like a matrix tennis[, c(&quot;country&quot;, &quot;date_of_birth&quot;)] ## country date_of_birth ## Andy Murray Great Britain 15 May 1987 ## Rafael Nadal Spain 3 June 1986 ## Stan Wawrinka Switzerland 28 March 1985 ## Novak Djokovic Serbia 22 May 1981 ## Roger Federer Switzerland 8 August 1981 # To acces a single element, let say the date of birth, # you can also use: tennis$date_of_birth ## [1] 15 May 1987 3 June 1986 28 March 1985 22 May 1981 8 August 1981 ## 5 Levels: 15 May 1987 22 May 1981 28 March 1985 ... 8 August 1981 4.5.3 Application: Non-parametric bootstrap Suppose we ask 10 students how much time they work at home for their calculus class, we obtain the following results (in hour) student_work &lt;- c(0, 0, 0, 0.25, 0.25, 0.75, 0.75, 1, 1.25, 4) We can compute the mean time spent mean(student_work) ## [1] 0.825 ADD SOMETHING ON T TEST t.test(student_work)$conf.int ## [1] -0.03495865 1.68495865 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 We can see that our confidence interval includes a negative values which clearly isn’t meaningful. Solution: (non-parametric) bootstrap which works as follows….. ADD SOMETHING Here is a simple function to implement this approach: # Number of boostrap replications B = 500 # Compute the length of vector n = length(student_work) # Confidence level alpha = 0.05 # Initialisation of boot_mean = rep(NA, B) for (i in 1:B){ student_work_star = student_work[sample(1:n, replace = TRUE)] boot_mean[i] = mean(student_work_star) } quantile(boot_mean, c(alpha/2, 1 - alpha/2)) ## 2.5% 97.5% ## 0.261875 1.625000 #hist(boot_mean, probability = TRUE) References "],
["control.html", "Chapter 5 Logical Operators and Control Stuctures 5.1 Logical Operators 5.2 Control Structures 5.3 Applications:", " Chapter 5 Logical Operators and Control Stuctures 5.1 Logical Operators 5.2 Control Structures 5.2.1 Selection Controls Basically if, if/else, if/elseif/else and switch 5.2.2 Iteration Controls 5.2.2.1 for 5.3 Applications: 5.3.1 Non-parametric Bootstrap Suppose we ask 10 students how much time they work at home for their calculus class, we obtain the following results (in hour) student_work = c(0, 0, 0, 0.25, 0.25, 0.75, 0.75, 1, 1.25, 4) We can compute the mean time spent mean(student_work) ## [1] 0.825 ADD SOMETHING ON T TEST t.test(student_work)$conf.int ## [1] -0.03495865 1.68495865 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 We can see that our confidence interval includes a negative values which clearly isn’t meaningful. Solution: (non-parametric) bootstrap which works as follows….. ADD SOMETHING Here is a simple function to implement this approach: # Number of boostrap replications B = 500 # Compute the length of vector n = length(student_work) # Confidence level alpha = 0.05 # Initialisation of boot_mean = rep(NA, B) for (i in 1:B){ student_work_star = student_work[sample(1:n, replace = TRUE)] boot_mean[i] = mean(student_work_star) } quantile(boot_mean, c(alpha/2, 1 - alpha/2)) ## 2.5% 97.5% ## 0.275 1.625 #hist(boot_mean, probability = TRUE) "],
["functions.html", "Chapter 6 Functions 6.1 Function arguments 6.2 Function environment 6.3 Function attributes", " Chapter 6 Functions This chapter aims at highlighting the main advantages, characteristics, arguments and structure of functions in R. As you already know, a function is a collection of logically connected commands and operations that allow the user to input some arguments and obtain a desired output (based on the given arguments) without having to rewrite the mentioned code each time that specific output is needed. Indeed, a common task in statistical research consists in running some simulation studies which give support (or not) to the use of a certain method of inference. In this case, it’s not efficient to rewrite the code each time it is needed within a simulation study because it would lead to lengthy code and increased risk of miss-coding. Considering this, in the previous chapter we discussed the non-parametric bootstrap which is a technique used to perform statistical inference in different cases (e.g. small sample sizes, uncertainty on the distribution of the underlying variable, etc.). However, there exist many other types of bootstrap techniques, including the parametric bootstrap which assumes a parametric model for the data (e.g. a Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) that represent the parameters). For this bootstrap technique, we assume that we know the underlying model generating the data and therefore estimate its parameters. Once this is done, we use the estimated parameters as if they were the true parameters and use them to simulate from the assumed model. In this manner we generate a parametric bootstrap sample on which we can compute the statistic we are intersted in (e.g. the sample mean), thereby obtaining a distribution for this statistic by simulating many other samples. What are the advantages of these different inferential methods? Which approach appears to be preferable to the others and under what circumstances? We can run some specific simulation studies to get an idea of the answers to these questions but, to do so, we will need to create functions the implement these methods. The next sections will present the main features of functions in R by studying their main components. R functions have three main components to them: body: the code lines containing the commands and operations which deliver the desired output; arguments: the inputs the user gives to the function which will determine the value or type of output of a function; environment: every R function is built within an enviroment of reference from which to source possible input values and/or other functions necessary for it to work. While the first component is the collection of commands that we saw in the previous chapters which deliver the output of interest, we will focus more on the last two components by building a function implementing the non-parametric bootstrap to find a confidence interval for the sample mean which we saw in the previous chapter: nonpar_boot &lt;- function(x, B = 500, alpha = 0.05) { boot_mean &lt;- rep(NA, B) n &lt;- length(x) for (i in 1:B) { sample_star &lt;- x[sample(1:n, replace = TRUE)] boot_mean[i] &lt;- mean(sample_star) } return(quantile(boot_mean, c(alpha/2, 1 - alpha/2))) } 6.1 Function arguments As you can see, the function’s name is “nonpar_boot( )” and it has three arguments to it: x which represents the original sample on which we would like to perform the bootstrap procedure; B which represents the number of bootstrap samples we wish to generate; alpha which determines the size of the confidence interval we wish to compute The function therefore has three arguments to it and these can be extracted by using the function formals(): formals(nonpar_boot) ## $x ## ## ## $B ## [1] 500 ## ## $alpha ## [1] 0.05 A feature which can be underlined at this stage is that, as can be seen for the above mentioned function, it is possible to provide default values for some, all or none of the function arguments. In this case, default values have been given for the arguments B and alpha, consequently the user does not need to specify the value of these arguments unless they wish to specify different values compared to the default values given. R functions are quite flexible when specifying the arguments. These can in fact be specified by exact name, prefix matching or position (and are given priority in this order). For example, one could specify the arguments for the above non-parametric bootstrap function as follows “nonpar_boot(B = 200, sim, a = 0.1)” where the object “sim” is the sample which in the function is named “x”. For a more in-depth overview of this feature refer to Wickham (2014). The definition of the argument values can be very flexible and can also be defined as functions of other arguments. For example, we could have defined the bootstrap function as follows: nonpar_boot_numb1 &lt;- function(x, B = ceiling(length(x)*alpha), alpha = 0.05) { boot_mean &lt;- rep(NA, B) n &lt;- length(x) for (i in 1:B) { sample_star &lt;- x[sample(1:n, replace = TRUE)] boot_mean[i] &lt;- mean(sample_star) } return(list(boot.number = B, boot.quant = quantile(boot_mean, c(alpha/2, 1 - alpha/2)))) } sim &lt;- rnorm(10e3) nonpar_boot_numb1(sim)$boot.number ## [1] 500 or define it through variables that are included within the function itself: nonpar_boot_numb2 &lt;- function(x, B = value, alpha = 0.05) { value &lt;- ceiling(length(x)*alpha) boot_mean &lt;- rep(NA, B) n &lt;- length(x) for (i in 1:B) { sample_star &lt;- x[sample(1:n, replace = TRUE)] boot_mean[i] &lt;- mean(sample_star) } return(list(boot.number = B, boot.quant = quantile(boot_mean, c(alpha/2, 1 - alpha/2)))) } nonpar_boot_numb2(sim)$boot.number ## [1] 500 6.2 Function environment 6.3 Function attributes References "],
["build-a-package.html", "Chapter 7 Build a package 7.1 Using Packages", " Chapter 7 Build a package Expalin how to build a package, I would add style guide and something on pkgdown 7.1 Using Packages Packages can be used in many ways. Here is an example of its practical uses. 7.1.1 Example: Making Maps birth_place = c(&quot;Glasgow, Scotland&quot;, &quot;Manacor, Spain&quot;, &quot;Lausanne, Switzerland&quot;, &quot;Belgrade, Serbia&quot;, &quot;Basel, Switzerland&quot;) library(ggmap) glasgow_coord = geocode(&quot;Glasgow, Scotland&quot;) glasgow_coord ## lon lat ## 1 -4.251806 55.86424 birth_coord = geocode(birth_place) birth_coord ## lon lat ## 1 -4.251806 55.86424 ## 2 3.209532 39.56972 ## 3 6.632273 46.51965 ## 4 20.448922 44.78657 ## 5 7.588576 47.55960 class(birth_coord) ## [1] &quot;data.frame&quot; birth_coord$Players = players birth_coord$GS = grand_slam_win birth_coord ## lon lat Players GS ## 1 -4.251806 55.86424 Andy Murray 9 ## 2 3.209532 39.56972 Rafael Nadal 15 ## 3 6.632273 46.51965 Stan Wawrinka 5 ## 4 20.448922 44.78657 Novak Djokovic 12 ## 5 7.588576 47.55960 Roger Federer 18 Let’s represent this information graphically. We haven’t seen how to make graph yet so don’t worry to much about the details of how this graph is made library(mapproj) map &lt;- get_map(location = &#39;Switzerland&#39;, zoom = 4) ggmap(map) + geom_point(data = birth_coord, aes(lon, lat, col = Players, size = GS)) + scale_size(name=&quot;Grand Slam Wins&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) "],
["chap-graphs.html", "Chapter 8 Graphics", " Chapter 8 Graphics In this chapter we discuss graphics with R. In this chapter, we will as an example the function sin(x) in the range \\(0, 2\\pi)\\). nb_points &lt;- 60 x &lt;- seq(from = 0, to = 4*pi, length.out = nb_points) y &lt;- sin(x) plot(x, y) In general, we found that the following approach is quite usefull, Step 1: Construct graph “frame”, i.e. basically an empty graph with the right title, labels, axis and so on. Often, this can be achieve with plot(). It is also useful to grid. Here is an example: plot(NA, xlim = range(x), ylim = range(y), main = &quot;my title&quot;, xlab = &quot;my xlab&quot;, ylab = &quot;my ylab&quot;, col.main = &quot;red&quot;, col.lab = &quot;darkgreen&quot;, col.axis = &quot;darkblue&quot;, cex.main = 2, cex.lab = 1, cex.axis = 0.5) grid() full link "],
["case-study-monte-carlo-integration.html", "Chapter 9 Case Study: Monte-Carlo Integration 9.1 Introduction 9.2 Properties 9.3 Implementation 9.4 Example: Normal Distribution 9.5 Example: Nonelementary integral 9.6 Problem: antithetic sampling", " Chapter 9 Case Study: Monte-Carlo Integration 9.1 Introduction Monte Carlo integration is a powerful technique for numerical integration. It is particularly useful to evaluate integrals of “high-dimension”. A detailed (and formal) discussion of this method is clearly beyond the scope of this class and we shall restrict our attention to most basic form(s) of Monte Carlo Integration and briefly discuss the rational behind this method. Originally, such Monte Carlo methods were known under various names among which statistical sampling was probably the most commonly used. In fact, the name Monte Carlo was popularized by several physics researchers, including Stanislaw Ulam, Enrico Fermi and John von Neumann. The name is believed to be a reference to a famous casino in Monaco where Stanislaw Ulam’s uncle would borrow money to gamble. Enrico Fermi was one of the first to this technique which he employed to study the properties newly-discovered neutron in the 1930s. Later, these methods played for example a central role in many of the simulations required for the Manhattan project. Suppose we are interested in computing the following integral: \\[I = \\int_a^b f(x) dx.\\] Of course, this integral can be approximated by a Riemann sum, \\[I \\approx \\Delta x \\sum_{i = 1}^n f(a + (i-1) \\Delta x),\\] where \\(\\Delta x = \\frac{b - a}{n}\\;\\) and the idea behind this approximation is that as the number of partions \\(n\\) increases the Riemann sum will become closer and closer to \\(I\\). Also (and under some technical conditions), we have that \\[I = \\lim_{n \\to \\infty} \\Delta x \\sum_{i = 1}^n f(a + (i-1) \\Delta x).\\] In fact, the rational of a Monte Carlo Integral is quite close to the Riemann sum since, in its most basic form, we approximate \\(I\\) by averaging samples of the function \\(f(x)\\) at uniform random point within the interval \\([a, b]\\). Therefore, the Monte Carlo estimator of \\(I\\) is given by \\[\\begin{equation} \\hat{I} = \\frac{b - a}{B} \\sum_{i = 1}^B f(X_i), \\tag{9.1} \\end{equation}\\] where \\(X_i = a + U_i (b - a)\\) and \\(U_i \\sim \\mathcal{U}(0,1)\\). In fact, (9.1) is quite intuitive as \\(\\frac{1}{B} \\sum_{i = 1}^B f(X_i)\\) represents an estimation of the average value of \\(f(x)\\) in the interval \\([a, b]\\) and thus \\(\\hat{I}\\) is simply the average value time the length of the interval, i.e. \\((b-a)\\). 9.2 Properties A more formal argument on the validity of this approach can be found in analyzing the statistical properties of the estimator \\(\\hat{I}\\). In order to do so, we start by considering its expected value \\[ \\mathbb{E}\\left[ \\hat{I} \\right] = \\frac{b - a}{B} \\sum_{i = 1}^B \\mathbb{E}\\left[ f(X_i) \\right] = \\frac{b - a}{B} \\sum_{i = 1}^B \\int f(x) g(x) dx, \\] where \\(g(x)\\) denotes the pdf of \\(X_i\\). Since \\(X_i \\sim \\mathcal{U}(a, b)\\) it follows that \\[ g(x) = \\left\\{ \\begin{array}{ll} \\frac{1}{b - a} &amp; \\mbox{if } x \\in [a, b] \\\\ 0 &amp; \\mbox{if } x \\not\\in [a, b] \\end{array} \\right. \\] Therefore, we have \\[ \\mathbb{E}\\left[ \\hat{I} \\right] = \\frac{b - a}{B} \\sum_{i = 1}^B \\int_a^b \\frac{f(x)}{b-a} dx = \\int_a^b f(x) dx = I, \\] Since \\(X_i\\) are iid, the same can be said about \\(f(X_i)\\) and therefore by the Strong Law of Large Numbers we have that \\(\\hat{I}\\) converge almost surely to \\(I\\), which means that \\[ \\mathbb{P}\\left(\\lim_{B \\to \\infty} \\hat{I} = I \\right) = 1. \\] This result implies that as the number of simulations \\(B\\) goes to infinity we can guarantee that the solution will be exact. If you are unfamiliar or don’t remeber well the Strong Law of Large Numbers we provide a reminder in the comment box below. IS THAT USEFUL? Unfortunately, this result does give us any information on how quickly this estimate converges to “sufficiently accurate” solution for the problem at hand. This can be done by studying the variance of \\(\\hat{I}\\) and its rate of convergence. Indeed, we have \\[ \\begin{aligned} \\operatorname{var} \\left( \\hat{I} \\right) &amp;= \\left(\\frac{b - a}{B}\\right)^2 \\sum_{i = 1}^B \\left\\{\\mathbb{E}\\left[f^2(X_i)\\right] - \\mathbb{E}^2\\left[f(X_i)\\right]\\right\\}\\\\ &amp;= \\frac{1}{B^2} \\sum_{i = 1}^B \\left\\{(b-a) \\int_a^b f^2(x) dx - \\left(\\int_a^b f(x) dx \\right)^2 \\right\\}\\\\ &amp;= \\frac{(b-a) I_2 - I^2}{B} \\end{aligned} \\] where \\(I_2 = \\int_a^b f^2(x) dx\\). A simple estimator of this quantity is given by \\[ \\hat{I}_2 = \\frac{b - a}{B} \\sum_{i = 1}^B f^2(X_i), \\] and therefore using \\(\\hat{I}\\) we obtain: \\[ \\widehat{\\operatorname{var}} \\left(\\hat{I} \\right) = \\frac{(b-a) \\hat{I}_2 - \\hat{I}^2}{B} = \\frac{b - a}{B^2} \\sum_{i = 1}^B\\left[ (b - a )f^2(X_i) - f(X_i)\\right] \\] Thus, it is easy to see that the rate of convergence of \\(\\widehat{\\operatorname{var}} \\left(\\hat{I} \\right)\\) is \\(B^{-1}\\) and we may write \\({\\operatorname{var}} \\left(\\hat{I} \\right) = \\mathcal{O}(B)\\). This implies that if we wish to reduce the error (or standard deviation) by half we need to quadruple \\(B\\). Such phenomon is very common in many research such as Statistics is often called the curse of dimensionality. 9.3 Implementation The function mc_int(), which is available in the support package, implements the above method. This functions has four inputs: x_range: A vector containing the integration domain, i.e. \\(a\\) and \\(b\\), fun: A string containing the function you wish to integrate where \\(x\\) is used to indicate the variable of integration, B: A numeric value to denote the number of Monte-Carlo replications, seed: A numeric to control the seed of the random number generator. For example, if you want to estimate \\[ \\int_1^3 \\frac{\\exp\\left(\\sin(x)\\right)}{x} dx, \\] using \\(10^4\\) Monte-Carlo replications, you can use the following command: mc_int(x_range = c(1,3), fun = &quot;exp(sin(x))/x&quot;, B = 10^5) ## $I ## [1] 2.558104 ## ## $var ## [1] 1.401222e-05 At this point, it is probably a good idea to try to programm this yourself and to compare your results (and code!) with the function mc_int(). This should be rather easy to implement but one thing that may be a little delicate is how to pass as an input the function you wish to integrate. A possible way of doing this is to use a string for this purpose so that, for example, if we have to integrate the function \\(\\sin(x)\\) you could simply write fun = sin(x) when calling your function. This implies that we should be able to “transform” a string into a function that we can evaluate, which is something that we can achieve by combining the functions eval and parse. An example is provided below: my_fun = &quot;x^2&quot; x = 0:3 eval(parse(text = my_fun)) ## [1] 0 1 4 9 If you having trouble to understand what these functions are doing have a look to their help files with write ?eval and ?parse. 9.4 Example: Normal Distribution Suppose that \\(X \\sim \\mathcal{N}(4, 1.25^2)\\) and that we are interested in computing the following probability \\(\\mathbb{P}\\left(1 &lt; X &lt; 4.5 \\right)\\). The probability density of the normal distribution for a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is given by: \\[ f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(- \\frac{\\left(x - \\mu\\right)^2}{2 \\sigma^2}\\right). \\] Therefore, the probability we are interested in can written as the following integral \\[ \\mathbb{P}\\left(1 &lt; X &lt; 4.5 \\right) = \\int_1^{4.5} \\frac{1}{\\sqrt{3.125 \\pi}} \\exp \\left(- \\frac{\\left(x - 4\\right)^2}{3.125}\\right). \\] Analytically, this is not an easy problem and of course there are many ways to solve it. However, we could try to use Monte-Carlo integral to solve it. For example: my_fun = &quot;1/sqrt(3.125*pi)*exp(-((x - 4)^2)/3.125)&quot; (prob = mc_int(x_range = c(1, 4.5), fun = my_fun, B = 10^7)) ## $I ## [1] 0.6471691 ## ## $var ## [1] 1.449985e-08 Based on this result, we can write \\(\\mathbb{P}\\left(1 &lt; X &lt; 4.5 \\right) \\approx 64.72 \\%\\) with a standard error of about 0.01%. We can compare our results with what we would obtain with the function pnorm which provide a nearly exact result: pnorm(4.5, 4, 1.25) - pnorm(1, 4, 1.25) ## [1] 0.6472242 This shows that our estimation is within one standard error of a near perfect result. 9.5 Example: Nonelementary integral Layman’s terms, a nonelementary integral of a given (elementary) function is an integral that is cannot be expressed as an elementary function. The French mathematicien Joseph Liouville was the first to proof the existing of such nonelementary integral. An well-known example of such integrals are the Fresnel integrals, which have been used for a very wide range of applications going from the computation of electromagnetic field intensity to roller roster design. These integrals are defined as: \\[S(y) = \\int_0^t \\sin\\left(x^2\\right) dx \\;\\;\\;\\;\\;\\; \\text{and} \\;\\;\\;\\;\\;\\; C(y) = \\int_0^y \\cos \\left( x^2 \\right) dx.\\] In this example, we will only consider \\(S(y)\\). In general it is believed that the most convenient way of evaluating these functions to arbitrary precision is to use power series representation that converges for all \\(y\\): \\[ S(y) = \\sum_{i = 1}^\\infty \\, \\left(-1\\right)^n \\, \\frac{y^{4i + 1}}{\\left(2i + 1\\right) \\, !\\left(4i + 3\\right)}. \\] In this example, we will study the estimation of \\(S(\\pi)\\) as well as the precision of this estimation. B = 4^(4:13) results = matrix(NA, length(B), 2) for (i in 1:length(B)){ mc_res = mc_int(c(0, 2), &quot;sin(x^2)&quot;, B = B[i], seed = i+12) results[i, ] = c(mc_res$I, sqrt(mc_res$var)) } trans_blue = hcl(h = seq(15, 375, length = 3), l = 65, c = 100, alpha = 0.15)[2] plot(NA, xlim = range(B), ylim = range(cbind(results[, 1] + results[,2], results[, 1] -results[,2])), log = &quot;x&quot;, ylab = &quot;Estimated Integral&quot;, xlab = &quot;Number of Simulations B&quot;, xaxt = &#39;n&#39;) grid() axis(1, at = B, labels = parse(text = paste(&quot;4^&quot;, 4:13, sep = &quot;&quot;))) polygon(c(B, rev(B)), c(results[, 1] + results[, 2], rev(results[, 1] - results[, 2])), border = NA, col = trans_blue) lines(B, results[, 1], type = &quot;b&quot;, col = &quot;blue4&quot;, pch = 16) abline(h = 0.8048208, col = &quot;red4&quot;, lty = 2) legend(&quot;topright&quot;, c(&quot;Estimated value&quot;, &quot;Standard error interval&quot;, &quot;Good approximation (MatLab)&quot;), bty = &quot;n&quot;, pch = c(16, 15, NA), lwd = c(1, NA, 1), lty = c(1, NA, 2), pt.cex = c(1, 2, NA), col = c(&quot;blue4&quot;, trans_blue, &quot;red4&quot;)) 9.6 Problem: antithetic sampling As we saw in the previous sections, “plain” Monte Carlo integration typically has an error variance of the form \\(\\sigma^2/B\\) (in the case of the estimator we consider previously we have \\(\\sigma^2 = (b-a)I_2 - I^2\\)). Therefore, to improve our answer we can sample with a larger value of \\(B\\), but this comes at the cost of a greater computational cost. Sometimes, we can find ways to \\(\\sigma^2\\) instead by modifying our estimator of \\(I\\). Methods to do this are known as variance reduction techniques, one them is know as antithetic sampling. Th "],
["basic-probability-and-statistics.html", "Chapter 10 Basic Probability and Statistics 10.1 Probability Distributions 10.2 Summary Statistics", " Chapter 10 Basic Probability and Statistics 10.1 Probability Distributions For probability distributions, we can compute basic values such as density, distribution, quantiles, random sampling using R built-in functions below. -dname calculates density (pdf) at input x. -pname calculates distribution (cdf) at input x. -qname calculates the quantile at an input probability. -rname generates a random draw from a particular distribution. Note that we will replace name with the name of a given distribution. For example, a a random number generator for uniform distribution will be runif. To learn more about specific parameters and documentation for a function, type ? before the function name into your R console. Common example probability distributions include, but are not limited to, binomial, chi-square, exponential, F, geometric, logistic, normal, poisson, t, and uniform. Here’s an example. Problem Assume that the test scores of a college entrance exam fits a normal distribution. Furthermore, the mean test score is 70, and the standard deviation is 15. What is the percentage of students scoring 90 or more in the exam? Answer In this case, we consider a random variable \\(X\\) that is normally distributed \\(X \\sim N(\\mu=70, \\sigma^2=25)\\). Since we are looking for the percentage of students scoring higher than 90, we are interested in the upper tail of the normal distribution. pnorm(q = 90, mean = 70, sd = 15, lower.tail = FALSE) ## [1] 0.09121122 We see that there is a 9% probability of students scoring 90 or more in the exam. Note that at ?pnorm we see an additional parameter, lower.tail that indicates TRUE (default) if probabilities are P[X ≤ x] and FALSE otherwise, where P[X &gt; x]. 10.2 Summary Statistics 10.2.1 Numeric Input For central tendency or spread statistics of numeric input, we can use the following R build-in functions below. mean calculates the mean of an input x. median calculates the median of an input x. var calculates the variance of an input x. sd calculates the standard deviation of an input x. IQR calculates the interquartile range of an input x. min calculates the minimum value of an input x. max calculates the maximum value of an input x. range returns a vector containing the minimum and maximum of all given arguments. 10.2.2 Factor Input For factor input, we can extract counts and percentages as summary by using table. Here I create an example dataset with colors 20 that are Yellow, 10 that are Green and 50 that are Blue. (my_colors = as.factor(c(rep(&quot;Yellow&quot;, 20), rep(&quot;Green&quot;, 10), rep(&quot;Blue&quot;, 50)))) ## [1] Yellow Yellow Yellow Yellow Yellow Yellow Yellow Yellow Yellow Yellow ## [11] Yellow Yellow Yellow Yellow Yellow Yellow Yellow Yellow Yellow Yellow ## [21] Green Green Green Green Green Green Green Green Green Green ## [31] Blue Blue Blue Blue Blue Blue Blue Blue Blue Blue ## [41] Blue Blue Blue Blue Blue Blue Blue Blue Blue Blue ## [51] Blue Blue Blue Blue Blue Blue Blue Blue Blue Blue ## [61] Blue Blue Blue Blue Blue Blue Blue Blue Blue Blue ## [71] Blue Blue Blue Blue Blue Blue Blue Blue Blue Blue ## Levels: Blue Green Yellow By using table we can see a frequency (count) table of the colors. table(my_colors) ## my_colors ## Blue Green Yellow ## 50 10 20 By diving the table by the number of total colors, we can get percentages instead. table(my_colors) / length(my_colors) ## my_colors ## Blue Green Yellow ## 0.625 0.125 0.250 10.2.3 Dataset For some datasets, there is another convenient way to get simple summary statistics. In this example, we will explore the Iris flower dataset contained in the R built-in datasets package. The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other. We can use summary to output the minimum, 1st quartile, median, mean, 3rd quartile, and maximum statistics, and frequency counts for factor inputs. summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## We can also str to get a basic feel for what the dataset looks like, in terms of feature types and the values that each feature contains. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Note that for big datasets, the computation time will be substantially longer. Hello World "]
]
