[
["data.html", "Chapter 2 Data Structures 2.1 Vectors 2.2 Matrix 2.3 Array 2.4 List 2.5 Dataframe 2.6 Data frames", " Chapter 2 Data Structures There are different data types that are commonly used in R among which the most important ones are the following: Numeric (or double): these are used to store real numbers. Examples: -4, 12.4532, 6. Integer: examples: 2L, 12L. Logical (or boolean): examples: TRUE, FALSE. Character: examples: &quot;a&quot;, &quot;Bonjour&quot;. In R there are basically five types of data structures in which elements can be stored. A data structure is said to homogeneous if it only contains elements of the same type (for example it only contains character or numeric values) and heterogenous if it contains elements of more than one type. The five types of data structrures are commonly summarized in a table similar to the one below: Dimension Homogenous Heterogeneous 1 Vector List 2 Matrix Dataframe n Array To illustrate how to use these data structures, we will consider the simple data set of the five best male single tennis players (as ranked by ATP on 07-15-2017). The data are presented in the table below: Name Date of Birth Born Country ATP Ranking Prize Money Win Percentage Grand Slam Wins Andy Murray 15 May 1987 Glasgow, Scotland Great Britain 1 60,449,649 78.07 9 Rafael Nadal 3 June 1986 Manacor, Spain Spain 2 85,920,132 82.48 15 Stan Wawrinka 28 March 1985 Lausanne, Switzerland Switzerland 3 30,577,981 63.96 5 Novak Djokovic 22 May 1987 Belgrade, Serbia Serbia 4 109,447,408 82.77 12 Roger Federer 8 August 1981 Basel, Switzerland Switzerland 5 104,445,185 81.80 18 It can be noticed that this data set contains columns with a variety of data types and in the next sections we will focus on these different types separately. 2.1 Vectors A vector has three important properties: Type, which corresponds the “kind” of objects in contains. It is possible to use the function typeof() to evaluate the type of objects in a vector. Length, i.e. the number of elements in a vector. This information can be obtained using the function length(). Attributes, some additional metadata attached to a vector. The functions attr() and attributes() can be used to store and retrive attributes (more details can be found in Section 2.1.4) c() is a generic function which combines arguments to form a vector. All arguments are coerced to a common type which is the type of the returned value, and all attributes except names are removed. To learn more about what a specific function’s documentation, type ?yourfunction onto the R Studio Console. For example, let us consider the number of grand slams won by the five players we are considering which are reported in the eigth column of the dataset: grand_slam_win = c(9, 15, 5, 12, 18) To display the values stored in grand_slam_win we could simply enter the following in the R console: grand_slam_win ## [1] 9 15 5 12 18 Alteratively, we could have created and displayed the value by using () around the definition of the object itself as follows: (grand_slam_win = c(9, 15, 5, 12, 18)) ## [1] 9 15 5 12 18 Various forms of “nested concatenation” can be used to defined vectors, for example we could also define grand_slam_win as (grand_slam_win = c(9, c(15, 5, c(12, c(18))))) ## [1] 9 15 5 12 18 This approach is often used to assemble vectors in various ways. It is also possible to define vector with characters, for example we could define a vector with the player names as follows: (players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;)) ## [1] &quot;Andy Murray&quot; &quot;Rafael Nadal&quot; &quot;Stan Wawrinka&quot; &quot;Novak Djokovic&quot; ## [5] &quot;Roger Federer&quot; 2.1.1 Type We can evaluate the kind or type of elements that are stored in a vector using the function typeof(). For example, for the vectors we just created we obtain: typeof(grand_slam_win) ## [1] &quot;double&quot; typeof(players) ## [1] &quot;character&quot; This is a little surprising as all the elements in grand_slam_win are integers and it would therefore seem natural to expect this as an output of the function typeof(). This is because R considers any number as a “double” by default, except when adding the suffix L after an integer. For example: typeof(1) ## [1] &quot;double&quot; typeof(1L) ## [1] &quot;integer&quot; Therefore, we could express grand_slam_win as follows: (grand_slam_win_int = c(9L, 15L, 5L, 12L, 18L)) ## [1] 9 15 5 12 18 typeof(grand_slam_win_int) ## [1] &quot;integer&quot; Naturally, the difference between the two in general is relatively unimportant but we can see that grand_slam_win_int takes less “space” among the two. Indeed we have object.size(grand_slam_win) ## 88 bytes object.size(grand_slam_win_int) ## 72 bytes 2.1.2 Coercion As indicated earlier, a vector has a homogenous data structure meaning that it can only contain a single type among all the data types. Therefore, when more than one data type is provided, R coerces the data into a “shared” type. To identify this “shared” type we can use this simple rule: \\[\\begin{equation*} \\text{logical} &lt; \\text{integer} &lt; \\text{numeric} &lt; \\text{character}, \\end{equation*}\\] which simply means that if a vector contains more than one data type, the “shared” type will be that of the “largest” type according to the above equations. Here are a few examples: # Logical + integer (mix_logic_int = c(TRUE, 1L)) ## [1] 1 1 typeof(mix_logic_int) ## [1] &quot;integer&quot; # Logical + character (mix_logic_char = c(TRUE, &quot;Hi&quot;)) ## [1] &quot;TRUE&quot; &quot;Hi&quot; typeof(mix_logic_char) ## [1] &quot;character&quot; # Integer + numeric (mix_int_num = c(1, 1L)) ## [1] 1 1 typeof(mix_int_num) ## [1] &quot;double&quot; # Integer + character (mix_int_char = c(1L, &quot;Hi&quot;)) ## [1] &quot;1&quot; &quot;Hi&quot; typeof(mix_int_char) ## [1] &quot;character&quot; 2.1.3 Subsetting Naturally, it is possible to “subset” the values of in our vector in many ways. Essentially, there are four main ways of subsetting a vector. Here we’ll only discuss the first three: Positive Index: We can access or subset the \\(i\\)-th element of a vector by simply using grand_slam_win[i] where \\(i\\) is a positive number between 1 and length of the vector. # Accesing the first element grand_slam_win[1] ## [1] 9 # Accesing the third and first value grand_slam_win[c(3, 1)] ## [1] 5 9 # Duplicated indices yield duplicated values grand_slam_win[c(1, 1, 2, 2, 3, 4)] ## [1] 9 9 15 15 5 12 Negative Index: We remove elements in a vector using negative indices: # Removing the second obervation grand_slam_win[-2] ## [1] 9 5 12 18 # Removing the first and fourth obserations grand_slam_win[c(-1, -4)] ## [1] 15 5 18 Logical Indices: Another useful approach is based on logical operators: # Access the first and fourth observations grand_slam_win[c(TRUE, FALSE, FALSE, TRUE, FALSE)] ## [1] 9 12 Here we could add some remarks on weird cases, for example grand_slam_win[c(1.2, 3.4)] (which rounds things up) or grand_slam_win[c(-1, 2)] (which doesn’t work as “mixed” indices are not permitted). 2.1.4 Attributes Let’s suppose that we conducted an experiment under specific conditions, say a date and a place which should be stored as attributes of the object containing the results of this experiment. Indeed, objects can have arbitrary additional attributes that are used to store metadata on the object of interest. For example: attr(grand_slam_win, &quot;date&quot;) = &quot;07-15-2017&quot; attr(grand_slam_win, &quot;type&quot;) = &quot;Men, Single&quot; To display the vector with its attributes grand_slam_win ## [1] 9 15 5 12 18 ## attr(,&quot;date&quot;) ## [1] &quot;07-15-2017&quot; ## attr(,&quot;type&quot;) ## [1] &quot;Men, Single&quot; To only display the attributes we have attributes(grand_slam_win) ## $date ## [1] &quot;07-15-2017&quot; ## ## $type ## [1] &quot;Men, Single&quot; It is also possible to extract a specific attribute attr(grand_slam_win, &quot;date&quot;) ## [1] &quot;07-15-2017&quot; 2.1.5 Adding labels In some cases it might be useful to add labels to vectors. For example, we could define the vector grand_slam_win and use the player’s names as labels, i.e. (grand_slam_win = c(&quot;Andy Murray&quot; = 9, &quot;Rafael Nadal&quot; = 15, &quot;Stan Wawrinka&quot; = 5, &quot;Novak Djokovic&quot; = 12, &quot;Roger Federer&quot; = 18)) ## Andy Murray Rafael Nadal Stan Wawrinka Novak Djokovic Roger Federer ## 9 15 5 12 18 The main advantage of this approach is that the number of grand slams won can now be referred to by the player’s name. For example: grand_slam_win[&quot;Andy Murray&quot;] ## Andy Murray ## 9 grand_slam_win[c(&quot;Andy Murray&quot;,&quot;Roger Federer&quot;)] ## Andy Murray Roger Federer ## 9 18 All labels (players’ names in our case) can be obtained witht the function names, i.e. names(grand_slam_win) ## [1] &quot;Andy Murray&quot; &quot;Rafael Nadal&quot; &quot;Stan Wawrinka&quot; &quot;Novak Djokovic&quot; ## [5] &quot;Roger Federer&quot; 2.1.6 Useful functions with vectors The reason for extracting or creating vectors often lies in the need to collect information from them. For this purpose, a series of useful functions are available that allow to extract information or arrange the vector elements in a certain manner which can be of interest to the user. Among the most commonly used functions we can find the following ones length() sum() mean() sort() and order() whose name is self-explicative in most cases. For example we have length(grand_slam_win) ## [1] 5 sum(grand_slam_win) ## [1] 59 mean(grand_slam_win) ## [1] 11.8 To sort the players by number of grand slam wins, we could use the function order() which returns the position of the elements of a vector sorted in an ascending order, order(grand_slam_win) ## [1] 3 1 4 2 5 Therefore, we can sort the players in ascending order of wins as follows players[order(grand_slam_win)] ## [1] &quot;Stan Wawrinka&quot; &quot;Andy Murray&quot; &quot;Novak Djokovic&quot; &quot;Rafael Nadal&quot; ## [5] &quot;Roger Federer&quot; which implies that Roger Federer won most grand slams. Another related function is sort() which simply sorts the elements of a vector in an ascending manner. For example, sort(grand_slam_win) ## Stan Wawrinka Andy Murray Novak Djokovic Rafael Nadal Roger Federer ## 5 9 12 15 18 which is compact version of grand_slam_win[order(grand_slam_win)] ## Stan Wawrinka Andy Murray Novak Djokovic Rafael Nadal Roger Federer ## 5 9 12 15 18 There are of course many other useful functions that allow to deal with vectors which we will not mention in this section but can be found in a variety of references (see for example Wickham (2014)). 2.1.7 Creation sequences When uing R for statistical programming or even data analysis it is very common to create sequences of numbers. Here are three common ways used to create such sequences: from:to: This method is quite inituitive and very compact. For example: (x = 1:3) ## [1] 1 2 3 (y = 3:1) ## [1] 3 2 1 (w = -1:-4) ## [1] -1 -2 -3 -4 (z = 1.3:3) ## [1] 1.3 2.3 seq_len(n): This function provides a simple way to generate a sequence from 1 to an arbitrary number n. In general, 1:n and seq_len(n) are equivalent with the notable exeptions where n = 0 and n &lt; 0. The reason for these exeptions will become clear in Section ??. Let’s see a few examples: n = 3 1:n ## [1] 1 2 3 seq_len(n) ## [1] 1 2 3 n = 0 1:n ## [1] 1 0 seq_len(n) ## integer(0) seq(a, b, by/length.out = d): This function can be used to create more “complex” sequences. It can either be used to create a sequence from a to b by increments of d (using the option by) or of a total length of d (using the option length.out). A few examples: (x = seq(1, 2.8, by = 0.4)) ## [1] 1.0 1.4 1.8 2.2 2.6 (y = seq(1, 2.8, length.out = 6)) ## [1] 1.00 1.36 1.72 2.08 2.44 2.80 It could be interesting to use a function like rep() that allows to create vectors with repeated values or sequences, for example: rep(c(1,2), times = 3, each = 1) ## [1] 1 2 1 2 1 2 rep(c(1,2), times = 1, each = 3) ## [1] 1 1 1 2 2 2 where the option times allows to specify how many times the object needs to be repeated and each regulates how many times each element in the object is repeated. 2.1.8 Example: Apple Stock Price Suppose that one is interested in analysing the behavior of Apple’s stock price over the last three months. The first thing that is needed is today’s date which can be obtained as follows (today = Sys.Date()) ## [1] &quot;2017-08-03&quot; Once this is done, we can obtain the date which is exactly three monmths ago (three_months_ago = seq(today, length = 2, by = &quot;-3 months&quot;)[2]) ## [1] &quot;2017-05-03&quot; With this information, we can now download Apple’s stock price and represent these stocks through a candlestick chart which summarizes information on daily opening and closing prices as well as minimum and maximum prices. Just to note, library() loads and attaches add-on packages. library(quantmod) getSymbols(&quot;AAPL&quot;, from = three_months_ago, to = today) ## [1] &quot;AAPL&quot; candleChart(AAPL, theme=&#39;white&#39;, type=&#39;candles&#39;) Once we have the prices, we can compute some returns which are defined as follows \\[\\begin{equation} r_t = \\frac{S_t - S_{t-1}}{S_{t-1}} \\end{equation}\\] where \\(r_t\\) are the returns at time t and \\(S_t\\) is the stock price. This is implemented in the function ClCl() within the quantmod package. For example, we can create a vector of returns as follows AAPL_returns = as.numeric(na.omit(ClCl(AAPL))) where na.omit is used to remove missing values in the stock prices vector since, if we have \\(n+1\\) stock prices, we will only have \\(n\\) returns and as.numeric is used to transform the computed returns into a numeric vector. We can now compute the mean and median of the returns over the considered period. mean(AAPL_returns) ## [1] 0.001137776 median(AAPL_returns) ## [1] 0.002749398 However, a statistic that is of particular interest to financial operators is the Excess Kurtosis which, for a random variable that we denote as \\(X\\), can be defined as \\[\\begin{equation} \\text{Kurt} = \\frac{{E}\\left[\\left(X - E[X]\\right)^4\\right]}{\\left({E}\\left[\\left(X - E[X]\\right)^2\\right]\\right)^2} - 3 \\end{equation}\\] The reason for defining this statistic as Excess Kurtosis lies in the fact that the standardized kurtosis is compared to that of a Gaussian distribution (whose kurtosis is equal to 3) which has exponentially decaying tails. Consequently, if the Excess Kurtosis is positive, this implies that the distribution has heavier tails than a Gaussian and therefore has higher probabilities of extreme events occurring. Given this statistic, it is useful to compute this on the observed data and for this purpose a common estimator of the excess Kurtosis is \\[\\begin{equation} k = \\frac{\\frac{1}{n} \\sum_{t = 1}^{n} \\left(r_t -\\bar{r}\\right)^4}{\\left(\\frac{1}{n} \\sum_{t = 1}^{n} \\left(r_t -\\bar{r}\\right)^2 \\right)^2} - 3 \\end{equation}\\] where \\(\\bar{k}\\) denotes the sample average of the returns, i.e. \\[\\begin{equation} \\bar{k} = \\frac{1}{n} \\sum_{i = 1}^n r_i \\end{equation}\\] mu = mean(AAPL_returns) (k = mean((AAPL_returns - mu)^4)/(mean((AAPL_returns - mu)^2))^2 - 3) ## [1] 2.815774 which is quite high tends to indicate the returns have a heavier tails than the normal distribution. 2.2 Matrix Matrices are another extremely common data structure in R. Compared to vectors, matrices have an additional dimension which, for example, allows to stock multiple equidimensional vectors within the same object. Below is an example of how to create a matrix in R: (mat = matrix(1:12, ncol = 4, nrow = 3)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Notice that the first argument to the function is a vector (in this case a vector with increasing elements from 1 to 12) which is then transformed into a matrix with four columns (ncol = 4) and three rows (nrow = 3). By default, the vectors are transformed into matrices by placing the elements by column (i.e. starting from the top of the first column to the bottom and then passing to the following column until all columns are full). If you wish to fill the matrix by row, all you need to do is specify the argument byrow = T. Usually the length of the vector (i.e. number of elements in the vector) is the result of the multiplication between the number of columns and number of rows. What happens if the vector has fewer elements for the same matrix dimension? What happens if the vector has more elements? It is often the case however that we already have equidimensional vectors available and we wish to stock them into a matrix. In these cases, two useful functions are cbind() and rbind() where the first function stocks the vectors vertically side-by-side while the second stocks the vectors horizontally one below the other. An example of the former is given below: players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;) grand_slam_win = c(9, 15, 5, 12, 18) win_percentage = c(78.07, 82.48, 63.96, 82.77, 81.80) (mat = cbind(grand_slam_win, win_percentage)) ## grand_slam_win win_percentage ## [1,] 9 78.07 ## [2,] 15 82.48 ## [3,] 5 63.96 ## [4,] 12 82.77 ## [5,] 18 81.80 The result in this case is a \\(5 \\times 2\\) matrix (while with rbind() it would have been a \\(2 \\times 5\\) matrix). Once the matrix is defined, we can assign names to its rows and columns by using resepctively rownames and colnames. Of course, the number of names must correspond to the respective matrix dimensions as shown in the following example where each row corresponds to a specific player (thereby using the players vector) and each column corresponds to a specific statistic on the players. rownames(mat) &lt;- players colnames(mat) &lt;- c(&quot;GS win&quot;, &quot;Win rate&quot;) mat ## GS win Win rate ## Andy Murray 9 78.07 ## Rafael Nadal 15 82.48 ## Stan Wawrinka 5 63.96 ## Novak Djokovic 12 82.77 ## Roger Federer 18 81.80 2.2.1 Subsetting As for vectors, it is possible to subset the elements of a matrix. However, in the case of matrices we’re dealing with two-dimensional data structures and it is therefore necessary to specify the position of the elements of interest in both dimensions. For this purpose, as with vectors, we can use [ ] but, as opposed to vectors, we need to add , within the square brackets where the rows are specified before the comma and the columns after it. Below are a few examples: mat[c(&quot;Stan Wawrinka&quot;, &quot;Roger Federer&quot;), ] ## GS win Win rate ## Stan Wawrinka 5 63.96 ## Roger Federer 18 81.80 mat[c(1, 3), ] ## GS win Win rate ## Andy Murray 9 78.07 ## Stan Wawrinka 5 63.96 mat[, 2] ## Andy Murray Rafael Nadal Stan Wawrinka Novak Djokovic Roger Federer ## 78.07 82.48 63.96 82.77 81.80 mat[1:3, 1] ## Andy Murray Rafael Nadal Stan Wawrinka ## 9 15 5 It can be noticed that, when a space is left blank before or after the comma, this means that respectively all the rows or all the columns are considered. 2.2.2 Linear Algebra As with vectors, there are some useful functions that can be used with matrices. A first example is the function dim() that allows to determine the dimension of a matrix. For example, consider the following \\(4 \\times 2\\) matrix \\[\\begin{equation*} \\mathbf{A} = \\left[ \\begin{matrix} 1 &amp; 5\\\\ 2 &amp; 6\\\\ 3 &amp; 7\\\\ 4 &amp; 8 \\end{matrix} \\right] \\end{equation*}\\] which can be created in R as follows: (A = matrix(1:8, 4, 2)) ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 ## [4,] 4 8 Therefore, we expect dim(A) to retrun the vector c(4, 2). Indeed, we have dim(A) ## [1] 4 2 Next, we consider the function t() allows transpose a matrix. For example, \\(\\mathbf{A}^T\\) is equal to: \\[\\begin{equation*} \\mathbf{A}^T = \\left[ \\begin{matrix} 1 &amp; 2 &amp; 3 &amp; 4\\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\end{matrix} \\right], \\end{equation*}\\] which is a \\(2 \\times 4\\) matrix. In R, we achieve this as follows (At &lt;- t(A)) ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 dim(At) ## [1] 2 4 Aside from playing with matrix dimensions, matrix algebraic operations have specific commands. For example, the operator %*% is used in R to denote matrix multiplication while, as opposed to scalar objects, the regular product operator * performs the Hadamard product (or element by element product) when applied to matrices. For example, consider the following matrix product \\[\\begin{equation*} \\mathbf{B} = \\mathbf{A}^T \\mathbf{A} = \\left[ \\begin{matrix} 30 &amp; 70\\\\ 70 &amp; 174 \\end{matrix} \\right], \\end{equation*}\\] which can be done in R as follows: (B = At %*% A) ## [,1] [,2] ## [1,] 30 70 ## [2,] 70 174 Other common matrix operations include finding the determinant of a matrix and finding its inverse. These are often used, for example, when computing the likelihood function for a variable following a Gaussian distribution or when simulating time series or spatial data. The functions that perform these operations are det() and solve() that respectively find the determinant and the inverse of a matrix (which necessarily has to be square). The function det() can be used to compute the determinant of a (squared) matrix. In the case of a \\(2 \\times 2\\) matrix, there exists a simple solution for the determinant which is \\[\\begin{equation*} \\text{det} \\left( \\mathbf{D} \\right) = \\text{det} \\left( \\left[ \\begin{matrix} d_1 &amp; d_2\\\\ d_3 &amp; d_4 \\end{matrix} \\right] \\right) = d_1 d_4 - d_2 d_3. \\end{equation*}\\] Consider the matrix \\(\\mathbf{B}\\), we have \\[\\begin{equation*} \\text{det} \\left( \\mathbf{B}\\right) = 30 \\cdot 174 - 70^2 = 320. \\end{equation*}\\] In R, we can simply do det(B) ## [1] 320 The function solve() is also an important function when working with matrices as it allows to inverse a matrix. It is worth remembering that a square matrix that is not invertible (i.e. \\(\\mathbf{A}^{-1}\\) doesn’t exist) is called singular and the determinant offers a way to “check” if this is the case for a given matrix. Indeed, a square matrix is singular if and only if its determinant is 0. Therefore, in the case of \\(\\mathbf{B}\\), we should be able to compute its inverse. As for the determinant, there exists a formula to compute the inverse of \\(2 \\times 2\\) matrices, i.e. \\[\\begin{equation*} \\mathbf{D}^{-1} = \\left[ \\begin{matrix} d_1 &amp; d_2\\\\ d_3 &amp; d_4 \\end{matrix} \\right]^{-1} = \\frac{1}{\\text{det}\\left( \\mathbf{D} \\right)} \\left[ \\begin{matrix} \\phantom{-}d_4 &amp; -d_2\\\\ -d_3 &amp; \\phantom{-}d_1 \\end{matrix} \\right]. \\end{equation*}\\] Considering the matrix \\(\\mathbf{B}\\), we obtain \\[\\begin{equation*} \\mathbf{B}^{-1} = \\left[ \\begin{matrix} 30 &amp; 70\\\\ 70 &amp; 174 \\end{matrix} \\right]^{-1} = \\frac{1}{320}\\left[ \\begin{matrix} \\phantom{-}174 &amp; -70\\\\ -70 &amp; \\phantom{-}30 \\end{matrix} \\right] = \\end{equation*}\\] (B_inv = solve(B)) ## [,1] [,2] ## [1,] 0.54375 -0.21875 ## [2,] -0.21875 0.09375 Finally, we can verify that \\[\\begin{equation*} \\mathbf{G} = \\mathbf{B} \\mathbf{B}^{-1}, \\end{equation*}\\] should be equal to the identity matrix, (G = B %*% B_inv) ## [,1] [,2] ## [1,] 1 -8.881784e-16 ## [2,] 0 1.000000e+00 The result is of course extremely close but \\(\\mathbf{G}\\) is not exactly equal to the identity matrix due to rounding and other numerical errors. Another function of interest is the function diag() that can be used to extract the diagonal of a matrix. For example, we have \\[\\begin{equation*} \\text{diag} \\left( \\mathbf{B} \\right) = \\left[30 \\;\\; 174\\right], \\end{equation*}\\] which can be done in R as follows: diag(B) ## [1] 30 174 Therefore, the function diag() allows to easily compute the trace of matrix (i.e. the sum of the diagonal elements). For example, \\[\\begin{equation*} \\text{tr} \\left( \\mathbf{B} \\right) = 204, \\end{equation*}\\] or in R sum(diag(B)) ## [1] 204 Another use of the function diag() is to create diagonal matrices. Indeed, if the argument of this function is a vector, its behavior is the following: \\[\\begin{equation*} \\text{diag} \\left(\\left[a_1 \\;\\; a_2 \\;\\; \\cdots \\;\\; a_n\\right]\\right) = \\left[ \\begin{matrix} a_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; a_2 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; a_n \\end{matrix} \\right]. \\end{equation*}\\] Therefore, this provides a simple way of creating an identity matrix by combining the functions diag() and rep() (discussed in the previous section) as follows: n = 4 (ident = diag(rep(1, n))) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 2.2.3 Example: Summary Statistics with Matrix Notation A simple example of the operations we discussed in the previous section is given by many common statistics that can be reexpressed using matrix notation. As an example, we will consider three common statistics that are the sample mean, variance and covariance. Let us consider the following two samples of size \\(n\\) \\[\\begin{equation*} \\begin{aligned} \\mathbf{x} &amp;= \\left[x_1 \\;\\; x_2 \\; \\;\\cdots \\;\\; x_n\\right]^T\\\\ \\mathbf{y} &amp;= \\left[y_1 \\;\\;\\; y_2 \\; \\;\\;\\cdots \\;\\;\\; y_n\\right]^T. \\end{aligned} \\end{equation*}\\] The sample mean of \\(\\mathbf{x}\\) is \\[\\begin{equation*} \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^{n} x_i, \\end{equation*}\\] and its sample variance is \\[\\begin{equation*} s_x^2 = \\frac{1}{n} \\sum_{i = 1}^n \\left(x_i - \\bar{x}\\right)^2. \\end{equation*}\\] The sample covariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is \\[\\begin{equation*} s_{x,y} = \\frac{1}{n} \\sum_{i = 1}^n \\left(X_i - \\bar{x}\\right) \\left(Y_i - \\bar{y}\\right), \\end{equation*}\\] where \\(\\bar{y}\\) denotes the sample mean of \\(\\mathbf{y}\\). Consider the sample mean, this statistic can be expressed in matrix notation as follows \\[\\begin{equation*} \\bar{x} = \\frac{1}{n} \\sum_{i = 1}^{n} x_i = \\frac{1}{n} \\mathbf{x}^T \\mathbf{1}, \\end{equation*}\\] where \\(\\mathbf{1}\\) is a column vector of \\(n\\) ones. \\[\\begin{equation*} \\begin{aligned} s_x^2 &amp;= \\frac{1}{n} \\sum_{i = 1}^n \\left(x_i - \\bar{x}\\right)^2 = \\frac{1}{n} \\sum_{i = 1}^n x_i^2 - \\bar{x}^2 = \\frac{1}{n} \\mathbf{x}^T \\mathbf{x} - \\bar{x}^2\\\\ &amp;= \\frac{1}{n} \\mathbf{x}^T \\mathbf{x} - \\left(\\frac{1}{n} \\mathbf{x}^T \\mathbf{1}\\right)^2 = \\frac{1}{n} \\left(\\mathbf{x}^T \\mathbf{x} - \\frac{1}{n} \\mathbf{x}^T \\mathbf{1} \\mathbf{1}^T \\mathbf{x}\\right)\\\\ &amp;= \\frac{1}{n}\\mathbf{x}^T \\left( \\mathbf{I} - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T \\right) \\mathbf{x} = \\frac{1}{n}\\mathbf{x}^T \\mathbf{H} \\mathbf{x}, \\end{aligned} \\end{equation*}\\] where \\(\\mathbf{H} = \\mathbf{I} - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T\\). This matrix is often called the centering matrix. Similarly, for the sample covariance we obtain \\[\\begin{equation*} \\begin{aligned} s_{x,y} &amp;= \\frac{1}{n} \\sum_{i = 1}^n \\left(x_i - \\bar{x}\\right) \\left(y_i - \\bar{y}\\right) = \\frac{1}{n}\\mathbf{x}^T \\mathbf{H} \\mathbf{y}. \\end{aligned} \\end{equation*}\\] In the code below we verify the validity of these results by comparing the value of the three statistics based on the different formulas. # Sample size n = 100 # Simulate random numbers from a zero mean normal distribution with # variance equal to 4. x = rnorm(n, 0, sqrt(4)) # Simulate random numbers from normal distribution with mean 3 and # variance equal to 1. y = rnorm(n, 3, 1) # Note that x and y are independent. # Sample mean one = rep(1, n) (x_bar = 1/n*sum(x)) ## [1] 0.1353213 (x_bar_mat = 1/n*t(x)%*%one) ## [,1] ## [1,] 0.1353213 # Sample variance H = diag(rep(1, n)) - 1/n * one %*% t(one) (s_x = 1/n * sum((x - x_bar)^2)) ## [1] 4.567581 (s_x_mat = 1/n*t(x) %*% H %*% x) ## [,1] ## [1,] 4.567581 # Sample covariance y_bar = 1/n*sum(y) (s_xy = 1/n*sum((x - x_bar)*(y - y_bar))) ## [1] -0.1375739 (s_xy_mat = 1/n*t(x) %*% H %*% y) ## [,1] ## [1,] -0.1375739 2.2.4 Example: Least-squares If the matrix \\(\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\) exists, the least-squares estimator for \\(\\boldsymbol{\\beta}\\) is given by: \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y} \\tag{2.1} \\end{equation}\\] In the comment box below, we derive Eq. (2.1). If you aren’t familiar with such calculations we suggest you read some introduction to linear regression (see for example Seber and Lee (2012)). The least-square estimator \\(\\hat{\\boldsymbol{\\beta}}\\) is given by \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\operatorname{argmin}_{\\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) \\end{equation*}\\] The first step of this derivation is to reexpress the term \\(\\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right)\\) as follows: \\[\\begin{equation*} \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) = \\mathbf{y}^T\\mathbf{y} + \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} - 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y}. \\end{equation*}\\] In case you were suprizied by the term \\(2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y}\\) remeber that a scalar can always be transposed without changing its value and therefore we have that $ ^T ^T = ^T $. Now, our next step is to the following derivative \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right). \\end{equation*}\\] To do this we should keep in mind the following results \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\boldsymbol{\\beta}^T \\mathbf{X}^T \\boldsymbol{y} = \\boldsymbol{y}^T \\mathbf{X}, \\end{equation*}\\] and \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} = 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X}. \\end{equation*}\\] The proof of these two results can for example be found in Propositions 7 and 9 of Prof. Barnes’ notes. Using these two results we obtain \\[\\begin{equation*} \\frac{\\partial}{\\partial \\, \\boldsymbol{\\beta}} \\; \\left( \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) = 2 \\boldsymbol{\\beta}^T \\mathbf{X}^T \\mathbf{X} - 2 \\boldsymbol{y}^T \\mathbf{X}. \\end{equation*}\\] By solving for the first order condition (and under some technical assumptions not discussed here) we can redefine \\(\\hat{\\boldsymbol{\\beta}}\\) through the follwing equation \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}}^T \\mathbf{X}^T \\mathbf{X} = \\boldsymbol{y}^T \\mathbf{X}, \\end{equation*}\\] which is equivalent to \\[\\begin{equation*} \\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\boldsymbol{y}. \\end{equation*}\\] If \\(\\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}\\) exists, \\(\\hat{\\boldsymbol{\\beta}}\\) is therefore given by \\[\\begin{equation*} \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y}, \\end{equation*}\\] which verifies Eq. (2.1). The variance of \\(\\hat{\\boldsymbol{\\beta}}\\) is given by \\[\\begin{equation} \\text{Var} \\left(\\hat{\\boldsymbol{\\beta}} \\right) = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}, \\tag{2.2} \\end{equation}\\] the derivation of this result is explained in the comment box below. If we let \\(\\mathbf{A} = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T\\), then we have \\[\\begin{equation*} \\begin{aligned} \\text{Var} \\left(\\hat{\\boldsymbol{\\beta}} \\right) &amp;= \\text{Var} \\left( \\mathbf{A} \\mathbf{y} \\right) = \\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T \\\\ &amp; = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{X} \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} = \\sigma^2 \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1}, \\end{aligned} \\end{equation*}\\] which verifies Eq. (2.2). To understand the above derivation it may be useful to remind and point out a few things: \\(\\text{Var} \\left( \\mathbf{A} \\mathbf{y} \\right) = \\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T\\) since \\(\\mathbf{A}\\) is not a random variable. \\(\\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T\\) since\\(\\text{Var} \\left( \\mathbf{y} \\right) = \\sigma^2 \\mathbf{I}\\) and therefore we have \\(\\mathbf{A} \\text{Var} \\left( \\mathbf{y} \\right) \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{I} \\mathbf{A}^T = \\sigma^2 \\mathbf{A} \\mathbf{A}^T\\). The result \\(\\mathbf{A} \\mathbf{A}^T = (\\mathbf{X}^T \\mathbf{X})^{-1}\\) is based on the fact that \\((\\mathbf{X}^T \\mathbf{X})^{-1}\\) is symmetric but this is not necessarily intuitive. Indeed, this follows from the fact that any square and invertible matrix \\(\\mathbf{B}\\) is such that the inverse and transpose operator commute, meaning that \\(( \\mathbf{B}^T )^{-1} = ( \\mathbf{B}^{-1} )^T\\). Therefore since the matrix \\(\\mathbf{X}^T \\mathbf{X}\\) is square and (by assumption) invertible we have \\([(\\mathbf{X}^T \\mathbf{X})^{-1}]^T = [(\\mathbf{X}^T \\mathbf{X})^{T}]^{-1} = ( \\mathbf{X}^T \\mathbf{X})^{-1}\\). In general, the residual variance is unknown and needs to be estimated. A common and unbiased estimator of \\(\\sigma^2\\) is given by \\[\\begin{equation} \\hat{\\sigma}^2 = \\frac{1}{n - p} \\left( \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\right)^T \\left( \\mathbf{y} - \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\right) \\tag{2.3} \\end{equation}\\] DO YOU GUYS THINK WE SHOULD SHOW THE UNBIASEDNESS IN BOX HERE. THIS IS A LITTLE MORE ADVANCED AS WE NEED TO USE PROJECTION MATRICES AND THEIR PROPERTIES. LET ME KNOW. ROB COMMENT: Maybe I’d ask them to show the unbiasdness as homework either theoretically or giving numerical support to the claim (e.g. the mean of different beta hats is close to the true beta) Let’s implement Eq. (2.1) to (2.3) and, later, compare the outcome with the lm() function for linear regression implemented in base R. To do so, we will study an example taken from Wood (2017) which discusses a dataset collected from the Hubble Space Telescope key project containing information on the velocity and relative distance of 24 galaxies. This information has been used to compute the “Hubble constant’’ which is a fixed parameter that links velocity and distance of celestial bodies through which it is possible to compute the age of the universe based on the”big bang&quot; theory. The link is given by this simple linear relationship \\[\\begin{equation*} y = \\beta x , \\end{equation*}\\] where \\(y\\) represents the velocity while \\(x\\) is the distance between galaxies. Once the Hubble constant is known, its inverse (i.e. \\(\\beta^{-1}\\)) gives the age of the universe based on the big bang theory. Therefore let us use the abovementioned dataset to estimate Hubble’s constant to then get an estimate of the age of the universe. This dataset can be found in the gamair package under the name hubble and when plotting the two variables of interest in a scatterplot there appears to be a positive linear relationship between the two variables. # Load gamair library and retrieve Hubble dataset library(gamair) ## Warning: package &#39;gamair&#39; was built under R version 3.4.1 data(hubble) # Plot data plot(y = hubble$y, x = hubble$x, col=&quot;red&quot;, main=&quot;Distance vs. Velocity&quot;, ylab = &quot;Velocity (in km/s)&quot;, xlab = &quot;Distance (in Megaparsecs)&quot;) Putting aside possible problems with the data that would require a more in-depth knowledge and discussion of linear regression theory, we can proceed to estimate the Hubble constant by using the velocity as the response variable \\(y\\) and the distance as the explanatory variable \\(x\\). Let us therefore start by implementing Eq. (2.1) to get an estimate of \\(\\beta\\). # Define parameters and variables n = dim(hubble)[1] # number of observations p = 1 # number of explanatory variables df = n - p # degrees of freedom x = hubble$x # distance (explanatory variable) y = hubble$y # velocity (response variable) # Estimate of beta from Eq. (1.1) beta.hat = solve(t(x)%*%x)%*%t(x)%*%y We now have an estimated value for \\(\\beta\\) that we denote as \\(\\hat{\\beta}\\). Due to different measurement units we perform a unit transformation and then compute the age in years (which requires an additional transformation since the previous one gives a unit of \\(s^{-1}\\)). hubble.const = beta.hat/3.09e19 # Estimated Hubble&#39;s constant in inverse seconds age.sec = 1/hubble.const # Age of the universe in seconds age.sec/(60^2*24*365) # Age of the universe in years ## [,1] ## [1,] 12794692825 Based on this estimation, the age of the universe appears to be almost 13 billion years. However, we know that \\(\\hat{\\beta}\\) is a random variable that therefore follows a distribution which, based on asymptotic statistical theory, is a normal distribution with expectaton \\(\\beta\\) and variance \\(\\sigma^2(X^TX)^{-1}\\). Now, let’s suppose that we have a hypothesis on the age of the universe, for example that of Creation Scientists who claim that the universe is 6000 years old based on a reading from the Bible. Assuming the validity of the big bang theory, which is not the case for Creation Scientists, let us nevertheless test if their claim appears to be reasonable within this postulated framework. In order to do so we need to know the variance of \\(\\hat{\\beta}\\) and we consequently need to estimate \\(\\sigma^2\\) since we don’t know it. Let us therefore use Eq. (2.2) and (2.3) to compute it. # Estimate of the residual variance from Eq. (1.3) resid = y - x*beta.hat ## Warning in x * beta.hat: Recycling array of length 1 in vector-array arithmetic is deprecated. ## Use c() or as.vector() instead. sigma2.hat = (1/df)*t(resid)%*%resid # Estimate of the variance of the estimated beta from Eq. (1.2) var.beta = sigma2.hat*solve(t(x)%*%x) We now have all the information regarding the distribution of \\(\\hat{\\beta}\\) to test the Creation Scientists’ hypothesis. Indeed, for this purpose we can build a 95% confidence interval for the true Hubble constant \\(\\beta\\) and understand if the Hubble constant implied by the postulated age of the universe falls within this interval. Firstly, we can determine this constant under the null hypothesis which can be defined as follows \\[\\begin{equation*} H_0 \\, : \\, \\beta = 163 \\times 10^6 , \\end{equation*}\\] since this value of \\(\\beta\\) would imply that the universe is roughly 6000 years old. The alternative hypothesis is that the true \\(\\beta\\) is not equal to the above quantity (i.e. \\(H_A \\, : \\, \\beta \\neq 163 \\times 10^6\\)). Since we now have the necessary information, let us build a confidence interval for which we will assume that the estimated variance of \\(\\hat{\\beta}\\) is actually the true variance (otherwise we would need to build a confidence interval based on the Student-t distribution which we will not deal with at this stage of our analysis). Assuming therefore that \\[\\begin{equation*} \\hat{\\beta} \\sim \\mathcal{N}\\left(\\beta,\\hat{\\sigma}^2(X^TX)^{-1}\\right) , \\end{equation*}\\] we consequently have that the confidence interval is given by \\[\\begin{equation} \\left[\\hat{\\beta} - z_{1-\\alpha/2}\\sqrt{\\hat{\\sigma}^2(X^TX)^{-1}} \\, , \\, \\hat{\\beta} + z_{1-\\alpha/2}\\sqrt{\\hat{\\sigma}^2(X^TX)^{-1}} \\right] , \\tag{2.4} \\end{equation}\\] where \\(z_{1-\\alpha/2}\\) is the \\((1-\\alpha/2)^{th}\\) quantile of the standard normal distribution where, in our case, \\(\\alpha = 0.05\\) which delivers a 95% confidence interval. Let us therefore replace the values in Eq. (2.4): # 95% confidence interval for the Hubble constant ci.beta = c(beta.hat - qnorm(p = 0.975)*sqrt(var.beta), beta.hat + qnorm(p = 0.975)*sqrt(var.beta)) ci.beta ## [1] 68.81032 84.35203 The confidence interval lies between 68 and 84 which clearly does not contain the value implied by the age of the universe postulated by the Creation Scientists. Hence, assuming the validity of the big bang theory, we can reject this hypothesis at a 95% level. To conclude this analysis, let us compare the results of Eq. (2.1) to (2.3) with the outputs of the linear regression function lm() that can be found in base R. # Linear regression with lm() function fit_lm = lm(y~x-1) # Compare outputs equation_results = c(beta.hat, sigma2.hat) function_results = c(fit_lm$coefficients, (1/df)*t(fit_lm$residuals)%*%fit_lm$residuals) results = cbind(equation_results, function_results) row.names(results) = c(&quot;beta.hat&quot;, &quot;sigma2.hat&quot;) results ## equation_results function_results ## beta.hat 76.58117 76.58117 ## sigma2.hat 67046.33165 67046.33165 As one can notice, the two procedures give the same outputs. 2.3 Array 2.4 List 2.5 Dataframe 2.5.1 Example: Making Maps birth_place = c(&quot;Glasgow, Scotland&quot;, &quot;Manacor, Spain&quot;, &quot;Lausanne, Switzerland&quot;, &quot;Belgrade, Serbia&quot;, &quot;Basel, Switzerland&quot;) library(ggmap) ## Warning: package &#39;ggmap&#39; was built under R version 3.4.1 ## Warning: package &#39;ggplot2&#39; was built under R version 3.4.1 glasgow_coord = geocode(&quot;Glasgow, Scotland&quot;) glasgow_coord ## lon lat ## 1 -4.251806 55.86424 birth_coord = geocode(birth_place) birth_coord ## lon lat ## 1 -4.251806 55.86424 ## 2 3.209532 39.56972 ## 3 6.632273 46.51965 ## 4 20.448922 44.78657 ## 5 7.588576 47.55960 class(birth_coord) ## [1] &quot;data.frame&quot; birth_coord$Players = players birth_coord$GS = grand_slam_win birth_coord ## lon lat Players GS ## 1 -4.251806 55.86424 Andy Murray 9 ## 2 3.209532 39.56972 Rafael Nadal 15 ## 3 6.632273 46.51965 Stan Wawrinka 5 ## 4 20.448922 44.78657 Novak Djokovic 12 ## 5 7.588576 47.55960 Roger Federer 18 Let’s represent this information graphically. We haven’t seen how to make graph yet so don’t worry to much about the details of how this graph is made library(mapproj) map &lt;- get_map(location = &#39;Switzerland&#39;, zoom = 4) ggmap(map) + geom_point(data = birth_coord, aes(lon, lat, col = Players, size = GS)) + scale_size(name=&quot;Grand Slam Wins&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) 2.6 Data frames A data frame is the most common way of storing data in R, it has a 2D structure and shares properties of both the matrix and the list. We can create a data frame using data.frame() ### Creation players = c(&quot;Andy Murray&quot;, &quot;Rafael Nadal&quot;, &quot;Stan Wawrinka&quot;, &quot;Novak Djokovic&quot;, &quot;Roger Federer&quot;) grand_slam_win = c(9, 15, 5, 12, 18) date_of_birth = c(&quot;15 May 1987&quot;, &quot;3 June 1986&quot;, &quot;28 March 1985&quot;, &quot;22 May 1981&quot;, &quot;8 August 1981&quot;) country = c(&quot;Great Britain&quot;, &quot;Spain&quot;, &quot;Switzerland&quot;, &quot;Serbia&quot;, &quot;Switzerland&quot;) ATP_ranking = c(1, 2, 3, 4, 5) prize_money = c(60449649, 85920132, 30577981, 109447408, 104445185) tennis = data.frame(date_of_birth, grand_slam_win, country, ATP_ranking, prize_money) dimnames(tennis)[[1]] = players tennis ## date_of_birth grand_slam_win country ATP_ranking ## Andy Murray 15 May 1987 9 Great Britain 1 ## Rafael Nadal 3 June 1986 15 Spain 2 ## Stan Wawrinka 28 March 1985 5 Switzerland 3 ## Novak Djokovic 22 May 1981 12 Serbia 4 ## Roger Federer 8 August 1981 18 Switzerland 5 ## prize_money ## Andy Murray 60449649 ## Rafael Nadal 85920132 ## Stan Wawrinka 30577981 ## Novak Djokovic 109447408 ## Roger Federer 104445185 We can check if we have achived our gooal by using: is.data.frame(tennis) ## [1] TRUE 2.6.1 Combination Different data frames can also be combined. Let say we want to add some ifomrmation to our initial table e.g. the player’s height and if he is right-handed or letf-handed. We can do so by using cbind() and rbind(): height &lt;- c(1.90, 1.85, 1.83, 1.88, 1.85) hand &lt;- c(&quot;R&quot;,&quot;L&quot;,&quot;R&quot;,&quot;R&quot;,&quot;R&quot;) (tennis = cbind(tennis, data.frame(height, hand))) ## date_of_birth grand_slam_win country ATP_ranking ## Andy Murray 15 May 1987 9 Great Britain 1 ## Rafael Nadal 3 June 1986 15 Spain 2 ## Stan Wawrinka 28 March 1985 5 Switzerland 3 ## Novak Djokovic 22 May 1981 12 Serbia 4 ## Roger Federer 8 August 1981 18 Switzerland 5 ## prize_money height hand ## Andy Murray 60449649 1.90 R ## Rafael Nadal 85920132 1.85 L ## Stan Wawrinka 30577981 1.83 R ## Novak Djokovic 109447408 1.88 R ## Roger Federer 104445185 1.85 R 2.6.2 Subsetting Like for vectors, it is also possible to subset the values that we have stored in our data frames. Since data frames possess the characteristics of both lists and matrices: if you subset with a single vector, they behave like lists; if you subset with two vectors, they behave like matrices. # Let say we want only want to know the country and date of # birth of the players # There are two ways to select columns from a data frame # Like a list: tennis[c(&quot;country&quot;, &quot;date_of_birth&quot;)] ## country date_of_birth ## Andy Murray Great Britain 15 May 1987 ## Rafael Nadal Spain 3 June 1986 ## Stan Wawrinka Switzerland 28 March 1985 ## Novak Djokovic Serbia 22 May 1981 ## Roger Federer Switzerland 8 August 1981 # Like a matrix tennis[, c(&quot;country&quot;, &quot;date_of_birth&quot;)] ## country date_of_birth ## Andy Murray Great Britain 15 May 1987 ## Rafael Nadal Spain 3 June 1986 ## Stan Wawrinka Switzerland 28 March 1985 ## Novak Djokovic Serbia 22 May 1981 ## Roger Federer Switzerland 8 August 1981 # To acces a single element, let say the date of birth, # you can also use: tennis$date_of_birth ## [1] 15 May 1987 3 June 1986 28 March 1985 22 May 1981 8 August 1981 ## 5 Levels: 15 May 1987 22 May 1981 28 March 1985 ... 8 August 1981 2.6.3 Application: Non-parametric bootstrap Suppose we ask 10 students how much time they work at home for their calculus class, we obtain the following results (in hour) student_work &lt;- c(0, 0, 0, 0.25, 0.25, 0.75, 0.75, 1, 1.25, 4) We can compute the mean time spent mean(student_work) ## [1] 0.825 ADD SOMETHING ON T TEST t.test(student_work)$conf.int ## [1] -0.03495865 1.68495865 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 We can see that our confidence interval includes a negative values which clearly isn’t meaningful. Solution: (non-parametric) bootstrap which works as follows….. ADD SOMETHING Here is a simple function to implement this approach: # Number of boostrap replications B = 500 # Compute the length of vector n = length(student_work) # Confidence level alpha = 0.05 # Initialisation of boot_mean = rep(NA, B) for (i in 1:B){ student_work_star = student_work[sample(1:n, replace = TRUE)] boot_mean[i] = mean(student_work_star) } quantile(boot_mean, c(alpha/2, 1 - alpha/2)) ## 2.5% 97.5% ## 0.311875 1.613125 #hist(boot_mean, probability = TRUE) Basic Probability and Statistics "]
]
