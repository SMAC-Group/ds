# Data Structures {#data}

<<<<<<< HEAD
Briefly explain the types of data strucutres in R. Basically there are five
=======
There are different data types that are commonly used in R among which the most important ones are the following:
>>>>>>> origin/master

- **Numeric** (or double): are used to store real numbers. Examples: -4, 12.4532, 6.

- **Integer**: examples: 2L, 12L.

- **Logical** (or boolean): examples: `TRUE`, `FALSE`.

- **Character**: examples: `"a"`, `"Bonjour"`. 

In R there are basically five type of data structures that can be used to store elements in. A data structure is said to *homogeneous* if it only contains elements of the same type (for example it only contains character or numeric values) and *heterogenous* if it contains elements of more than one type. The five type of data structrures are commonly summarized in the table similar to the one below:

Dimension      Homogenous    Heterogeneous
------------- ------------- ----------------
1               Vector        List
2                Matrix       Dataframe
n               Array

To illustrate how to use these data structures, we will consider the simple data set of five best (as ranked by ATP on 07-15-2017) single men tennis players. The data are presented in the table below:

 Name              Date of Birth         Born                Country            ATP Ranking     Prize Money    Win Percentage   Grand Slam Wins
----------------  ---------------  ---------------------- -------------------  ---------------  -------------- ---------------- ------------------
Andy Murray        15 May 1987     Glasgow, Scotland       Great Britain            1             60,449,649         78.07               9
Rafael Nadal       3 June 1986     Manacor, Spain           Spain                   2            85,920,132         82.48               15
Stan Wawrinka      28 March 1985   Lausanne, Switzerland    Switzerland             3            30,577,981         63.96              5
Novak Djokovic    22 May 1987       Belgrade, Serbia          Serbia                4            109,447,408        82.77              12
Roger Federer      8 August 1981    Basel, Switzerland      Switzerland            5            104,445,185        81.80              18


## Vectors 

A vector has three important properties:

```{block2, note-text, type='rmdimportant'}
- **Type**, which corresponds the "kind" of objects in contains. It is possible to use the function `typeof()` to evaluate the type of objects in a vector.
- **Length**, i.e. the number of elements in a vector. This information can be obtained using the function `length()`.
- **Attributes**, some additional metadata attached to a vector. The functions `attr()` and `attributes()` can be used to store and retrive attributes (more details can be found in Section \@ref(vectattr))
```

For example, let us consider the number of grand slam won by the five players we are considering: 

```{r}
grand_slam_win = c(9, 15, 5, 12, 18)
```

To display the values stores in `grand_slam_win` we could simply do:

```{r}
grand_slam_win
```

Alteratively, we could have create and display the value by using `()` around the definition, for example:

```{r}
(grand_slam_win = c(9, 15, 5, 12, 18))
```

Various forms of "nest concatenation" can be used to defined vectors, for example we could also define `grand_slam_win` as

```{r}
(grand_slam_win = c(9, c(15, 5, c(12, c(18)))))
```

This approach is often used to assemnble vector in various ways.

It is also possible to define vector with characters, for example we could define a vector with the player names as follows:

```{r}
(players = c("Andy Murray", "Rafael Nadal", "Stan Wawrinka", 
             "Novak Djokovic", "Roger Federer"))
```

### Type 

We can evaluate the kind or type of elements that are stored in a vector using the function `typeof()`. For example, for the vectors we just created we obtain:

```{r}
typeof(grand_slam_win)
typeof(players)
```

This is a little suprzing as all the elements in `grand_slam_win` are integers it would seem natural to expect this as an output of the function `typeof()`. This is R considers by default any number as a "double", expet when spefiying `L` after an integer. For example,  

```{r}
typeof(1)
typeof(1L)
```

Therefore, we could express `exp_res` as follows:

```{r}
(grand_slam_win_int = c(1L, 2L, 3L, 5L, 1L))
typeof(grand_slam_win_int)
```

Naturally, the difference between the two in general relatively unimportant but we can see that `exp_res_int` takes less "space" to two. For example, we have

```{r}
object.size(grand_slam_win)
object.size(grand_slam_win_int)
```

### Coercion

As indicated earlier a vector is a homogenous data structures data, meaning that it can only contain a single type of data type. Therefore, when more than one data types are presented R *coerces* the data into a "shared" type. To identify this "shared" we can use this simple rule:

\begin{equation*}
 \text{logical} < \text{integer} < \text{numeric} < \text{character},
\end{equation*}

which simply means that if a vector contains more than data type it will be of the type of the "largest" according to the above equations. Here are few examples:

```{r}
# Logical + integer
(mix_logic_int = c(TRUE, 1L))
typeof(mix_logic_int)

# Logical + character
(mix_logic_char = c(TRUE, "Hi"))
typeof(mix_logic_char)

# Integer + numeric
(mix_int_num = c(1, 1L))
typeof(mix_int_num)

# Integer + character
(mix_int_char = c(1L, "Hi"))
typeof(mix_int_char)
```


### Subsetting

Naturally, it is possible to "subset" the values of in our vectror in many ways. Essentially, there are four main ways of subsetting a vector. Here we'll only discuss the first three:

- **Positive Index**: We can *access* or *subset* the $i$-th element of a vector by simply using `exp_res[i]` where $i$ is a positive number between 1 and length of the vector.

```{r}
# Accesing the first element
grand_slam_win[1]

# Accesing the third and first value
grand_slam_win[c(3, 1)]

# Duplicated indices yield duplicated values
grand_slam_win[c(1, 1, 2, 2, 3, 4)]
```

- **Negative Index**: We *remove* elements in a vector using negative indices:

```{r}
# Removing the second obervation
grand_slam_win[-2]

# Removing the first and fourth obserations
grand_slam_win[c(-1, -4)]
```

- **Logical Indices**: Another usefull approach is based on *logical* operators:

```{r}
# Access the first and fourth observations
grand_slam_win[c(TRUE, FALSE, FALSE, TRUE, FALSE)]
```

```{block2, type='rmdnote'}
Here we could add some remarks on weird cases, for example `exp_res[c(1.2, 3.4)]` (which rounds things up) or `exp_res[c(-1, 2)]` (which doesn't work as "mixed" indices are not permitted).
```

### Attributes {#vectattr}

Our experiment was conducted under specific conditions, say a date and a place which should be store are attributes. Indeed, objects can have arbitrary additional attributes, used to store metadata about the considered object. For example:

```{r}
attr(grand_slam_win, "date") = "07-15-2017"
attr(grand_slam_win, "type") = "Men, Single"
```

To display the vector with its attributes

```{r}
grand_slam_win
```

To only display the attributes
```{r}
attributes(grand_slam_win)
```

It is also possible to extract a specific attribute

```{r}
attr(grand_slam_win, "date")
```

### Adding labels {#addlab}

In some cases, it might be useful to add label to vectors. For example, we could defined the vector `grand_slam_win` and use as labels the player's names, i.e.

```{r}
(grand_slam_win = c("Andy Murray" = 9, "Rafael Nadal" = 15, 
                   "Stan Wawrinka" = 5, "Novak Djokovic" = 12, 
                   "Roger Federer" = 18))
```

The main advantage of this approach is that the number of grand slam won can now be referred to by the player's name. For example:

```{r}
grand_slam_win["Andy Murray"]
grand_slam_win[c("Andy Murray","Roger Federer")]
```

All labels (players' names in our case) can be obtained witht the function `names`, i.e.

```{r}
names(grand_slam_win)
```

### Useful functions with vectors

Add some text here

`length()` `sum()` `mean()` `sort()` and `order()`

For example

```{r}
length(grand_slam_win)
sum(grand_slam_win)
mean(grand_slam_win)
```

To sort the player by number of grand slam we could use the function `order()` which returns the *position* of the elements of a vector sorted in an ascending manner,

```{r}
order(grand_slam_win)
```

Therefore, we can sort the players as follow

```{r}
players[order(grand_slam_win)]
```

showing the Roger Federer won the most grand slam. Another related function is `sort()` which simply sorts the elements of a vector (in an ascending manner). For example,

```{r}
sort(grand_slam_win)
```

which is compact version of

```{r}
grand_slam_win[order(grand_slam_win)]
```

There are of course many other usefull to dealing with vectors.

### Creation sequences

When uing R for statstical programming or even data analysis it is very common to create sequences of numbers. Here are three common ways for generatign such sequences:

- `from:to`: This method is quite inituitive and very compact. For example:

```{r}
(x = 1:3)
(y = 3:1)
(w = -1:-4)
(z = 1.3:3)
```

- `seq_len(n)`: This function provides a simple way to generate a sequence from 1 to an arbitrary number `n`. In general, `1:n` and `seq_len(n)` are equivalent with the notable exeptions of the cases `n = 0` and `n < 0`. The reason for these exeption will become clear in Section \@ref(forloop). Let's see a few examples:

```{r}
n = 3
1:n
seq_len(n)

n = 0
1:n
seq_len(n)
```


- `seq(a, b, by/length.out = d)`: This function can be used to create more "complexe" sequences. It either be used to create a sequence from `a` to `b` by increments of `d` (using the option `by`) or of a total length of `d` (using the option `length.out`). A few examples:

```{r}
(x = seq(1, 2.8, by = 0.4))
(y = seq(1, 2.8, length.out = 6))
```


Maybe it would be interesting to add something `rep()` for example:

```{r}
rep(c(1,2), times = 3, each = 1)
rep(c(1,2), times = 1, each = 3)
```

### Example: Apple Stock Price 

How to get today's date

```{r}
(today = Sys.Date())
```

Three monmths ago

```{r}
(three_months_ago = seq(today, length = 2, by = "-3 months")[2])
```

Let's download Apple stock price

```{r, message = FALSE, fig.height = 5, fig.width = 6, fig.align = "center", warning = FALSE}
library(quantmod)
getSymbols("AAPL", from = three_months_ago, to = today)
candleChart(AAPL, theme='white', type='candles')
```

Let's compute some returns...

\begin{equation}
  r_t = \frac{S_t - S_{t-1}}{S_{t-1}}
\end{equation}

where $r_t$ are the return, $S_t$ the stock price. This is implemented in the function `ClCl()` of the package `quantmod`. For example, we can create a vector of returns as follows 


```{r}
AAPL_returns = as.numeric(na.omit(ClCl(AAPL)))
```

`na.omit` to remove missing value as if we have $n+1$ stock prices we only $n$ returns
`as.numeric` to transform into a vector. We can now compute mean and median return over the period

```{r}
mean(AAPL_returns)
median(AAPL_returns)
```

Excess Kurtosis can be defined for a random variable $X$ as

\begin{equation}
  \text{Kurt} = \frac{{E}\left[\left(X - E[X]\right)^4\right]}{\left({E}\left[\left(X - E[X]\right)^2\right]\right)^2} - 3
\end{equation}

The reason excess is .... A common estimator of the excess Kurtosis is

\begin{equation}
  k = \frac{\frac{1}{n} \sum_{t = 1}^{n} \left(r_t -\bar{r}\right)^4}{\left(\frac{1}{n} \sum_{t = 1}^{n} \left(r_t -\bar{r}\right)^2 \right)^2} - 3
\end{equation}

where $\bar{k}$ denotes the sample average of the returns, i.e.

\begin{equation}
  \bar{k} = \frac{1}{n} \sum_{i = 1}^n r_i
\end{equation}

```{r}
mu = mean(AAPL_returns)
(k = mean((AAPL_returns - mu)^4)/(mean((AAPL_returns - mu)^2))^2 - 3)
```
 
which is quite high tends to indicate the returns have a heavier tails than the normal distribution.
 
## Matrix

Example of matrix creation

```{r}
(mat = matrix(1:12, ncol = 4,  nrow = 3))
```

explain `cbind` and `rbind`

```{r}
players = c("Andy Murray", "Rafael Nadal", "Stan Wawrinka", 
             "Novak Djokovic", "Roger Federer")
grand_slam_win = c(9, 15, 5, 12, 18)
win_percentage = c(78.07, 82.48, 63.96, 82.77, 81.80)
(mat = cbind(grand_slam_win, win_percentage))
```

explain `rownames` `colnames`

```{r}
rownames(mat) <- players
colnames(mat) <- c("GS win", "Win rate")
mat
```

### Subsetting

```{r}
mat[c("Stan Wawrinka", "Roger Federer"), ]
mat[c(1, 3), ]
mat[, 2]
mat[1:3, 1]
```

### Useful fun + matrix algebra

The function `dim()` allows to determine the dimension of a matrix. For example, consider the following $4 \times 2$ matrix

\begin{equation*}
\mathbf{A} = \left[
\begin{matrix}
1 & 5\\
2 & 6\\
3 & 7\\
4 & 8
\end{matrix}
\right]
\end{equation*}

which can be defined as:
```{r}
(A = matrix(1:8, 4, 2))
```

Therefore, we expect `dim(A)` to retrun the vector `c(4, 2)`. Indeed, we have

```{r}
dim(A)
```

Next, we consider the function `t()` allows transpose a matrix. For example, $\mathbf{A}^T$ is equal to:

\begin{equation*}
\mathbf{A}^T = \left[
\begin{matrix}
1 & 2 & 3 & 4\\
5 & 6 & 7 & 8
\end{matrix}
\right],
\end{equation*}

which is a $2 \times 4$ matrix. In R, we obtain

```{r}
(At <- t(A))
dim(At)
```

The operator `%*%` is used in R to denote matrix multiplicate. Note that the regular product opertor `*` used with matrices performs the Hadamar product (or element by element product). For example,

```{r}

```
`%*%` `det()` `solve()` 

### Example: variance as matrix

```{r}
n = 100
x = rnorm(n, 0, sqrt(4))
(sig2 = 1/n*sum((x - mean(x))^2))
(sig2_mat = as.numeric(1/n*t(x)%*%x - (1/n*t(rep(1,n))%*%x)^2))
```

It is also interesting to compare `sig2` and `sig2_mat`

```{r}
sig2 - sig2_mat
```

numerical error


### Example: Least-squares

If the matrix $\left(\mathbf{X}^T \mathbf{X}\right)^{-1}$, the least-squares estimator for $\boldsymbol{\beta}$ is given by:

\begin{equation}
  \hat{\boldsymbol{\beta}} = \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{y}
(\#eq:lsformula)
\end{equation}

In the comment box below, we derive Eq. \@ref(eq:lsformula). If you aren't familiar with such calculation it might to read or something like this


```{block2, type='rmdtip'}
The least-square estimator $\hat{\boldsymbol{\beta}}$ can be defined as

\begin{equation*}
 \hat{\boldsymbol{\beta}} = \operatorname{argmin}_{\boldsymbol{\beta}} \; \left( \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \right)^T \left( \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right) 
\end{equation*}

The first step of this derivation is to rexpress the term  $\left( \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \right)^T \left( \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right)$ as follows:

\begin{equation*}
    \left( \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \right)^T \left( \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right) =  \mathbf{y}^T\mathbf{y} +  \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} - 2 \boldsymbol{\beta}^T \mathbf{X}^T \boldsymbol{y}.
\end{equation*}

In case you were suprizied by the term $2 \boldsymbol{\beta}^T \mathbf{X}^T \boldsymbol{y}$ remeber that a scalar can always be transpose with changing its value and therefore we have that $ \boldsymbol{\beta}^T \mathbf{X}^T \boldsymbol{y} =  \boldsymbol{y}^T \mathbf{X}  \boldsymbol{\beta}$. Now, out next step is the compute 

\begin{equation*}
  \frac{\partial}{\partial \, \boldsymbol{\beta}} \; \left( \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \right)^T \left( \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right).
\end{equation*}

To do this we should remeber the following results

\begin{equation*}
  \frac{\partial}{\partial \, \boldsymbol{\beta}} \; \boldsymbol{\beta}^T \mathbf{X}^T \boldsymbol{y} =   
   \boldsymbol{y}^T \mathbf{X},
\end{equation*}

and

\begin{equation*}
  \frac{\partial}{\partial \, \boldsymbol{\beta}} \; \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} \boldsymbol{\beta} =  2 \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X}.
\end{equation*}

The proof of these two results can for example be found in Propositions 7 and 9 of [Prof. Barnes' notes](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf). Using these two results we obtain

\begin{equation*}
  \frac{\partial}{\partial \, \boldsymbol{\beta}} \; \left( \mathbf{y} - \mathbf{X}\boldsymbol{\beta} \right)^T \left( \mathbf{y} - \mathbf{X} \boldsymbol{\beta} \right) = 2 \boldsymbol{\beta}^T \mathbf{X}^T \mathbf{X} - 2 \boldsymbol{y}^T \mathbf{X}.
\end{equation*}

By solving for the first order condition (and under some technical assumptions not discussed here) we can redefine $\hat{\boldsymbol{\beta}}$ through the follwing equation

\begin{equation*}
  \hat{\boldsymbol{\beta}}^T \mathbf{X}^T \mathbf{X} =  \boldsymbol{y}^T \mathbf{X},
\end{equation*}

which is equivalent to

\begin{equation*}
  \mathbf{X}^T \mathbf{X} \hat{\boldsymbol{\beta}} =   \mathbf{X}^T \boldsymbol{y}.
\end{equation*}

If $\left(\mathbf{X}^T \mathbf{X}\right)^{-1}$ exist, $\hat{\boldsymbol{\beta}}$ is therefore given by

\begin{equation*}
  \hat{\boldsymbol{\beta}} = \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T \mathbf{y},
\end{equation*}

which verifies Eq. \@ref(eq:lsformula).
```


The variance of $\hat{\boldsymbol{\beta}}$ is given by

\begin{equation}
\text{Var} \left(\hat{\boldsymbol{\beta}} \right) = \sigma^2 \left(\mathbf{X}^T \mathbf{X}\right)^{-1},
(\#eq:lsvar)
\end{equation}

the derivation of this results is explain in the comment box below.

```{block2, type='rmdtip'}
We let $\mathbf{A} = \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T$. Then, we have

\begin{equation*}
\begin{aligned}
  \text{Var} \left(\hat{\boldsymbol{\beta}} \right) &= \text{Var} \left( \mathbf{A} \mathbf{y} \right) = \mathbf{A} \text{Var} \left(  \mathbf{y} \right) \mathbf{A}^T = \sigma^2 \mathbf{A} \mathbf{A}^T \\
  & = \sigma^2 \left(\mathbf{X}^T \mathbf{X}\right)^{-1} \mathbf{X}^T  \mathbf{X} \left(\mathbf{X}^T \mathbf{X}\right)^{-1} = \sigma^2 \left(\mathbf{X}^T \mathbf{X}\right)^{-1},
\end{aligned}
\end{equation*}

which verifies Eq. \@ref(eq:lsvar). To understand the above derivation we might be usefull to remind and point out a few things:

- $\text{Var} \left( \mathbf{A} \mathbf{y} \right) = \mathbf{A} \text{Var} \left(  \mathbf{y} \right) \mathbf{A}^T$ since $\mathbf{A}$ is not a random variable.
- $\mathbf{A} \text{Var} \left(  \mathbf{y} \right) \mathbf{A}^T = \sigma^2 \mathbf{A} \mathbf{A}^T$ since$\text{Var} \left(  \mathbf{y} \right) = \sigma^2 \mathbf{I}$ and therefore we have $\mathbf{A} \text{Var} \left(  \mathbf{y} \right) \mathbf{A}^T = \sigma^2 \mathbf{A} \mathbf{I} \mathbf{A}^T = \sigma^2 \mathbf{A} \mathbf{A}^T$.
- The result $\mathbf{A} \mathbf{A}^T = (\mathbf{X}^T \mathbf{X})^{-1}$ is based on the fact that $(\mathbf{X}^T \mathbf{X})^{-1}$ is symmetric but this is not necessarily intuitive. Indeed, this follows from the fact that any square and invertible matrix $\mathbf{B}$ is such that the inverse and transpose operator commute, meaning that $( \mathbf{B}^T )^{-1} = ( \mathbf{B}^{-1} )^T$.
Therefore since the matrix $\mathbf{X}^T \mathbf{X}$ is square and (by assumption) invertible we have 
$[(\mathbf{X}^T \mathbf{X})^{-1}]^T = [(\mathbf{X}^T \mathbf{X})^{T}]^{-1} = ( \mathbf{X}^T \mathbf{X})^{-1}$.
```

In general, the residual variance is unknown and needs to estimate. A common and unbiased estimator of $\sigma^2$ is given by

\begin{equation}
  \hat{\sigma}^2 = \frac{1}{n - p}  \left( \mathbf{y} - \mathbf{X}\hat{\boldsymbol{\beta}} \right)^T \left( \mathbf{y} - \mathbf{X} \hat{\boldsymbol{\beta}}\right) 
  (\#eq:lssig2hat)
\end{equation}

DO YOU GUYS THINK WE SHOULD SHOW THE UNBIASEDNESS IN BOX HERE. THIS IS A LITTLE MORE ADVANCED AS WE NEED TO USE PROJECTION MATRICES AND THEIR PROPERTIES. LET ME KNOW.

Let's implement Eq. \@ref(eq:lsformula) to \@ref(eq:lssig2hat) and compare with the `lm()` implemented in base R. Before doing maybe we could use the dataset `hubble`. I think it is quite cool as it can be used to estimate the "age" of the universe and test wether the estimate of the age of the universe by Creation Scientists (based on a reading of the Bible) is reasonable. This is an example based on Simon Woods book. If we do this example, we should add something on confidence interval and I think he could the normal distribution (instead of t-distribution) to avoid going into details. Anyway the t-test relies on Normal assumption which is hard to verify. In the following chapter, we could use this example to show the various form of boostrap (parametric, non-parametric, semi-parametric). Let me know what you think.


## Array

## List

## Dataframe
 
 
### Example: Making Maps


```{r}
birth_place = c("Glasgow, Scotland", "Manacor, Spain", "Lausanne, Switzerland",
                "Belgrade, Serbia", "Basel, Switzerland")
```

```{r, message = FALSE}
library(ggmap)
glasgow_coord = geocode("Glasgow, Scotland")
```

```{r}
glasgow_coord
```

```{r, message = FALSE, cache = TRUE}
birth_coord = geocode(birth_place)
```

```{r}
birth_coord
```

```{r}
class(birth_coord)
```

```{r}
birth_coord$Players = players
birth_coord$GS = grand_slam_win
```

```{r}
birth_coord
```

Let's represent this information graphically. We haven't seen how to make graph yet so don't worry to much about the details of how this graph is made

```{r, message = FALSE, fig.align = "center", fig.height = 5, fig.width = 7.5, cache = TRUE}
library(mapproj)
map <- get_map(location = 'Switzerland', zoom = 4)
ggmap(map) + geom_point(data = birth_coord, 
             aes(lon, lat, col = Players, size = GS)) + 
             scale_size(name="Grand Slam Wins") + 
             xlab("Longitude") + ylab("Latitude")
```


## Data frames

A data frame is the most common way of storing data in R, it has a 2D structure and shares properties of both the matrix and the list. 

We can create a data frame using data.frame() 

```{r}
### Creation

players = c("Andy Murray", "Rafael Nadal", "Stan Wawrinka", 
             "Novak Djokovic", "Roger Federer")

grand_slam_win = c(9, 15, 5, 12, 18)

date_of_birth = c("15 May 1987", "3 June 1986", "28 March 1985", 
                  "22 May 1981", "8 August 1981")

country = c("Great Britain", "Spain", "Switzerland", 
            "Serbia", "Switzerland")
ATP_ranking = c(1, 2, 3, 4, 5)

prize_money = c(60449649, 85920132, 30577981, 109447408, 104445185)

tennis = data.frame(date_of_birth, grand_slam_win, country, 
                    ATP_ranking, prize_money)

dimnames(tennis)[[1]] = players
tennis

```

We can check if we have achived our gooal by using:

```{r}
is.data.frame(tennis)
```
### Combination

Different data frames can also be combined. Let say we want to add some ifomrmation to our initial table e.g. the player's height and if he is right-handed or letf-handed. 

We can do so by using `cbind()` and `rbind()`:

```{r}
height <- c(1.90, 1.85, 1.83, 1.88, 1.85)
hand <- c("R","L","R","R","R")

(tennis = cbind(tennis, data.frame(height, hand)))
```

### Subsetting

Like for vectors, it is also possible to subset the values that we have stored in our data frames. Since data frames possess the characteristics of both lists and matrices: if you subset with a single vector, they behave like lists; if you subset with two vectors, they behave like matrices.

```{r}
# Let say we want only want to know the country and date of 
# birth of the players

# There are two ways to select columns from a data frame
# Like a list:
tennis[c("country", "date_of_birth")]

# Like a matrix
tennis[, c("country", "date_of_birth")]

# To acces a single element, let say the date of birth, 
# you can also use:
tennis$date_of_birth
```

### Application: Non-parametric bootstrap

Suppose we ask 10 students how much time they work at home for their calculus class, we obtain the following results (in hour)

```{r}
student_work <- c(0, 0, 0, 0.25, 0.25, 0.75, 0.75, 1, 1.25, 4)
```

We can compute the mean time spent

```{r}
mean(student_work)
```

*ADD SOMETHING ON T TEST*

```{r}
t.test(student_work)$conf.int
```

We can see that our confidence interval includes a negative values which clearly isn't meaningful. Solution: (non-parametric) bootstrap which works as follows..... ADD SOMETHING

Here is a simple function to implement this approach:

```{r}
# Number of boostrap replications
B = 500

# Compute the length of vector
n = length(student_work)

# Confidence level
alpha = 0.05

# Initialisation of 
boot_mean = rep(NA, B)

for (i in 1:B){
  student_work_star = student_work[sample(1:n, replace = TRUE)]
  boot_mean[i] = mean(student_work_star)
}

quantile(boot_mean, c(alpha/2, 1 - alpha/2))
#hist(boot_mean, probability = TRUE)

```
