# Case Study: Monte-Carlo Integration

```{r, echo=FALSE}
library(support)
```

Monte Carlo integration is a powerful technique for numerical integration. It is particularly useful to evaluate integrals of "high-dimension". A detailed (and formal) discussion of this method is clearly beyond the scope of this class and we shall restrict our attention to most basic form(s) of Monte Carlo Integration and briefly discuss the rational behind this method.

Originally, such Monte Carlo methods were known under various names among which *statistical sampling* was probably the most commonly used. In fact, the name *Monte Carlo* was popularized by several physics researchers, including Stanislaw Ulam, Enrico Fermi and John von Neumann. The name is believed to be a reference to a famous casino in Monaco where Stanislaw Ulamâ€™s uncle would borrow money to gamble. Enrico Fermi was one of the first to this technique which he employed to study the properties newly-discovered neutron in the 1930s. Later, these methods played for example a central role in many of the simulations required for the Manhattan project.

Suppose we are interested in computing the following integral: 

\[I = \int_a^b f(x) dx.\]

Of course, this integral can be approximated by a Riemann sum,

\[I \approx \Delta x \sum_{i = 1}^n f(a + (i-1) \Delta x),\]

where $\Delta x = \frac{b - a}{n}\;$ and the idea behind this approximation is that as the number of partions $n$ increases the Riemann sum will become closer and closer to $I$. Also (and under some technical conditions), we have that

\[I = \lim_{n \to \infty} \Delta x \sum_{i = 1}^n f(a + (i-1) \Delta x).\]

In fact, the rational of a Monte Carlo Integral is quite close to the Riemann sum since, in its most basic form, we approximate $I$ by averaging samples of the function $f(x)$ at uniform random point within the interval $[a, b]$. Therefore, the Monte Carlo *estimator* of $I$ is given by

\begin{equation} 
\hat{I} = \frac{b - a}{B} \sum_{i = 1}^B f(X_i), 
 (\#eq:binom)
\end{equation}

where $X_i = a + U_i (b - a)$ and $U_i \sim \mathcal{U}(0,1)$. In fact, \@ref(eq:binom) is quite intuitive as $\frac{1}{B} \sum_{i = 1}^B f(X_i)$ represents an estimation of the average value of $f(x)$ in the interval $[a, b]$ and thus $\hat{I}$ is simply the average value time the length of the interval, i.e. $(b-a)$.

A more formal argument on the validity of this approach can be found in analyzing the statistical properties of the estimator $\hat{I}$. In order to do so, we start by considering its expected value
\[
\mathbb{E}\left[ \hat{I} \right] = \frac{b - a}{B} \sum_{i = 1}^B \mathbb{E}\left[ f(X_i)  \right]
= \frac{b - a}{B} \sum_{i = 1}^B \int f(x) g(x) dx, 
\]

where $g(x)$ denotes the pdf of $X_i$. Since $X_i \sim \mathcal{U}(a, b)$ it follows that

\[
g(x) = \left\{
	\begin{array}{ll}
		\frac{1}{b - a}  & \mbox{if } x \in [a, b] \\
		0 & \mbox{if } x \not\in [a, b]
	\end{array}
\right.
\]

Therefore, we have

\[
\mathbb{E}\left[ \hat{I} \right] = \frac{b - a}{B} \sum_{i = 1}^B \int_a^b \frac{f(x)}{b-a} dx = \int_a^b f(x) dx = I, 
\]

Since $X_i$ are iid, the same can be said about $f(X_i)$ and therefore by the **Strong Law of Large Numbers** we have that $\hat{I}$ converge *almost surely* to $I$, which means that

\[
\mathbb{P}\left(\lim_{B \to \infty} \hat{I} = I    \right) = 1.
\]

This result implies that as the number of simulations $B$ goes to infinity we can guarantee that the solution will be exact. If you are unfamiliar or don't remeber well the Strong Law of Large Numbers we provide a reminder in the comment box below.

```{block2, type='rmdtip'}
IS THAT USEFUL?
```

Unfortunately, this result does give us any information on how quickly this estimate converges to "sufficiently accurate" solution for the problem at hand. This can be done by studying the variance of $\hat{I}$ and its *rate of convergence*. Indeed, we have

\[
\begin{aligned}
\operatorname{var} \left( \hat{I} \right) &= \left(\frac{b - a}{B}\right)^2  \sum_{i = 1}^B \left\{\mathbb{E}\left[f^2(X_i)\right] - \mathbb{E}^2\left[f(X_i)\right]\right\}\\
&= \frac{1}{B^2} \sum_{i = 1}^B \left\{(b-a) \int_a^b f^2(x) dx - \left(\int_a^b f(x) dx \right)^2 \right\}\\
&= \frac{(b-a) I_2 - I^2}{B}
\end{aligned}
\]

where $I_2 = \int_a^b f^2(x) dx$. A simple estimator of this quantity is given by

\[
\hat{I}_2 = \frac{b - a}{B} \sum_{i = 1}^B f^2(X_i),
\]

and therefore using $\hat{I}$ we obtain:

\[
\widehat{\operatorname{var}} \left(\hat{I} \right) = \frac{(b-a) \hat{I}_2 - \hat{I}^2}{B} = \frac{b - a}{B^2} \sum_{i = 1}^B\left[ (b - a )f^2(X_i) - f(X_i)\right]
\]

Thus, it is easy to see that the rate of convergence of $\widehat{\operatorname{var}} \left(\hat{I} \right)$ is $B^{-1}$ and we may write ${\operatorname{var}} \left(\hat{I} \right) = \mathcal{O}(B)$. This implies that if we wish to reduce the error (or standard deviation) by half we need to quadruple $B$. Such phenomon is very common in many research such as Statistics is often called the *curse of dimensionality*.

The function `mc_int()`, which is presented below implements the above procedure and is also available in the `?????` package. 

The thing that may be a little difficul to understand with the approach is how to pass the function we wish to integrate is passed on as an input `mc_int`. Indeed, we are using a string for this purpose so for example if we have to integrate the function $\sin(x)$ we will simply write `fun = sin(x)` when calling `mc_int`. This implies that we should be able to "transform" a string into a function we can evaluate, which is something that we can achieve by combining the functions `eval` and `parse`. An example is provided below:

```{r exampleeval}
my_fun = "x^2"
x = 0:3
eval(parse(text = my_fun))
```

If you having trouble to understand what these functions are doing have a look to their help files with write `?eval` and `?parse`.

Using this function we can consider the following example. Suppose that we interested in computing the integral of the function $f(x) = x^2$ in the domain $(0,\, 3)$. Tthis integral is simple enough and we actually know the true value of $I$ in this case, indeed we have

\[
I = \int_0^3 x^2 dx = \frac{1}{3} \left[x^3\right]_0^3 = 9.
\]


```{r}
mc_int(x_range = c(0,3), fun = "x^2", B = 10^5)
```


```{r, fig.height = 2.5, fig.width = 10, eval = FALSE}
n_step = 10^3
x = seq(from = -0.35, to = 3.35, length.out = n_step)
y = x^2
set.seed(6)
Ui = runif(4, 0 ,3)

par(mfrow = c(1, 4))



for (i in 1:4){
  plot(NA, xlim = range(x), ylim = range(y), xlab = "x", ylab = "f(x)")
  grid()
  trans_col = hcl(h = seq(15, 375, length = 5), l = 65, c = 100, alpha = 0.15)[1:5]
  lines(x, y, col = "blue4")
  points(Ui[i], Ui[i]^2, pch = 16, col = "yellow2")
  polygon(c(0, Ui[i], Ui[i], 0), c(0, 0, Ui[i]^2, Ui[i]^2), 
          border = NA, col = trans_col[i])
}
```


Example

\[
I = \int_0^3 x^2 dx = \frac{1}{3} \left[x^3\right]_0^3 = 9
\]


```{r, eval = FALSE}
B = 4^(4:12)
results = matrix(NA, length(B), 3)
for (i in 1:length(B)){
  mc_res = monte_carlo_integral_uniform(c(0, 3), "x^2", B = B[i], seed = i+1)
  results[i, ] = mc_res$I + 2*c(0, sqrt(mc_res$var), -sqrt(mc_res$var))
}

trans_blue = hcl(h = seq(15, 375, length = 3), l = 65, c = 100, alpha = 0.15)[2]
plot(NA, xlim = range(B), ylim = range(results[, 2:3]), log = "x", 
     ylab = "Estimated Integral", xlab = "Number of Simulations B",
     xaxt = 'n')
grid()
axis(1, at = B, labels = parse(text = paste("4^", 4:12, sep = "")))
polygon(c(B, rev(B)), c(results[, 2], rev(results[, 3])), border = NA, col = trans_blue)
lines(B, results[, 1], type = "b", col = "blue4", pch = 16)
abline(h = 9, col = "red4", lty = 2)
legend("topright", c(expression(I[1]), "Confidence Interval", "True Value"), bty = "n",
       pch = c(16, 15, NA), lwd = c(1, NA, 1), lty = c(1, NA, 2), 
       pt.cex = c(1, 2, NA), col = c("blue4", trans_blue, "red4"))
```

\[
\widehat{\operatorname{var}} \left(\hat{I} \right) = \mathcal{O}(B^{-1}).
\]


The Monte-Carlo approach generalized in the following way. We suppose again that we wish to solve  

\[\int_a^b f(x) dx,\]

and that $X_i \sim G$ where the pdf of $X_i$ is such that

\[
\max_{\mathbb{R} \setminus [a, b]} \; g(x) = 0 .
\]


\[
\tilde{I} = \frac{1}{B}\sum_{i = 1}^B \frac{f(X_i)}{g(X_i)}
\]

Of course, in the case $X_i \mathcal{U}(a, b)$, then $\hat{I}$ and $\hat{I}$ are equivalent.

\[
\mathbb{E} \left[ \tilde{I} \right] = 
\]











To understand the very basic idea of Monte Carlo integration we shall start with a simple example. Suppose we wish to compute:

\[
I_1 = \int_{-1}^2 3x^2 dx.
\]

It is easy to verify that $\max_{\Omega} \; f(x) = 12$ and $\min_{\Omega} \; f(x) = 0$ where $\Omega = (-1, 2)$ and therefore we have that

\[I_1 \leq I_2 = \int_{-1}^2 \int_{0}^{12} dy dx = 36.\]

The following code and image illustrates the situation:

```{r, eval = FALSE}
n_step = 10^3
x = seq(from = -1.35, to = 2.35, length.out = n_step)
x_in_omega = x[x >= -1 & x <= 2]
y = 3*x^2
col_I1 = hcl(h = seq(15, 375, length = 3), l = 65, c = 100, alpha = 0.2)[2]
plot(NA, xlim = range(x), ylim = range(y), xlab = "x", ylab = "f(x)")
grid()
polygon(c(-1, 2, 2, -1), c(0, 0, 12, 12), border = "red3", lty = 2, lwd = 2)
polygon(c(x_in_omega, rev(x_in_omega)), c(rep(0, length(x_in_omega)), 
        rev(y[x >= -1 & x <= 2])), border = "NA", col = col_I1)
lines(x, y, col = "blue4")
legend("topleft", c(expression(I[1]), expression(I[2]), "f(x)"), bty = "n",
       pch = c(NA, 15, NA), lwd = c(2, NA, 1), lty = c(2, NA, 1), 
       pt.cex = c(2, 2, NA), col = c("red3", col_I1, "blue4"))
```

Suppose that we generate (uniformly) a large number points, with coordinates ($U_1$, $U_2$) inside the red region. We assume that $U_1 \sim \mathcal{U}(-1, 2)$ and $U_2 \sim \mathcal{U}(0, 12)$. The function below also t

```{r, eval = FALSE}
#' Generate random points with bivariate uniform distribution
#' 
#' @param x_range A vector of dimension 2 used to denote the parameter 
#' of the uniform distribution on the x-axis.
#' @param y_range A vector of dimension 2 used to denote the parameter 
#' of the uniform distribution on the y-axis.
#' @param B A positive number used to denote the number of simulation.
#' @param seed A number used to control the seed of the random number
#' generated by this function.
#' @return A B x 2 matrix containing the simulated points.
#' @examples
#' gen_points(x_range = c(0, 1), y_range = c(0, 2), B = 10)
#' gen_points(x_range = c(-10, 1), y_range = c(0, 0.1), B = 5)
gen_points = function(x_range, y_range, B, seed = 1291){
  # A few checks
  # Check x_range
  if (length(x_range) != 2 || x_range[1] >= x_range[2]){
    error("x_range is uncorrely specified")
  }
  
  # Check y_range
  if (length(y_range) != 2 || y_range[1] >= y_range[2]){
    error("y_range is uncorrely specified")
  }
  
  # Check B
  if (B < 1){
    error("B is uncorrely specified")
  }
  
  # Set seed
  set.seed(seed)
  
  # Let's draw some uniforms
  mat_out = matrix(runif(2*B), B, 2)
  mat_out[, 1] = mat_out[, 1]*diff(x_range) + x_range[1]
  mat_out[, 2] = mat_out[, 2]*diff(y_range) + y_range[1]
  mat_out
}
```


```{r, eval = FALSE}
B = 500
mc_points = gen_points(x_range = c(-1, 2), y_range = c(0, 12), B = B)
n_step = 10^3
x = seq(from = -1.35, to = 2.35, length.out = n_step)
x_in_omega = x[x >= -1 & x <= 2]
y = 3*x^2
col_I1 = hcl(h = seq(15, 375, length = 3), l = 65, c = 100, alpha = 0.2)[2]
plot(NA, xlim = range(x), ylim = range(y), xlab = "x", ylab = "f(x)")
grid()
polygon(c(-1, 2, 2, -1), c(0, 0, 12, 12), border = "red3", lty = 2, lwd = 2)
polygon(c(x_in_omega, rev(x_in_omega)), c(rep(0, length(x_in_omega)), 
        rev(y[x >= -1 & x <= 2])), border = "NA", col = col_I1)
lines(x, y, col = "blue4")
points(mc_points, pch = 19, cex = 0.5)
legend("topleft", c(expression(I[1]), expression(I[2]), "f(x)"), bty = "n",
       pch = c(NA, 15, NA), lwd = c(2, NA, 1), lty = c(2, NA, 1), 
       pt.cex = c(2, 2, NA), col = c("red3", col_I1, "blue4"))

```
